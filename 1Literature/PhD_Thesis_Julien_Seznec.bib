@inproceedings{papadimitriou1994complexity,
author = {Papadimitriou, Christos H and Tsitsiklis, John N},
booktitle = {Proceedings of IEEE 9th Annual Conference on Structure in Complexity Theory},
organization = {IEEE},
pages = {318--322},
title = {{The complexity of optimal queueing network control}},
year = {1994}
}
@book{hambleton2013item,
author = {Hambleton, Ronald K and Swaminathan, Hariharan},
publisher = {Springer Science {\&} Business Media},
title = {{Item response theory: Principles and applications}},
year = {2013}
}
@article{valko2013finite,
author = {Valko, Michal and Korda, Nathaniel and Munos, R{\'{e}}mi and Flaounas, Ilias and Cristianini, Nelo},
journal = {arXiv preprint arXiv:1309.6869},
title = {{Finite-time analysis of kernelised contextual bandits}},
year = {2013}
}
@inproceedings{valko2014spectral,
author = {Valko, Michal and Munos, R{\'{e}}mi and Kveton, Branislav and Koc{\'{a}}k, Tom{\'{a}}{\v{s}}},
booktitle = {International Conference on Machine Learning},
pages = {46--54},
title = {{Spectral bandits for smooth graph functions}},
year = {2014}
}
@incollection{filippi2010parametric,
author = {Filippi, Sarah and Cappe, Olivier and Garivier, Aur{\'{e}}lien and Szepesv{\'{a}}ri, Csaba},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {Lafferty, J D and Williams, C K I and Shawe-Taylor, J and Zemel, R S and Culotta, A},
pages = {586--594},
publisher = {Curran Associates, Inc.},
title = {{Parametric Bandits: The Generalized Linear Case}},
url = {http://papers.nips.cc/paper/4166-parametric-bandits-the-generalized-linear-case.pdf},
year = {2010}
}
@article{allesiardo2017non-stationary,
author = {Allesiardo, Robin and F{\'{e}}raud, Rapha{\"{e}}l and Maillard, Odalric-Ambrym},
doi = {10.1007/s41060-017-0050-5},
journal = {Int. J. Data Sci. Anal.},
number = {4},
pages = {267--283},
title = {{The non-stationary stochastic multi-armed bandit problem}},
url = {https://doi.org/10.1007/s41060-017-0050-5},
volume = {3},
year = {2017}
}
@inproceedings{cella2020stochastic,
abstract = {Motivated by recommendation problems in music streaming platforms, we propose a nonstationary stochastic bandit model in which the expected reward of an arm depends on the number of rounds that have passed since the arm was last pulled. After proving that finding an optimal policy is NP-hard even when all model parameters are known, we introduce a class of ranking policies provably approximating, to within a constant factor, the expected reward of the optimal policy. We show an algorithm whose regret with respect to the best ranking policy is bounded by {\$}\backslashwidetilde{\{}\backslashscO{\}}\backslashbig(\backslash!\backslashsqrt{\{}kT{\}}\backslashbig){\$}, where {\$}k{\$} is the number of arms and {\$}T{\$} is time. Our algorithm uses only {\$}\backslashscO\backslashbig(k\backslashln\backslashln T){\$} switches, which helps when switching between policies is costly. As constructing the class of learning policies requires ordering the arms according to their expectations, we also bound the number of pulls required to do so. Finally, we run experiments to compare our algorithm against UCB on different problem instances.},
address = {Online},
author = {Cella, Leonardo and Cesa-Bianchi, Nicol{\'{o}}},
editor = {Chiappa, Silvia and Calandra, Roberto},
pages = {1168--1177},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Stochastic Bandits with Delay-Dependent Payoffs}},
url = {http://proceedings.mlr.press/v108/cella20a.html},
volume = {108},
year = {2020}
}
@inproceedings{seznec2020single,
author = {Seznec, Julien and Menard, Pierre and Lazaric, Alessandro and Valko, Michal},
booktitle = {International Conference on Artificial Intelligence and Statistics},
pages = {3784--3794},
title = {{A single algorithm for both restless and rested rotting bandits}},
year = {2020}
}
@article{ventola2015antibiotic,
author = {Ventola, C Lee},
journal = {Pharmacy and therapeutics},
number = {4},
pages = {277},
publisher = {MediMedia, USA},
title = {{The antibiotic resistance crisis: part 1: causes and threats}},
volume = {40},
year = {2015}
}
@article{ventola2015antibiotic2,
author = {Ventola, C Lee},
journal = {Pharmacy and Therapeutics},
number = {5},
pages = {344},
publisher = {MediMedia, USA},
title = {{The antibiotic resistance crisis: part 2: management strategies and new agents}},
volume = {40},
year = {2015}
}
@inproceedings{agarwal2017corralling,
author = {Agarwal, Alekh and Luo, Haipeng and Neyshabur, Behnam and Schapire, Robert E},
booktitle = {Conference on Learning Theory},
organization = {PMLR},
pages = {12--38},
title = {{Corralling a band of bandit algorithms}},
year = {2017}
}
@inproceedings{besson2018aggregation,
author = {Besson, Lilian and Kaufmann, Emilie and Moy, Christophe},
booktitle = {2018 IEEE Wireless Communications and Networking Conference (WCNC)},
organization = {IEEE},
pages = {1--6},
title = {{Aggregation of multi-armed bandits learning algorithms for opportunistic spectrum access}},
year = {2018}
}
@article{andrieu2003introduction,
author = {Andrieu, Christophe and {De Freitas}, Nando and Doucet, Arnaud and Jordan, Michael I},
journal = {Machine learning},
number = {1-2},
pages = {5--43},
publisher = {Springer},
title = {{An introduction to MCMC for machine learning}},
volume = {50},
year = {2003}
}
@article{maillard2019sequential,
author = {Maillard, Odalric-Ambrym},
title = {{Sequential change-point detection: Laplace concentration of scan statistics and non-asymptotic delay bounds}},
year = {2019}
}
@article{clement2015multi,
author = {Clement, Benjamin and Roy, Didier and Oudeyer, Pierre-Yves and Lopes, Manuel},
journal = {Journal of Educational Data Mining},
number = {2},
publisher = {Citeseer},
title = {{Multi-Armed Bandits for Intelligent Tutoring Systems}},
volume = {7},
year = {2015}
}
@article{rollinson2015predictive,
author = {Rollinson, Joseph and Brunskill, Emma},
journal = {International Educational Data Mining Society},
publisher = {ERIC},
title = {{From Predictive Models to Instructional Policies.}},
year = {2015}
}
@inproceedings{kaser2016stop,
author = {K{\"{a}}ser, Tanja and Klingler, Severin and Gross, Markus},
booktitle = {Proceedings of the sixth international conference on learning analytics {\&} knowledge},
pages = {289--298},
title = {{When to stop? Towards universal instructional policies}},
year = {2016}
}
@article{rafferty2016faster,
author = {Rafferty, Anna N and Brunskill, Emma and Griffiths, Thomas L and Shafto, Patrick},
journal = {Cognitive science},
number = {6},
pages = {1290--1332},
publisher = {Wiley Online Library},
title = {{Faster teaching via pomdp planning}},
volume = {40},
year = {2016}
}
@phdthesis{pikeburke2019phd,
abstract = {This thesis is concerned with the study of sequential decision problems motivated by the challenge of selecting questions to give to students in an online educational environment. In online education there is the potential to develop personalized and adaptive learning environments, where students can receive individualized sequences of questions which update as the student is observed to be struggling or flourishing. In order to achieve this personalization, we must learn about how good each question is, while simultaneously giving students good questions. Multi-armed bandits are a popular technique for sequential decision making under uncertainty. Due to their online nature and their ability to balance the trade-off between exploitation and exploration, multi-armed bandits lend themselves naturally to this problem of adaptively selecting questions in education software. However, due to the complexity of the educational problem, standard approaches to multi-armed bandits cannot be applied directly. In this thesis variants of the multi-armed bandit problem specifically motivated by the issues arising in the educational domain are considered. Particular focus will be placed on ton the statistical and mathematical foundations of such approaches.},
author = {Pike-Burke, Ciara},
doi = {10.17635/lancaster/thesis/604},
keywords = {Multi-armed bandit,Sequential decision making},
publisher = {Lancaster University},
school = {Lancaster University},
title = {{Sequential decision problems in online education}},
year = {2019}
}
@article{melesko2019computer,
author = {Melesko, Jaroslav and Novickij, Vitalij},
journal = {Applied Sciences},
number = {20},
pages = {4303},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Computer Adaptive Testing Using Upper-Confidence Bound Algorithm for Formative Assessment}},
volume = {9},
year = {2019}
}
@inproceedings{teng2018interactive,
abstract = {The arise of E-learning systems has led to an anytime-anywhere-learning environment for everyone by providing various online courses and tests. However, due to the lack of teacher-student interaction, such ubiquitous learning is generally not as effective as offline classes. In traditional offline courses, teachers facilitate real-time interaction to teach students in accordance with personal aptitude from students' feedback in classes. Without the interruption of instructors, it is difficult for users to be aware of personal unknowns. In this paper, we address an important issue on the exploration of 'user unknowns' from an interactive question-answering process in E-learning systems. A novel interactive learning system, called CagMab, is devised to interactively recommend questions with a round-by-round strategy, which contributes to applications such as a conversational bot for self-evaluation. The flow enables users to discover their weakness and further helps them to progress. In fact, despite its importance, discovering personal unknowns remains a challenging problem in E-learning systems. Even though formulating the problem with the multi-armed bandit framework provides a solution, it often leads to suboptimal results for interactive unknowns recommendation as it simply relies on the contextual features of answered questions. Note that each question is associated with concepts and similar concepts are likely to be linked manually or systematically, which naturally forms the concept graphs. Mining the rich relationships among users, questions and concepts could be potentially helpful in providing better unknowns recommendation. To this end, in this paper, we develop a novel interactive learning framework by borrowing strengths from concept-aware graph embedding for learning user unknowns. Our experimental studies on real data show that the proposed framework can effectively discover user unknowns in an interactive fashion for the recommendation in E-learning systems.},
author = {Teng, S and Li, J and Ting, L P and Chuang, K and Liu, H},
booktitle = {2018 IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2018.00065},
issn = {2374-8486},
keywords = {computer aided instruction;data mining;educational},
month = {nov},
pages = {497--506},
title = {{Interactive Unknowns Recommendation in E-Learning Systems}},
year = {2018}
}
@article{luckin2001designing,
author = {Luckin, Rosemary},
journal = {Information Technology in Childhood Education Annual},
number = {1},
pages = {57--85},
publisher = {Association for the Advancement of Computing in Education (AACE)},
title = {{Designing children's software to ensure productive interactivity through collaboration in the Zone of Proximal Development (ZPD)}},
volume = {2001},
year = {2001}
}
@article{xu2016personalized,
author = {Xu, Jie and Xing, Tianwei and {Van Der Schaar}, Mihaela},
journal = {IEEE Transactions on Signal Processing},
number = {20},
pages = {5340--5352},
publisher = {IEEE},
title = {{Personalized course sequence recommendations}},
volume = {64},
year = {2016}
}
@inproceedings{mu2018combining,
author = {Mu, Tong and Wang, Shuhan and Andersen, Erik and Brunskill, Emma},
booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},
pages = {1--4},
title = {{Combining adaptivity with progression ordering for intelligent tutoring systems}},
year = {2018}
}
@inproceedings{neu2015explore,
author = {Neu, Gergely},
booktitle = {Advances in Neural Information Processing Systems},
pages = {3168--3176},
title = {{Explore no more: Improved high-probability regret bounds for non-stochastic bandits}},
year = {2015}
}
@inproceedings{lindsey2013optimizing,
author = {Lindsey, Robert V and Mozer, Michael C and Huggins, William J and Pashler, Harold},
booktitle = {Advances in neural information processing systems},
pages = {2778--2786},
title = {{Optimizing instructional policies}},
year = {2013}
}
@article{bubeck2012regret,
author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicolo},
journal = {arXiv preprint arXiv:1204.5721},
title = {{Regret analysis of stochastic and nonstochastic multi-armed bandit problems}},
year = {2012}
}
@article{gottlieb2013information,
author = {Gottlieb, Jacqueline and Oudeyer, Pierre-Yves and Lopes, Manuel and Baranes, Adrien},
journal = {Trends in Cognitive Sciences},
pages = {1--9},
title = {{Information-seeking, curiosity, and attention: computational and neural mechanisms}},
volume = {20},
year = {2013}
}
@inproceedings{kocsis2006discounted,
author = {Kocsis, Levente and Szepesv{\'{a}}ri, Csaba},
booktitle = {2nd PASCAL Challenges Workshop},
title = {{Discounted ucb}},
volume = {2},
year = {2006}
}
@article{cella2019stochastic,
abstract = {Motivated by recommendation problems in music streaming platforms, we propose a nonstationary stochastic bandit model in which the expected reward of an arm depends on the number of rounds that have passed since the arm was last pulled. After proving that finding an optimal policy is NP-hard even when all model parameters are known, we introduce a class of ranking policies provably approximating, to within a constant factor, the expected reward of the optimal policy. We show an algorithm whose regret with respect to the best ranking policy is bounded by {\$}\backslashwidetilde{\{}\backslashmathcal{\{}O{\}}{\}}\backslashbig(\backslash!\backslashsqrt{\{}kT{\}}\backslashbig){\$}, where {\$}k{\$} is the number of arms and {\$}T{\$} is time. Our algorithm uses only {\$}\backslashmathcal{\{}O{\}}\backslashbig(k\backslashln\backslashln T\backslashbig){\$} switches, which helps when switching between policies is costly. As constructing the class of learning policies requires ordering the arms according to their expectations, we also bound the number of pulls required to do so. Finally, we run experiments to compare our algorithm against UCB on different problem instances.},
archivePrefix = {arXiv},
arxivId = {1910.02757},
author = {Cella, Leonardo and Cesa-Bianchi, Nicol{\`{o}}},
eprint = {1910.02757},
file = {::},
month = {oct},
title = {{Stochastic Bandits with Delay-Dependent Payoffs}},
url = {http://arxiv.org/abs/1910.02757},
year = {2019}
}
@book{sutton1998book,
author = {Sutton, Richard S and Barto, Andrew G},
edition = {Second},
publisher = {The MIT Press},
title = {{Reinforcement Learning: An Introduction}},
url = {http://incompleteideas.net/book/the-book-2nd.html},
year = {2018}
}
@inproceedings{abe1999associative,
address = {San Francisco, CA, USA},
author = {Abe, Naoki and Long, Philip M},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
isbn = {1558606122},
pages = {3--11},
publisher = {Morgan Kaufmann Publishers Inc.},
series = {ICML '99},
title = {{Associative Reinforcement Learning Using Linear Probabilistic Concepts}},
year = {1999}
}
@inproceedings{lattimore2017end,
abstract = {Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for finite-armed bandits, notably the otimism principle and Thompson sampling. Prior analysis has mostly focussed on the worst-case setting. We analyse the asymptotic regret and show matching upper and lower bounds on what is achievable. Surprisingly, our results show that no algorithm based on optimism or Thompson sampling will ever achieve the optimal rate. In fact, they can be arbitrarily far from optimal, even in very simple cases. This is a disturbing result because these techniques are standard tools that are widely used for sequential optimisation, for example, generalised linear bandits and reinforcement learning.},
address = {Fort Lauderdale, FL, USA},
author = {Lattimore, Tor and Szepesvari, Csaba},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
editor = {Singh, Aarti and Zhu, Jerry},
pages = {728--737},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits}},
url = {http://proceedings.mlr.press/v54/lattimore17a.html},
volume = {54},
year = {2017}
}
@inproceedings{mukherjee2017thresholding,
author = {Mukherjee, Subhojyoti and Purushothama, Naveen Kolar and Sudarsanam, Nandan and Ravindran, Balaraman},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
isbn = {9780999241103},
pages = {2515--2521},
publisher = {AAAI Press},
series = {IJCAI'17},
title = {{Thresholding Bandits with Augmented UCB}},
year = {2017}
}
@inproceedings{garivier2016optimalbai,
abstract = {We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the ‘Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.},
address = {Columbia University, New York, New York, USA},
author = {Garivier, Aur{\'{e}}lien and Kaufmann, Emilie},
booktitle = {29th Annual Conference on Learning Theory},
editor = {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
pages = {998--1027},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Optimal Best Arm Identification with Fixed Confidence}},
url = {http://proceedings.mlr.press/v49/garivier16a.html},
volume = {49},
year = {2016}
}
@inproceedings{erraqabi2017trading,
abstract = {In multi-armed bandits, the most common objective is the maximization of the cumulative reward. Alternative settings include active exploration, where a learner tries to gain accurate estimates of the rewards of all arms. While these objectives are contrasting, in many scenarios it is desirable to trade off rewards and errors. For instance, in educational games the designer wants to gather generalizable knowledge about the behavior of the students and teaching strategies (small estimation errors) but, at the same time, the system needs to avoid giving a bad experience to the players, who may leave the system permanently (large reward). In this paper, we formalize this tradeoff and introduce the ForcingBalance algorithm whose performance is provably close to the best possible tradeoff strategy. Finally, we demonstrate on real-world educational data that ForcingBalance returns useful information about the arms without compromising the overall reward.},
address = {Fort Lauderdale, FL, USA},
author = {Erraqabi, Akram and Lazaric, Alessandro and Valko, Michal and Brunskill, Emma and Liu, Yun-En},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
editor = {Singh, Aarti and Zhu, Jerry},
pages = {709--717},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Trading off Rewards and Errors in Multi-Armed Bandits}},
url = {http://proceedings.mlr.press/v54/erraqabi17a.html},
volume = {54},
year = {2017}
}
@inbook{tewari2017contextual,
abstract = {The first paper on contextual bandits was written by Michael Woodroofe in 1979 (Journal of the American Statistical Association, 74(368), 799--806, 1979) but the term ``contextual bandits'' was invented only recently in 2008 by Langford and Zhang (Advances in neural information processing systems, pages 817--824, 2008). Woodroofe's motivating application was clinical trials whereas modern interest in this problem was driven to a great extent by problems on the internet, such as online ad and online news article placement. We have now come full circle because contextual bandits provide a natural framework for sequential decision making in mobile health. We will survey the contextual bandits literature with a focus on modifications needed to adapt existing approaches to the mobile health setting. We discuss specific challenges in this direction such as: good initialization of the learning algorithm, finding interpretable policies, assessing usefulness of tailoring variables, computational considerations, robustness to failure of assumptions, and dealing with variables that are costly to acquire and missing.},
address = {Cham},
author = {Tewari, Ambuj and Murphy, Susan A},
booktitle = {Mobile Health: Sensors, Analytic Methods, and Applications},
doi = {10.1007/978-3-319-51394-2_25},
editor = {Rehg, James M and Murphy, Susan A and Kumar, Santosh},
isbn = {978-3-319-51394-2},
pages = {495--517},
publisher = {Springer International Publishing},
title = {{From Ads to Interventions: Contextual Bandits in Mobile Health}},
url = {https://doi.org/10.1007/978-3-319-51394-2{\_}25},
year = {2017}
}
@inproceedings{carpentier2016tight,
abstract = {We consider the problem of $\backslash$textitbest arm identification with a $\backslash$textitfixed budget T, in the K-armed stochastic bandit setting, with arms distribution defined on [0,1]. We prove that any bandit strategy, for at least one bandit problem characterized by a complexity H, will misidentify the best arm with probability lower bounded by {\$}\backslashexp\backslashBig(-\backslashfrac{\{}T{\}}\backslashlog(K)H\backslashBig){\$}, where {\$}H{\$} is the sum for all sub-optimal arms of the inverse of the squared gaps. Our result disproves formally the general belief - coming from results in the fixed confidence setting - that there must exist an algorithm for this problem whose probability of error is upper bounded by {\$}\backslashexp(-T/H){\$}. This also proves that some existing strategies based on the Successive Rejection of the arms are optimal - closing therefore the current gap between upper and lower bounds for the fixed budget best arm identification problem.},
address = {Columbia University, New York, New York, USA},
author = {Carpentier, Alexandra and Locatelli, Andrea},
booktitle = {29th Annual Conference on Learning Theory},
editor = {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
pages = {590--604},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit Problem}},
url = {http://proceedings.mlr.press/v49/carpentier16.html},
volume = {49},
year = {2016}
}
@inproceedings{locatelli2016thresholding,
abstract = {We study a specific combinatorial pure exploration stochastic bandit problem where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and for a fixed time horizon. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with fixed budget for which provably optimal strategies are constructed.},
address = {New York, New York, USA},
author = {Locatelli, Andrea and Gutzeit, Maurilio and Carpentier, Alexandra},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
editor = {Balcan, Maria Florina and Weinberger, Kilian Q},
pages = {1690--1698},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{An optimal algorithm for the Thresholding Bandit Problem}},
url = {http://proceedings.mlr.press/v48/locatelli16.html},
volume = {48},
year = {2016}
}
@inproceedings{liu2014trading,
author = {Liu, Yun-En and Mandel, Travis and Brunskill, Emma and Popovic, Zoran},
booktitle = {EDM},
title = {{Trading Off Scientific Knowledge and User Learning with Multi-Armed Bandits}},
year = {2014}
}
@inproceedings{audibert2010bai,
address = {Haifa, Israel},
author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
booktitle = {COLT - 23th Conference on Learning Theory - 2010},
month = {jun},
pages = {13 p.},
title = {{Best Arm Identification in Multi-Armed Bandits}},
url = {https://hal-enpc.archives-ouvertes.fr/hal-00654404},
year = {2010}
}
@incollection{abbasi2011improved,
author = {Abbasi-yadkori, Yasin and P{\'{a}}l, D{\'{a}}vid and Szepesv{\'{a}}ri, Csaba},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {Shawe-Taylor, J and Zemel, R S and Bartlett, P L and Pereira, F and Weinberger, K Q},
pages = {2312--2320},
publisher = {Curran Associates, Inc.},
title = {{Improved Algorithms for Linear Stochastic Bandits}},
url = {http://papers.nips.cc/paper/4417-improved-algorithms-for-linear-stochastic-bandits.pdf},
year = {2011}
}
@article{auer2002using,
author = {Auer, Peter},
file = {::},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
number = {Nov},
pages = {397--422},
title = {{Using Confidence Bounds for Exploitation-Exploration Trade-offs}},
volume = {3},
year = {2002}
}
@article{lai1987adaptive,
author = {Lai, Tze Leung},
doi = {10.1214/AOS/1176350495},
issn = {0090-5364},
journal = {Annals of Statistics},
keywords = {Sequential experimentation,adaptive control,boundary crossings,dynamic allocation,upper confidence bounds},
number = {3},
pages = {1091--1114},
publisher = {Institute of Mathematical Statistics},
title = {{Adaptive Treatment Allocation and the Multi-Armed Bandit Problem}},
url = {https://projecteuclid.org/euclid.aos/1176350495},
volume = {15},
year = {1987}
}
@article{garivier2016thresholding,
abstract = {We analyze the sample complexity of the thresholding bandit problem, with and without the assumption that the mean values of the arms are increasing. In each case, we provide a lower bound valid for any risk {\$}\backslashdelta{\$} and any {\$}\backslashdelta{\$}-correct algorithm; in addition, we propose an algorithm whose sample complexity is of the same order of magnitude for small risks. This work is motivated by phase 1 clinical trials, a practically important setting where the arm means are increasing by nature, and where no satisfactory solution is available so far.},
archivePrefix = {arXiv},
arxivId = {1711.04454},
author = {Garivier, Aur{\'{e}}lien and M{\'{e}}nard, Pierre and Rossi, Laurent and Menard, Pierre},
eprint = {1711.04454},
file = {::},
month = {nov},
title = {{Thresholding Bandit for Dose-ranging: The Impact of Monotonicity}},
url = {http://arxiv.org/abs/1711.04454},
year = {2017}
}
@article{kaufmann2016complexity,
author = {Kaufmann, Emilie and Capp{\'{e}}, Olivier and Garivier, Aur{\'{e}}lien},
file = {::},
issn = {1533-7928},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {1--42},
title = {{On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models}},
url = {http://jmlr.org/papers/v17/kaufman16a.html},
volume = {17},
year = {2016}
}
@inproceedings{gabillon2012bai,
abstract = {We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms.},
author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
isbn = {9781627480031},
issn = {10495258},
pages = {3212--3220},
title = {{Best arm identification: A unified approach to fixed budget and fixed confidence}},
volume = {4},
year = {2012}
}
@article{tsallis1988possible,
abstract = {With the use of a quantity normally scaled in multifractals, a generalized form is postulated for entropy, namely Sq ≡k [1 - ∑i=1W piq]/(q-1), where q∈ℝ characterizes the generalization and pi are the probabilities associated with W (microscopic) configurations (W∈ℕ). The main properties associated with this entropy are established, particularly those corresponding to the microcanonical and canonical ensembles. The Boltzmann-Gibbs statistics is recovered as the q→1 limit. {\textcopyright} 1988 Plenum Publishing Corporation.},
author = {Tsallis, Constantino},
doi = {10.1007/BF01016429},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Generalized statistics,entropy,multifractals,statistical ensembles},
month = {jul},
number = {1-2},
pages = {479--487},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{Possible generalization of Boltzmann-Gibbs statistics}},
volume = {52},
year = {1988}
}
@inproceedings{mcmahan2011ftrl,
abstract = {We prove that many mirror descent algorithms for online convex optimization (such as online gradient descent) have an equivalent interpretation as follow-the-regularized-leader (FTRL) algorithms. This observation makes the relationships between many commonly used algorithms explicit, and provides theoretical insight on previous experimental observations. In particular, even though the FOBOS composite mirror descent algorithm handles L 1 regularization explicitly, it has been observed that the FTRL-style Regularized Dual Averaging (RDA) algorithm is even more effective at producing sparsity. Our results demonstrate that the key difference between these algorithms is how they handle the cumulative L 1 penalty. While FOBOS handles the L 1 term exactly on any given update, we show that it is effectively using subgradient approximations to the L 1 penalty from previous rounds, leading to less sparsity than RDA, which handles the cumulative penalty in closed form. The FTRL-Proximal algorithm, which we introduce, can be seen as a hybrid of these two algorithms, and significantly outperforms both on a large, real-world dataset. Copyright 2011 by the authors.},
author = {McMahan, H. Brendan},
booktitle = {Journal of Machine Learning Research},
issn = {15324435},
pages = {525--533},
title = {{Follow-the-regularized-leader and mirror descent: Equivalence theorems and L1 regularization}},
volume = {15},
year = {2011}
}
@inproceedings{kaufmann2012bayesian,
abstract = {Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution. We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed Bayes-UCB, satisfies finite-time regret bounds that imply its asymptotic optimality. More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.},
address = {La Palma, Canary Islands},
author = {Kaufmann, Emilie and Cappe, Olivier and Garivier, Aurelien},
booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
editor = {Lawrence, Neil D and Girolami, Mark},
pages = {592--600},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{On Bayesian Upper Confidence Bounds for Bandit Problems}},
url = {http://proceedings.mlr.press/v22/kaufmann12.html},
volume = {22},
year = {2012}
}
@article{zimmert2018tsallis,
archivePrefix = {arXiv},
arxivId = {1807.07623},
author = {Zimmert, Julian and Seldin, Yevgeny},
eprint = {1807.07623},
file = {::},
month = {jul},
title = {{Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits}},
url = {http://arxiv.org/abs/1807.07623},
year = {2018}
}
@inproceedings{agrawal2013finite,
abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have comparable or better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that proves the first near-optimal problem-independent bound of O($\backslash$sqrtNT$\backslash$ln T) on the expected regret of this algorithm. Our novel martingale-based analysis techniques are conceptually simple, and easily extend to distributions other than the Beta distribution. For the version of Thompson Sampling that uses Gaussian priors, we prove a problem-independent bound of O($\backslash$sqrtNT$\backslash$ln N) on the expected regret, and demonstrate the optimality of this bound by providing a matching lower bound. This lower bound of $\Omega$($\backslash$sqrtNT$\backslash$ln N) is the first lower bound on the performance of a natural version of Thompson Sampling that is away from the general lower bound of O($\backslash$sqrtNT) for the multi-armed bandit problem. Our near-optimal problem-independent bounds for Thompson Sampling solve a COLT 2012 open problem of Chapelle and Li. Additionally, our techniques simultaneously provide the optimal problem-dependent bound of (1+$\epsilon$)$\backslash$sum{\_}i $\backslash$frac$\backslash$ln Td($\mu${\_}i, $\mu${\_}1)+O($\backslash$fracN$\epsilon${\^{}}2) on the expected regret. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [2012].},
address = {Scottsdale, Arizona, USA},
author = {Agrawal, Shipra and Goyal, Navin},
booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
editor = {Carvalho, Carlos M and Ravikumar, Pradeep},
pages = {99--107},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Further Optimal Regret Bounds for Thompson Sampling}},
url = {http://proceedings.mlr.press/v31/agrawal13a.html},
volume = {31},
year = {2013}
}
@inproceedings{kaufmann2012ts,
abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case. {\textcopyright} 2012 Springer-Verlag.},
author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'{e}}mi},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-34106-9_18},
isbn = {9783642341052},
issn = {03029743},
month = {oct},
pages = {199--213},
publisher = {Springer, Berlin, Heidelberg},
title = {{Thompson sampling: An asymptotically optimal finite-time analysis}},
volume = {7568 LNAI},
year = {2012}
}
@inproceedings{degenne2016anytime,
abstract = {We introduce an anytime algorithm for stochastic multi-armed bandit with optimal distribution free and distribution dependent bounds (for a specific family of parameters). The performances of this algorithm (as well as another one motivated by the conjectured optimal bound) are evaluated empirically. A similar analysis is provided with full information, to serve as a benchmark.},
address = {New York, New York, USA},
author = {Degenne, R{\'{e}}my and Perchet, Vianney},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
editor = {Balcan, Maria Florina and Weinberger, Kilian Q},
pages = {1587--1595},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Anytime optimal algorithms in stochastic multi-armed bandits}},
url = {http://proceedings.mlr.press/v48/degenne16.html},
volume = {48},
year = {2016}
}
@article{lattimore2018refining,
author = {Lattimore, Tor},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {regret minimisation,sequential decision making,stochastic bandits},
month = {jan},
number = {1},
pages = {765--796},
publisher = {JMLR.org},
title = {{Refining the Confidence Level for Optimistic Bandit Strategies}},
volume = {19},
year = {2018}
}
@inproceedings{menard2017klucb++,
abstract = {We propose the {\$}\backslashtext{\{}kl-UCB{\}}{\^{}}{\{}++{\}}{\$} algorithm for regret minimization in stochastic bandit models with exponential families of distributions. We prove that it is simultaneously asymptotically optimal (in the sense of Lai and Robbins' lower bound) and minimax optimal. This is the first algorithm proved to enjoy these two properties at the same time. This work thus merges two different lines of research with simple and clear proofs.},
address = {Kyoto University, Kyoto, Japan},
author = {M{\'{e}}nard, Pierre and Garivier, Aur{\'{e}}lien},
booktitle = {Proceedings of the 28th International Conference on Algorithmic Learning Theory},
editor = {Hanneke, Steve and Reyzin, Lev},
pages = {223--237},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{A minimax and asymptotically optimal algorithm for stochastic bandits}},
url = {http://proceedings.mlr.press/v76/m{\'{e}}nard17a.html},
volume = {76},
year = {2017}
}
@article{jin2020mots,
abstract = {Thompson sampling is one of the most widely used algorithms for many online decision problems, due to its simplicity in implementation and superior empirical performance over other state-of-the-art methods. Despite its popularity and empirical success, it has remained an open problem whether Thompson sampling can achieve the minimax optimal regret {\$}O(\backslashsqrt{\{}KT{\}}){\$} for {\$}K{\$}-armed bandit problems, where {\$}T{\$} is the total time horizon. In this paper, we solve this long open problem by proposing a new Thompson sampling algorithm called MOTS that adaptively truncates the sampling result of the chosen arm at each time step. We prove that this simple variant of Thompson sampling achieves the minimax optimal regret bound {\$}O(\backslashsqrt{\{}KT{\}}){\$} for finite time horizon {\$}T{\$} and also the asymptotic optimal regret bound when {\$}T{\$} grows to infinity as well. This is the first time that the minimax optimality of multi-armed bandit problems has been attained by Thompson sampling type of algorithms.},
archivePrefix = {arXiv},
arxivId = {2003.01803},
author = {Jin, Tianyuan and Xu, Pan and Shi, Jieming and Xiao, Xiaokui and Gu, Quanquan},
eprint = {2003.01803},
file = {::},
month = {mar},
title = {{MOTS: Minimax Optimal Thompson Sampling}},
url = {http://arxiv.org/abs/2003.01803},
year = {2020}
}
@article{cappe2013klucb,
abstract = {We consider optimal sequential allocation in the context of the so-called stochastic multi-armed bandit model. We describe a generic index policy, in the sense of Gittins [J. R. Stat. Soc. Ser. B Stat. Methodol. 41 (1979) 148–177], based on upper confidence bounds of the arm payoffs computed using the Kullback–Leibler divergence. We consider two classes of distributions for which instances of this general idea are analyzed: the kl-UCB algorithm is designed for one-parameter exponential families and the empirical KL-UCB algorithm for bounded and finitely supported distributions. Our main contribution is a unified finite-time analysis of the regret of these algorithms that asymptotically matches the lower bounds of Lai and Robbins [Adv. in Appl. Math. 6 (1985) 4–22] and Burnetas and Katehakis [Adv. in Appl. Math. 17 (1996) 122–142], respectively. We also investigate the behavior of these algorithms when used with general bounded rewards, showing in particular that they provide significant improvements over the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1210.1136},
author = {Capp{\'{e}}, Olivier and Garivier, Aur{\'{e}}lien and Maillard, Odalric Ambrym and Munos, R{\'{e}}mi and Stoltz, Gilles},
doi = {10.1214/13-AOS1119},
eprint = {1210.1136},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Key words,Kullback–Leibler divergence,Phrases. Multi-armed bandit problems,Sequential testing,Upper confidence bound},
number = {3},
pages = {1516--1541},
publisher = {Institute of Mathematical Statistics},
title = {{Kullback–leibler upper confidence bounds for optimal sequential allocation}},
volume = {41},
year = {2013}
}
@article{audibert2009ucbv,
abstract = {Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Audibert, Jean Yves and Munos, R{\'{e}}mi and Szepesv{\'{a}}ri, Csaba},
doi = {10.1016/j.tcs.2009.01.016},
file = {::},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Bernstein's inequality,Exploration-exploitation tradeoff,High-probability bound,Multi-armed bandits,Risk analysis},
month = {apr},
number = {19},
pages = {1876--1902},
publisher = {Elsevier},
title = {{Exploration-exploitation tradeoff using variance estimates in multi-armed bandits}},
volume = {410},
year = {2009}
}
@article{burnetas1996optimal,
abstract = {Consider the problem of sequential sampling from m statistical populations to maximize the expected sum of outcomes in the long run. Under suitable assumptions on the unknown parameters $\theta$= ∈ $\Theta$, it is shown that there exists a class CR of adaptive policies with the following properties: (i) The expected n horizon reward V$\pi$0n ($\theta$=) under any policy $\pi$0 in CR is equal to n$\mu$,*($\eta$=) - M($\theta$=)log n + o(log n), as n → ∞, where $\mu$*($\theta$=) is the largest population mean and M($\theta$=) is a constant. (ii) Policies in CR are asymptotically optimal within a larger class CUF of "uniformly fast convergent" policies in the sense that limn → ∞(n$\mu$,*($\theta$=) - V$\pi$0n ($\theta$=))/ (n$\mu$*($\theta$=) - V$\pi$n($\theta$=)) ≤ 1, for any $\pi$ ∈ CUF and any $\theta$= ∈ $\Theta$ such that M($\theta$=) {\textgreater} 0. Policies in CR are specified via easily computable indices, defined as unique solutions to dual problems that arise naturally from the functional form of M($\theta$=). In addition, the assumptions are verified for populations specified by nonparametric discrete univariate distributions with finite support. In the case of normal populations with unknown means and variances, we leave as an open problem the verification of one assumption. {\textcopyright} 1996 Academic Press, Inc.},
author = {Burnetas, Apostolos N. and Katehakis, Michael N.},
doi = {10.1006/aama.1996.0007},
file = {::},
issn = {01968858},
journal = {Advances in Applied Mathematics},
month = {jun},
number = {2},
pages = {122--142},
publisher = {Academic Press Inc.},
title = {{Optimal adaptive policies for sequential allocation problems}},
volume = {17},
year = {1996}
}
@article{villar2015bandit,
abstract = {Multi-armed bandit problems (MABPs) are a special type of optimal control problem well suited to model resource allocation under uncertainty in a wide variety of contexts. Since the first publication of the optimal solution of the classic MABP by a dynamic index rule, the bandit literature quickly diversified and emerged as an active research topic. Across this literature, the use of bandit models to optimally design clinical trials became a typical motivating application, yet little of the resulting theory has ever been used in the actual design and analysis of clinical trials. To this end, we review two MABP decision-theoretic approaches to the optimal allocation of treatments in a clinical trial: the infinite-horizon Bayesian Bernoulli MABP and the finite-horizon variant. These models possess distinct theoretical properties and lead to separate allocation rules in a clinical trial design context. We evaluate their performance compared to other allocation rules, including fixed randomization. Our results indicate that bandit approaches offer significant advantages, in terms of assigning more patients to better treatments, and severe limitations, in terms of their resulting statistical power. We propose a novel bandit-based patient allocation rule that overcomes the issue of low power, thus removing a potential barrier for their use in practice.},
author = {Villar, Sof{\'{i}}a S. and Bowden, Jack and Wason, James},
doi = {10.1214/14-STS504},
file = {::},
issn = {08834237},
journal = {Statistical Science},
keywords = {Gittins index,Multi-armed bandit,Patient allocation,Response adaptive procedures,Whittle index},
number = {2},
pages = {199--215},
publisher = {Institute of Mathematical Statistics},
title = {{Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges}},
volume = {30},
year = {2015}
}
@inproceedings{pikeburke2019recovering,
author = {Pike-Burke, Ciara and Grunewalder, Steffen},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {{H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch{\'{e}}-Buc and E. Fox and R. Garnett}},
pages = {14122----14131},
publisher = {Curran Associates, Inc.},
title = {{Recovering Bandits}},
url = {http://papers.nips.cc/paper/9561-recovering-bandits.pdf},
year = {2019}
}
@incollection{wei2016tracking,
author = {Wei, Chen-Yu and Hong, Yi-Te and Lu, Chi-Jen},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {Lee, D D and Sugiyama, M and Luxburg, U V and Guyon, I and Garnett, R},
pages = {3972--3980},
publisher = {Curran Associates, Inc.},
title = {{Tracking the Best Expert in Non-stationary Stochastic Environments}},
url = {http://papers.nips.cc/paper/6536-tracking-the-best-expert-in-non-stationary-stochastic-environments.pdf},
year = {2016}
}
@inproceedings{immorlica2018recharging,
abstract = {We introduce a general model of bandit problems in which the expected payout of an arm is an increasing concave function of the time since it was last played. We first develop a PTAS for the underlying optimization problem of determining a reward-maximizing sequence of arm pulls. We then show how to use this PTAS in a learning setting to obtain sublinear regret.},
author = {Immorlica, Nicole and Kleinberg, Robert},
booktitle = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
doi = {10.1109/FOCS.2018.00037},
isbn = {9781538642306},
issn = {02725428},
keywords = {immorlica2018recharging},
month = {nov},
pages = {309--319},
publisher = {IEEE Computer Society},
title = {{Recharging bandits}},
volume = {2018-Octob},
year = {2018}
}
@misc{SMPyBandits,
annote = {Code at https://github.com/SMPyBandits/SMPyBandits/, documentation at https://smpybandits.github.io/},
author = {Besson, Lilian},
howpublished = {Online at: $\backslash$url{\{}GitHub.com/SMPyBandits/SMPyBandits{\}}},
title = {{SMPyBandits: an Open-Source Research Framework for Single and Multi-Players Multi-Arms Bandits (MAB) Algorithms in Python}},
url = {https://github.com/SMPyBandits/SMPyBandits/},
year = {2018}
}
@inproceedings{seznec2019rotting,
abstract = {In stochastic multi-armed bandits, the reward distribution of each arm is assumed to be stationary. This assumption is often violated in practice (e.g., in recommendation systems), where the reward of an arm may change whenever is selected, i.e., rested bandit setting. In this paper, we consider the non-parametric rotting bandit setting, where rewards can only decrease. We introduce the filtering on expanding window average (FEWA) algorithm that constructs moving averages of increasing windows to identify arms that are more likely to return high rewards when pulled once more. We prove that for an unknown horizon T, and without any knowledge on the decreasing behavior of the K arms, FEWA achieves problem-dependent regret bound of {\$}O(\backslashlog(KT)){\$}, and a problem-independent one of {\$}O(\backslashsqrt(KT)){\$}. Our result substantially improves over the algorithm of Levine et al. (2017), which suffers regret {\$}O(K{\^{}}(1/3) T{\^{}}(2/3){\$}. FEWA also matches known bounds for the stochastic bandit setting, thus showing that the rotting bandits are not harder. Finally, we report simulations confirming the theoretical improvements of FEWA.},
author = {Seznec, Julien and Locatelli, Andrea and Carpentier, Alexandra and Lazaric, Alessandro and Valko, Michal},
booktitle = {Proceedings of Machine Learning Research, The 22nd International Conference on Artificial Intelligence and Statistics, 16-18 April 2019.},
editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
pages = {2564--2572},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Rotting bandits are no harder than stochastic ones}},
url = {http://proceedings.mlr.press/v89/seznec19a.html},
volume = {89},
year = {2019}
}
@inproceedings{grid5000,
abstract = {Almost ten years after its premises, the Grid'5000 testbed has become one of the most complete testbed for designing or evaluating large-scale distributed systems. Initially dedicated to the study of High Performance Computing, the infrastructure has evolved to address wider concerns related to Desktop Computing, the Internet of Services and more recently the Cloud Computing paradigm. This paper present recent improvements of the Grid'5000 software and services stack to support large-scale experiments using virtualization technologies as building blocks. Such contributions include the deployment of customized software environments, the reservation of dedicated network domain and the possibility to isolate them from the others, and the automation of experiments with a REST API. We illustrate the interest of these contributions by describing three different use-cases of large-scale experiments on the Grid'5000 testbed. The first one leverages virtual machines to conduct larger experiments spread over 4000 peers. The second one describes the deployment of 10000 KVM instances over 4 Grid'5000 sites. Finally, the last use case introduces a one-click deployment tool to easily deploy major IaaS solutions. The conclusion highlights some important challenges of Grid'5000 related to the use of OpenFlow and to the management of applications dealing with tremendous amount of data. {\textcopyright} Springer International Publishing Switzerland 2013.},
author = {Balouek, Daniel and Amarie, Alexandra Carpen and Charrier, Ghislain and Desprez, Fr{\'{e}}d{\'{e}}ric and Jeannot, Emmanuel and Jeanvoine, Emmanuel and L{\`{e}}bre, Adrien and Margery, David and Niclausse, Nicolas and Nussbaum, Lucas and Richard, Olivier and Perez, Christian and Quesnel, Flavien and Rohr, Cyril and Sarzyniec, Luc},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-319-04519-1_1},
isbn = {9783319045184},
issn = {18650929},
keywords = {Cloud Computing,Distributed Systems,Experiments,Large-Scale Testbed,Virtualization},
pages = {3--20},
publisher = {Springer Verlag},
title = {{Adding Virtualization Capabilities to the Grid'5000 Testbed}},
volume = {367 CCIS},
year = {2013}
}
@inproceedings{chen2019new,
abstract = {We propose the first contextual bandit algorithm that is parameter-free, efficient, and optimal in terms of dynamic regret. Specifically, our algorithm achieves {\$}\backslashmathcal{\{}O{\}}(\backslashmin{\{}\backslashsqrt{\{}KST{\}}, K{\^{}}{\{}\backslashfrac{\{}1{\}}{\{}3{\}}{\}}\Delta{\^{}}{\{}\backslashfrac{\{}1{\}}{\{}3{\}}{\}}T{\^{}}{\{}\backslashfrac{\{}2{\}}{\{}3{\}}{\}}{\}}){\$} dynamic regret for a contextual bandit problem with {\$}T{\$} rounds, {\$}K{\$} actions, {\$}S{\$} switches and {\$}\Delta{\$} total variation in data distributions. Importantly, our algorithm is adaptive and does not need to know {\$}S{\$} or {\$}\Delta{\$} ahead of time, and can be implemented efficiently assuming access to an ERM oracle. Our results strictly improve the {\$}\backslashmathcal{\{}O{\}} (\backslashmin {\{}S{\^{}}{\{}\backslashfrac{\{}1{\}}{\{}4{\}}{\}}T{\^{}}{\{}\backslashfrac{\{}3{\}}{\{}4{\}}{\}}, \Delta{\^{}}{\{}\backslashfrac{\{}1{\}}{\{}5{\}}{\}}T{\^{}}{\{}\backslashfrac{\{}4{\}}{\{}5{\}}{\}}{\}}){\$} bound of (Luo et al., 2018), and greatly generalize and improve the {\$}\backslashmathcal{\{}O{\}}(\backslashsqrt{\{}ST{\}}){\$} result of (Auer et al., 2018) that holds only for the two-armed bandit problem without contextual information. The key novelty of our algorithm is to introduce {\{}$\backslash$it replay phases{\}}, in which the algorithm acts according to its previous decisions for a certain amount of time in order to detect non-stationarity while maintaining a good balance between exploration and exploitation.},
address = {Phoenix, USA},
author = {Chen, Yifang and Lee, Chung-Wei and Luo, Haipeng and Wei, Chen-Yu},
booktitle = {Proceedings of the Thirty-Second Conference on Learning Theory},
editor = {Beygelzimer, Alina and Hsu, Daniel},
pages = {696--726},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{A New Algorithm for Non-stationary Contextual Bandits: Efficient, Optimal and Parameter-free}},
url = {http://proceedings.mlr.press/v99/chen19b.html},
volume = {99},
year = {2019}
}
@inproceedings{cheung2019new,
abstract = {We introduce algorithms that achieve state-of-the-art dynamic regret bounds for non-stationary linear stochastic bandit setting. It captures natural applications such as dynamic pricing and ads allocation in a changing environment. We show how the difficulty posed by the non-stationarity can be overcome by a novel marriage between stochastic and adversarial bandits learning algorithms. Our main contributions are the tuned Sliding Window UCB (SW-UCB) algorithm with optimal dynamic regret, and the tuning free bandit-over-bandit (BOB) framework built on top of the SW-UCB algorithm with best (compared to existing literature) dynamic regret.},
author = {Cheung, Wang Chi and Simchi-Levi, David and Zhu, Ruihao},
booktitle = {Proceedings of Machine Learning Research},
editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
pages = {1079--1087},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Learning to Optimize under Non-Stationarity}},
url = {http://proceedings.mlr.press/v89/cheung19b.html},
volume = {89},
year = {2019}
}
@article{garivier2018explore,
abstract = {We revisit lower bounds on the regret in the case of multiarmed bandit problems. We obtain nonasymptotic, distribution-dependent bounds and provide simple proofs based only on well-known properties of Kullback–Leibler divergences. These bounds show in particular that in the initial phase the regret grows almost linearly, and that the well-known logarithmic growth of the regret only holds in a final phase. The proof techniques come to the essence of the information-theoretic arguments used and they involve no unnecessary complications.},
archivePrefix = {arXiv},
arxivId = {1602.07182},
author = {Garivier, Aur{\'{e}}lien and M{\'{e}}nard, Pierre and Stoltz, Gilles},
doi = {10.1287/moor.2017.0928},
eprint = {1602.07182},
issn = {15265471},
journal = {Mathematics of Operations Research},
keywords = {Cumulative regret,Information-theoretic proof techniques,Multiarmed bandits,Nonasymptotic lower bounds},
number = {2},
pages = {377--399},
publisher = {INFORMS Inst.for Operations Res.and the Management Sciences},
title = {{Explore first, exploit next: The true shape of regret in bandit problems}},
volume = {44},
year = {2019}
}
@incollection{warlop2018fighting,
author = {Warlop, Romain and Lazaric, Alessandro and Mary, J{\'{e}}r{\'{e}}mie},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and Cesa-Bianchi, N and Garnett, R},
pages = {1757--1768},
publisher = {Curran Associates, Inc.},
title = {{Fighting Boredom in Recommender Systems with Linear Reinforcement Learning}},
url = {http://papers.nips.cc/paper/7447-fighting-boredom-in-recommender-systems-with-linear-reinforcement-learning.pdf},
year = {2018}
}
@inproceedings{auer2019adaptively,
abstract = {We consider the variant of the stochastic multi-armed bandit problem where the stochastic reward distributions may change abruptly several times. In contrast to previous work, we are able to achieve (nearly) optimal mini-max regret bounds without knowing the number of changes. For this setting, we propose an algorithm called ADSWITCH and provide performance guarantees for the regret evaluated against the optimal non-stationary policy. Our regret bound is the first optimal bound for an algorithm that is not tuned with respect to the number of changes.},
address = {Phoenix, USA},
author = {Auer, Peter and Gajane, Pratik and Ortner, Ronald},
booktitle = {Proceedings of the Thirty-Second Conference on Learning Theory},
editor = {Beygelzimer, Alina and Hsu, Daniel},
pages = {138--158},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Adaptively Tracking the Best Bandit Arm with an Unknown Number of Distribution Changes}},
url = {http://proceedings.mlr.press/v99/auer19a.html},
volume = {99},
year = {2019}
}
@inproceedings{liu2018change-detection,
abstract = {The multi-armed bandit problem has been extensively studied under the stationary assumption. However in reality, this assumption often does not hold because the distributions of rewards themselves may change over time. In this paper, we propose a change-detection (CD) based framework for multi-armed bandit problems under the piecewise-stationary setting, and study a class of change-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that actively detects change points and restarts the UCB indices. We then develop CUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum (CUSUM) and Page-Hinkley Test (PHT) to detect changes. We show that CUSUM-UCB obtains the best known regret upper bound under mild assumptions. We also demonstrate the regret reduction of the CD-UCB policies over arbitrary Bernoulli rewards and Yahoo! datasets of webpage click-through rates.},
author = {Liu, Fang and Lee, Joohyun and Shroff, Ness},
keywords = {Multi-armed bandits; Online learning; Change detec},
title = {{A Change-Detection Based Framework for Piecewise-Stationary Multi-Armed Bandit Problem}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16939},
year = {2018}
}
@article{thompson1933likelihood,
author = {Thompson, William R},
doi = {10.1093/biomet/25.3-4.285},
issn = {00063444},
journal = {Biometrika},
number = {3/4},
pages = {285--294},
publisher = {[Oxford University Press, Biometrika Trust]},
title = {{On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples}},
url = {http://www.jstor.org/stable/2332286},
volume = {25},
year = {1933}
}
@inproceedings{louedec2016algorithme,
abstract = {Un nombre croissant de syst{\`{e}}mes num{\'{e}}riques font appel {\`{a}} des algorithmes de bandits pour combiner efficacement exploration de l'environnement et exploitation de l'information accumul{\'{e}}e. Les mod{\`{e}}les de bandits classiques sont toutefois assez na{\"{i}}fs : ils se bornent {\`{a}} un nombre fix{\'{e}} de choix disponibles (appel{\'{e}}s bras), et {\`{a}} des r{\'{e}}ponses ne variant pas au cours du temps. Pour les moteurs de recommandation, par exemple, il s'agit de limitations s{\'{e}}v{\`{e}}res : de nouveaux items {\`{a}} recommander apparaissent r{\'{e}}guli{\`{e}}rement, et les anciens ont une tendance pr{\'{e}}visible {\`{a}} perdre de l'attractivit{\'{e}}. Pour faire face {\`{a}} ces probl{\`{e}}mes, des strat{\'{e}}gies capables de g{\'{e}}rer l'{\'{e}}volution temporelle du gain moyen associ{\'{e}} {\`{a}} chaque bras ont {\'{e}}t{\'{e}} propos{\'{e}}es. Si ces strat{\'{e}}gies sont assez g{\'{e}}n{\'{e}}rales, elles ne sont pas forc{\'{e}}ment les plus efficaces dans le cas o{\`{u}} la forme de cette {\'{e}}volution temporelle est largement connue a priori. Dans cet article nous proposons deux nouvelles strat{\'{e}}gies capables de prendre en compte d'une part l'obsolescence progressive de chaque bras, et d'autre part l'arriv{\'{e}}e de nouveaux bras : Fading-UCB, pour laquelle nous fournissons une analyse d{\'{e}}taill{\'{e}}e de la borne sup{\'{e}}rieure de regret, et Trust and abandon. Nous montrons exp{\'{e}}rimentalement que les deux strat{\'{e}}gies propos{\'{e}}es permettent d'obtenir de meilleures performances que celles obtenues par les strat{\'{e}}gies de l'{\'{e}}tat de l'art.},
address = {http://www.lif.univ-mrs.fr},
author = {Lou{\"{e}}dec, Jonathan and Rossi, Laurent and Chevalier, Max and Garivier, Aur{\'{e}}lien and Mothe, Josiane},
booktitle = {Conf{\'{e}}rence francophone sur l'Apprentissage Automatique, Marseille, 05/07/2016-07/07/2016},
keywords = {Bandit non-stationnaire.,Flux de donn{\'{e}}es,Probl{\`{e}}mes de bandit,SIGEVI,Strat{\'{e}}gies UCB,Syst{\`{e}}mes de recommandation},
pages = {(en ligne)},
publisher = {Laboratoire d'Informatique Fondamentale de Marseille},
title = {{Algorithme de bandit et obsolescence : un mod{\`{e}}le pour la recommandation (regular paper)}},
url = {http://www.irit.fr/publis/SIG/2016{\_}CAP{\_}LRCGM.pdf - http://oatao.univ-toulouse.fr/17130/},
year = {2016}
}
@inproceedings{komiyama2014time-decaying,
abstract = {Contents displayed on web portals (e.g., news articles at Yahoo.com) are usually adaptively selected from a dynamic set of candidate items, and the attractiveness of each item decays over time. The goal of those websites is to maximize the engagement of users (usually measured by their clicks) on the selected items. We formulate this kind of applications as a new variant of bandit problems where new arms are dynamically added into the candidate set and the expected reward of each arm decays as the round proceeds. For this new problem, a direct application of the algorithms designed for stochastic MAB (e.g., UCB) will lead to over-estimation of the rewards of old arms, and thus cause a misidentification of the optimal arm. To tackle this challenge, we propose a new algorithm that can adaptively estimate the temporal dynamics in the rewards of the arms, and effectively identify the best arm at a given time point on this basis. When the temporal dynamics are represented by a set of features, the proposed algorithm is able to enjoy a sub-linear regret. Our experiments verify the effectiveness of the proposed algorithm.},
address = {Cham},
author = {Komiyama, Junpei and Qin, Tao},
booktitle = {Web and Internet Economics (WINE)},
editor = {Liu, Tie-Yan and Qi, Qi and Ye, Yinyu},
isbn = {978-3-319-13129-0},
pages = {460--466},
publisher = {Springer International Publishing},
title = {{Time-Decaying Bandits for Non-stationary Systems}},
year = {2014}
}
@inproceedings{cao2019nearly,
abstract = {Multi-armed bandit (MAB) is a class of online learning problems where a learning agent aims to maximize its expected cumulative reward while repeatedly selecting to pull arms with unknown reward distributions. We consider a scenario where the reward distributions may change in a piecewise-stationary fashion at unknown time steps. We show that by incorporating a simple change-detection component with classic UCB algorithms to detect and adapt to changes, our so-called M-UCB algorithm can achieve nearly optimal regret bound on the order of {\$}O(\backslashsqrt{\{}MKT\backslashlog T{\}}){\$}, where {\$}T{\$} is the number of time steps, {\$}K{\$} is the number of arms, and {\$}M{\$} is the number of stationary segments. Comparison with the best available lower bound shows that our M-UCB is nearly optimal in {\$}T{\$} up to a logarithmic factor. We also compare M-UCB with the state-of-the-art algorithms in numerical experiments using a public Yahoo! dataset and a real-world digital marketing dataset to demonstrate its superior performance.},
author = {Cao, Yang and Wen, Zheng and Kveton, Branislav and Xie, Yao},
booktitle = {Proceedings of Machine Learning Research},
editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
pages = {418--427},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Nearly Optimal Adaptive Procedure with Change Detection for Piecewise-Stationary Bandit}},
url = {http://proceedings.mlr.press/v89/cao19a.html},
volume = {89},
year = {2019}
}
@inproceedings{zimmert2019optimal,
abstract = {We derive an algorithm that achieves the optimal (up to constants) pseudo-regret in both adversarial and stochastic multi-armed bandits without prior knowledge of the regime and time horizon. The algorithm is based on online mirror descent with Tsallis entropy regularizer. We provide a complete characterization of such algorithms and show that Tsallis entropy with power {\$}\alpha= 1/2{\$} achieves the goal. In addition, the proposed algorithm enjoys improved regret guarantees in two intermediate regimes: the moderately contaminated stochastic regime defined by Seldin and Slivkins [22] and the stochastically constrained adversary studied by Wei and Luo [26]. The algorithm also obtains adversarial and stochastic optimality in the utility-based dueling bandit setting. We provide empirical evaluation of the algorithm demonstrating that it outperforms Ucb1 and Exp3 in stochastic environments. In certain adversarial regimes the algorithm significantly outperforms Ucb1 and Thompson Sampling, which exhibit close to linear regret.},
author = {Zimmert, Julian and Seldin, Yevgeny},
booktitle = {Proceedings of Machine Learning Research},
editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
pages = {467--475},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{An Optimal Algorithm for Stochastic and Adversarial Bandits}},
url = {http://proceedings.mlr.press/v89/zimmert19a.html},
volume = {89},
year = {2019}
}
@incollection{russac2019weighted,
author = {Russac, Yoan and Vernade, Claire and Capp{\'{e}}, Olivier},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
pages = {12040--12049},
publisher = {Curran Associates, Inc.},
title = {{Weighted Linear Bandits for Non-Stationary Environments}},
url = {http://papers.nips.cc/paper/9372-weighted-linear-bandits-for-non-stationary-environments.pdf},
year = {2019}
}
@incollection{besbes2014stochastic,
archivePrefix = {arXiv},
arxivId = {1405.3316},
author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q},
eprint = {1405.3316},
pages = {199--207},
publisher = {Curran Associates, Inc.},
title = {{Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards}},
url = {http://papers.nips.cc/paper/5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards.pdf},
year = {2014}
}
@article{mukherjee2019distribution,
abstract = {We consider the setup of stochastic multi-armed bandits in the case when reward distributions are piecewise i.i.d. and bounded with unknown changepoints. We focus on the case when changes happen simultaneously on all arms, and in stark contrast with the existing literature, we target gap-dependent (as opposed to only gap-independent) regret bounds involving the magnitude of changes {\$}(\backslashDelta{\^{}}{\{}chg{\}}{\_}{\{}i,g{\}}){\$} and optimality-gaps ({\$}\backslashDelta{\^{}}{\{}opt{\}}{\_}{\{}i,g{\}}{\$}). Diverging from previous works, we assume the more realistic scenario that there can be undetectable changepoint gaps and under a different set of assumptions, we show that as long as the compounded delayed detection for each changepoint is bounded there is no need for forced exploration to actively detect changepoints. We introduce two adaptations of UCB-strategies that employ scan-statistics in order to actively detect the changepoints, without knowing in advance the changepoints and also the mean before and after any change. Our first method $\backslash$UCBLCPD does not know the number of changepoints {\$}G{\$} or time horizon {\$}T{\$} and achieves the first time-uniform concentration bound for this setting using the Laplace method of integration. The second strategy $\backslash$ImpCPD makes use of the knowledge of {\$}T{\$} to achieve the order optimal regret bound of {\$}\backslashmin\backslashbig\backslashlbrace O(\backslashsum\backslashlimits{\_}{\{}i=1{\}}{\^{}}{\{}K{\}} \backslashsum\backslashlimits{\_}{\{}g=1{\}}{\^{}}{\{}G{\}}\backslashfrac{\{}\backslashlog(T/H{\_}{\{}1,g{\}}){\}}{\{}\backslashDelta{\^{}}{\{}opt{\}}{\_}{\{}i,g{\}}{\}}), O(\backslashsqrt{\{}GT{\}})\backslashbig\backslashrbrace{\$}, (where {\$}H{\_}{\{}1,g{\}}{\$} is the problem complexity) thereby closing an important gap with respect to the lower bound in a specific challenging setting. Our theoretical findings are supported by numerical experiments on synthetic and real-life datasets.},
archivePrefix = {arXiv},
arxivId = {1905.13159},
author = {Mukherjee, Subhojyoti and Maillard, Odalric-Ambrym},
eprint = {1905.13159},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Mukherjee, Maillard - 2019 - Distribution-dependent and Time-uniform Bounds for Piecewise i.i.d Bandits(2).pdf:pdf},
month = {may},
title = {{Distribution-dependent and Time-uniform Bounds for Piecewise i.i.d Bandits}},
url = {http://arxiv.org/abs/1905.13159},
year = {2019}
}
@article{besson2019generalized,
abstract = {We propose a new algorithm for the piece-wise $\backslash$iid{\{}{\}} non-stationary bandit problem with bounded rewards. Our proposal, GLR-klUCB, combines an efficient bandit algorithm, klUCB, with an efficient, parameter-free, change-point detector, the Bernoulli Generalized Likelihood Ratio Test, for which we provide new theoretical guarantees of independent interest. We analyze two variants of our strategy, based on local restarts and global restarts, and show that their regret is upper-bounded by {\$}\backslashmathcal{\{}O{\}}(\backslashUpsilon{\_}T \backslashsqrt{\{}T \backslashlog(T){\}}){\$} if the number of change-points {\$}\backslashUpsilon{\_}T{\$} is unknown, and by {\$}\backslashmathcal{\{}O{\}}(\backslashsqrt{\{}\backslashUpsilon{\_}T T \backslashlog(T){\}}){\$} if {\$}\backslashUpsilon{\_}T{\$} is known. This improves the state-of-the-art bounds, as our algorithm needs no tuning based on knowledge of the problem complexity other than {\$}\backslashUpsilon{\_}T{\$}. We present numerical experiments showing that GLR-klUCB outperforms passively and actively adaptive algorithms from the literature, and highlight the benefit of using local restarts.},
archivePrefix = {arXiv},
arxivId = {1902.01575},
author = {Besson, Lilian and Kaufmann, Emilie},
eprint = {1902.01575},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Besson, Kaufmann - 2019 - The Generalized Likelihood Ratio Test meets klUCB an Improved Algorithm for Piece-Wise Non-Stationary Bandits.pdf:pdf},
month = {feb},
title = {{The Generalized Likelihood Ratio Test meets klUCB: an Improved Algorithm for Piece-Wise Non-Stationary Bandits}},
url = {http://arxiv.org/abs/1902.01575},
year = {2019}
}
@article{traca2015regulating,
abstract = {In retail, there are predictable yet dramatic time-dependent patterns in customer behavior, such as periodic changes in the number of visitors, or increases in visitors just before major holidays. The current paradigm of multi-armed bandit analysis does not take these known patterns into account. This means that for applications in retail, where prices are fixed for periods of time, current bandit algorithms will not suffice. This work provides a remedy that takes the time-dependent patterns into account, and we show how this remedy is implemented in the UCB and {\{}$\backslash$epsilon{\}}-greedy methods and we introduce a new policy called the variable arm pool method. In the corrected methods, exploitation (greed) is regulated over time, so that more exploitation occurs during higher reward periods, and more exploration occurs in periods of low reward. In order to understand why regret is reduced with the corrected methods, we present a set of bounds that provide insight into why we would want to exploit during periods of high reward, and discuss the impact on regret. Our proposed methods perform well in experiments, and were inspired by a high-scoring entry in the Exploration and Exploitation 3 contest using data from Yahoo! Front Page. That entry heavily used time-series methods to regulate greed over time, which was substantially more effective than other contextual bandit methods.},
archivePrefix = {arXiv},
arxivId = {1505.05629},
author = {Trac{\`{a}}, Stefano and Rudin, Cynthia},
eprint = {1505.05629},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Trac{\`{a}}, Rudin - 2015 - Regulating Greed Over Time.pdf:pdf},
month = {may},
title = {{Regulating Greed Over Time}},
url = {http://arxiv.org/abs/1505.05629},
year = {2015}
}
@inproceedings{chapelle2011empirical,
abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
author = {Chapelle, Olivier and Li, Lihong},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS 2011)},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Chapelle, Li - 2011 - An Empirical Evaluation of Thompson Sampling.pdf:pdf},
pages = {2249--2257},
title = {{An Empirical Evaluation of Thompson Sampling}},
year = {2011}
}
@article{whittle1988restless,
abstract = {We consider a population of n projects which in general continue to evolve whether in operation or not (although by different rules). It is desired to choose the projects in operation at each instant of time so as to maximise the expected rate of reward, under a constraint upon the expected number of projects in operation. The Lagrange multiplier associated with this constraint defines an index which reduces to the Gittins index when projects not being operated are static. If one is constrained to operate m projects exactly then arguments are advanced to support the conjecture that, for m and n large in constant ratio, the policy of operating the m projects of largest current index is nearly optimal. The index is evaluated for some particular projects.},
author = {Whittle, P.},
doi = {10.2307/3214163},
issn = {0021-9002},
journal = {Journal of Applied Probability},
number = {A},
pages = {287--298},
publisher = {Cambridge University Press (CUP)},
title = {{Restless bandits: activity allocation in a changing world}},
url = {https://www.jstor.org/stable/3214163},
volume = {25},
year = {1988}
}
@article{whittle1980multi,
abstract = {A plausible conjecture (C) has the implication that a relationship (12) holds between the maximal expected rewards for a multi-project process and for a one-project process (F and $\phi$i respectively), if the option of retirement with reward M is available. The validity of this relation and optimality of Gittins' index rule are verified simultaneously by dynamic programming methods. These results are partially extended to the case of so-called "bandit superprocesses".},
author = {Whittle, P.},
doi = {10.2307/2984953},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
pages = {143--149},
publisher = {WileyRoyal Statistical Society},
title = {{Multi-Armed Bandits and the Gittins Index}},
url = {https://www.jstor.org/stable/2984953},
volume = {42},
year = {1980}
}
@book{chow1997probability,
abstract = {3rd ed. Classes of sets, measures, and probability spaces -- Binomial random variables -- Independence -- Integration in a probability space -- Sums of independent random variables -- Measure extensions, Lebesgue-Stieltjes measure, Kolmogorov consistency theorem -- Conditional expectation, conditional independence, introduction to martingales -- Distribution functions and characteristic functions -- Central limit theorems -- Limit theorems for independent random variables -- Martingales -- Infinitely divisible laws.},
author = {Chow, Yuan Shih and Teicher, Henry.},
isbn = {9780387982281},
pages = {488},
publisher = {Springer},
title = {{Probability theory : independence, interchangeability, martingales}},
url = {https://www.springer.com/gp/book/9780387982281},
year = {1997}
}
@article{lai1985asymptotically,
author = {Lai, T. L. and Robbins, Herbert},
doi = {10.1016/0196-8858(85)90002-8},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Lai, Robbins - 1985 - Asymptotically efficient adaptive allocation rules.pdf:pdf},
issn = {10902074},
journal = {Advances in Applied Mathematics},
month = {mar},
number = {1},
pages = {4--22},
publisher = {Academic Press},
title = {{Asymptotically efficient adaptive allocation rules}},
url = {https://www.sciencedirect.com/science/article/pii/0196885885900028},
volume = {6},
year = {1985}
}
@article{auer2002nonstochastic,
abstract = {In the multiarmed bandit problem, a gambler must decide which arm of K non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines. In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the per-round payoff of our algorithm approaches that of the best arm at the rate O(T-1/2). We show by a matching lower bound that this is the best possible. We also prove that our algorithm approaches the per-round payoff of any set of strategies at a similar rate: if the best strategy is chosen from a pool of N strategies, then our algorithm approaches the per-round payoff of the strategy at the rate O((log N)1/2T-1/2). Finally, we apply our results to the problem of playing an unknown repeated matrix game. We show that our algorithm approaches the minimax payoff of the unknown game at the rate O(T-1/2).},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E.},
doi = {10.1137/S0097539701398375},
isbn = {10957111},
issn = {00975397},
journal = {SIAM Journal on Computing},
keywords = {Adversarial bandit problem,Unknown matrix games},
month = {jan},
number = {1},
pages = {48--77},
publisher = {Society for Industrial and Applied Mathematics},
title = {{The nonstochastic multiarmed bandit problem}},
volume = {32},
year = {2003}
}
@book{lattimore2020banditbook,
abstract = {Decision-making in the face of uncertainty is a significant challenge in machine learning, and the multi-armed bandit model is a commonly used framework to address it. This comprehensive and rigorous introduction to the multi-armed bandit problem examines all the major settings, including stochastic, adversarial, and Bayesian frameworks. A focus on both mathematical intuition and carefully worked proofs makes this an excellent reference for established researchers and a helpful resource for graduate students in computer science, engineering, statistics, applied mathematics and economics. Linear bandits receive special attention as one of the most useful models in applications, while other chapters are dedicated to combinatorial bandits, ranking, non-stationary problems, Thompson sampling and pure exploration. The book ends with a peek into the world beyond bandits with an introduction to partial monitoring and learning in Markov decision processes.},
author = {Lattimore, Tor and Szepesv{\'{a}}ri, Csaba},
isbn = {1108486827},
month = {jun},
publisher = {Cambridge University Press UK},
title = {{Bandit Algorithms}},
url = {https://tor-lattimore.com/downloads/book/book.pdf},
year = {2020}
}
@inproceedings{bifet2007learning,
abstract = {We present a new approach for dealing with distribution change and concept drift when learning from data sequences that may vary with time. We use sliding windows whose size, instead of being fixed a priori, is recomputed online according to the rate of change observed from the data in the window itself. This delivers the user or programmer from having to guess a time-scale for change. Contrary to many related works, we provide rigorous guarantees of performance, as bounds on the rates of false positives and false negatives. Using ideas from data stream algorithmics, we develop a time-and memory-efficient version of this algorithm, called ADWIN2. We show how to combine ADWIN2 with the Na{\"{i}}ve Bayes (NB) predictor, in two ways: one, using it to monitor the error rate of the current model and declare when revision is necessary and, two, putting it inside the NB predictor to maintain up-to-date estimations of conditional probabilities in the data. We test our approach using synthetic and real data streams and compare them to both fixed-size and variable-size window strategies with good results.},
author = {Bifet, Albert and Gavald{\`{a}}, Ricard},
booktitle = {Proceedings of the 7th SIAM International Conference on Data Mining},
doi = {10.1137/1.9781611972771.42},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Bifet, Gavald{\`{a}} - 2007 - Learning from time-changing data with adaptive windowing.pdf:pdf},
isbn = {9780898716306},
keywords = {Concept and distribution drift,Data streams,Na{\"{i}}ve bayes,Time-changing data},
pages = {443--448},
title = {{Learning from time-changing data with adaptive windowing}},
year = {2007}
}
@inproceedings{audibert2009minimax,
abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit problem. Concretely, we remove an extraneous logarithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays.},
author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
booktitle = {Proceedings of the Conference on Learning Theory (COLT), 2009},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Audibert, Bubeck - 2009 - Minimax policies for adversarial and stochastic bandits.pdf:pdf},
keywords = {()},
pages = {217--226},
title = {{Minimax policies for adversarial and stochastic bandits}},
url = {https://hal-enpc.archives-ouvertes.fr/hal-00834882},
year = {2009}
}
@article{besson2018doubling,
abstract = {An online reinforcement learning algorithm is anytime if it does not need to know in advance the horizon T of the experiment. A well-known technique to obtain an anytime algorithm from any non-anytime algorithm is the "Doubling Trick". In the context of adversarial or stochastic multi-armed bandits, the performance of an algorithm is measured by its regret, and we study two families of sequences of growing horizons (geometric and exponential) to generalize previously known results that certain doubling tricks can be used to conserve certain regret bounds. In a broad setting, we prove that a geometric doubling trick can be used to conserve (minimax) bounds in {\$}R\backslash{\_}T = O(\backslashsqrt{\{}T{\}}){\$} but cannot conserve (distribution-dependent) bounds in {\$}R\backslash{\_}T = O(\backslashlog T){\$}. We give insights as to why exponential doubling tricks may be better, as they conserve bounds in {\$}R\backslash{\_}T = O(\backslashlog T){\$}, and are close to conserving bounds in {\$}R\backslash{\_}T = O(\backslashsqrt{\{}T{\}}){\$}.},
archivePrefix = {arXiv},
arxivId = {1803.06971},
author = {Besson, Lilian and Kaufmann, Emilie},
eprint = {1803.06971},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Besson, Kaufmann - 2018 - What Doubling Tricks Can and Can't Do for Multi-Armed Bandits.pdf:pdf},
month = {mar},
title = {{What Doubling Tricks Can and Can't Do for Multi-Armed Bandits}},
url = {http://arxiv.org/abs/1803.06971},
year = {2018}
}
@inproceedings{garivier2011upper-confidence-bound,
abstract = {Many problems, such as cognitive radio, parameter control of a scanning tunnelling microscope or internet advertisement, can be modelled as non-stationary bandit problems where the distributions of rewards changes abruptly at unknown time instants. In this paper, we analyze two algorithms designed for solving this issue: discounted UCB (D-UCB) and sliding-window UCB (SW-UCB). We establish an upper-bound for the expected regret by upper-bounding the expectation of the number of times suboptimal arms are played. The proof relies on an interesting Hoeffding type inequality for self normalized deviations with a random number of summands. We establish a lower-bound for the regret in presence of abrupt changes in the arms reward distributions. We show that the discounted UCB and the sliding-window UCB both match the lower-bound up to a logarithmic factor. Numerical simulations show that D-UCB and SW-UCB perform significantly better than existing soft-max methods like EXP3.S. {\textcopyright} 2011 Springer-Verlag.},
author = {Garivier, Aur{\'{e}}lien and Moulines, Eric},
booktitle = {Proceedings of the 22nd International Conference on Algorithmic Learning Theory (ALT), 2011, Espoo, Finland.},
doi = {10.1007/978-3-642-24412-4_16},
isbn = {9783642244117},
issn = {03029743},
pages = {174--188},
publisher = {Springer, Berlin, Heidelberg},
title = {{On upper-confidence bound policies for switching bandit problems}},
volume = {6925 LNAI},
year = {2011}
}
@inproceedings{levine2017rotting,
abstract = {The Multi-Armed Bandits (MAB) framework highlights the trade-off between acquiring new knowledge (Exploration) and leveraging available knowledge (Ex-ploitation). In the classical MAB problem, a decision maker must choose an arm at each time step, upon which she receives a reward. The decision maker's objective is to maximize her cumulative expected reward over the time horizon. The MAB problem has been studied extensively, specifically under the assumption of the arms' rewards distributions being stationary, or quasi-stationary, over time. We consider a variant of the MAB framework, which we termed Rotting Bandits, where each arm's expected reward decays as a function of the number of times it has been pulled. We are motivated by many real-world scenarios such as online advertising , content recommendation, crowdsourcing, and more. We present algorithms, accompanied by simulations, and derive theoretical guarantees.},
archivePrefix = {arXiv},
arxivId = {1702.07274},
author = {Levine, Nir and Crammer, Koby and Mannor, Shie},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
eprint = {1702.07274},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Levine, Crammer, Mannor - 2017 - Rotting Bandits.pdf:pdf},
pages = {3074--3083},
title = {{Rotting Bandits}},
year = {2017}
}
@article{heidari2016tight,
abstract = {We consider a variant of the well-studied multi-armed bandit problem in which the reward from each action evolves monotonically in the number of times the decision maker chooses to take that action. We are motivated by settings in which we must give a series of homogeneous tasks to a finite set of arms (workers) whose performance may improve (due to learning) or decay (due to loss of interest) with repeated trials. We assume that the arm-dependent rates at which the rewards change are unknown to the decision maker, and propose algorithms with provably optimal policy regret bounds, a much stronger notion than the often-studied external regret. For the case where the rewards are increasing and concave, we give an algorithm whose policy regret is sublinear and has a (provably necessary) dependence on the time required to distinguish the optimal arm from the rest. We illustrate the behavior and performance of this algorithm via simulations. For the decreasing case, we present a simple greedy approach and show that the policy regret of this algorithm is constant and upper bounded by the number of arms.},
author = {Heidari, Hoda and Kearns, Michael and Roth, Aaron},
journal = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
pages = {1562--1570},
title = {{Tight Policy Regret Bounds for Improving and Decaying Bandits}},
year = {2016}
}
@article{auer2002finite,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Fischer, Paul},
doi = {10.1023/A:1013689704352},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Auer, Cesa-Bianchi, Fischer - 2002 - Finite-time analysis of the multiarmed bandit problem.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Adaptive allocation rules,Bandit problems,Finite horizon regret},
month = {may},
number = {2-3},
pages = {235--256},
publisher = {Springer},
title = {{Finite-time analysis of the multiarmed bandit problem}},
volume = {47},
year = {2002}
}
@inproceedings{lan2016contextual,
abstract = {Recent developments in machine learning have the potential to revolutionize education by providing an optimized, personalized learning experience for each student. We study the problem of selecting the best personalized learning action that each student should take next given their learning history; possible actions could include reading a textbook section, watching a lecture video, interacting with a simulation or lab, solving a practice question, and so on. We first estimate each student's knowledge profile from their binary-valued graded responses to questions in their previous assessments using the SPARFA framework. We then employ these knowledge profiles as contexts in the contextual (multi-armed) bandits framework to learn a policy that selects the personalized learning actions that maximize each student's immediate success, i.e., their performance on their next assessment. We develop two algorithms for personalized learning action selection. While one is mainly of theoretical interest, we experimentally validate the other using a real-world educational dataset. Our experimental results demonstrate that our approach achieves superior or comparable performance as compared to existing algorithms in terms of maximizing the students' immediate success.},
author = {Lan, Andrew S and Baraniuk, Richard G},
booktitle = {Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Lan, Baraniuk - Unknown - A Contextual Bandits Framework for Personalized Learning Action Selection.pdf:pdf},
pages = {424--429},
title = {{A contextual bandits framework for personalized learning action selection}},
url = {https://pdfs.semanticscholar.org/a19e/4e14c424597df1d11f2cf99d09452b3da25b.pdf},
year = {2016}
}
@article{bouneffouf2016multi-armed,
abstract = {We consider a variant of the multi-armed bandit model, which we call multi-armed bandit problem with known trend, where the gambler knows the shape of the reward function of each arm but not its distribution. This new problem is motivated by different on-line problems like active learning, music and interface recommendation applications, where when an arm is sampled by the model the received reward change according to a known trend. By adapting the standard multi-armed bandit algorithm UCB1 to take advantage of this setting, we propose the new algorithm named Adjusted Upper Confidence Bound (A-UCB) that assumes a stochastic model. We provide upper bounds of the regret which compare favorably with the ones of UCB1. We also confirm that experimentally with different simulations.},
author = {Bouneffouf, Djallel and F{\'{e}}raud, Raphael},
doi = {10.1016/J.NEUCOM.2016.02.052},
issn = {0925-2312},
journal = {Neurocomputing},
month = {sep},
pages = {16--21},
publisher = {Elsevier},
title = {{Multi-armed bandit problem with known trend}},
url = {https://www.sciencedirect.com/science/article/pii/S092523121600299X},
volume = {205},
year = {2016}
}
