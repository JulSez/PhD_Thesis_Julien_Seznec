%!TEX root = ../main.tex
\section*{Plan détaillé}

Cette thèse peut s'approcher de deux manières différentes. En version longue, elle traite de la possibilité d'appliquer des modèles d'apprentissage par renforcement pour améliorer les séquences d'exercices données à un élève sur une plateforme en ligne. En version courte, elle peut se lire comme une thèse apportant des contributions fondamentales à des problèmes de bandits à plusieurs bras non-stationnaires. Commençons par présenter la version longue.

Cette thèse s'est déroulée dans le cadre du dispositif CIFRE avec l'entreprise Lelivrescolaire.fr. Cette entreprise a développé Afterclasse, un site de révision en ligne gratuitement accessible et massivement utilisé. Dans le Chapitre~\ref{ch:afterclasse}, on donne quelques éléments de contexte concernant Afterclasse et Lelivrescolaire.fr. Le but de la thèse y est précisé : améliorer la séquence d'exercices en fonction des résultats de chaque élève, avec un focus sur le court terme. En effet, les élèves "bachotent" sur la plateforme et sont assez peu engagés dans le temps. Il est donc naturel de chercher à les aider sur le court terme. Pour cela, il nous faut évaluer rapidement ce qu'ils savent et ce qu'ils ne savent pas et utiliser cette connaissance pour les réorienter vers les exercices qui leurs seraient les plus utiles.

Le Chapitre~\ref{ch:exploration} présente les modèles les plus classiques d'exploration-exploitation en machine learning. Cette revue de littérature peu mathématisée (par rapport au standard du domaine) a pour but de présenter de manière détaillée mais abordable les questions et les réponses apportées par la communauté du machine learning sur ce dilemme naturel entre exploration et exploitation. Une emphase particulière est mise sur les problèmes de bandits à plusieurs bras. Dans ces problèmes, un agent choisit séquentiellement parmi plusieurs actions et obtient une récompense dépendant de l'action choisit. Le but est de réussir à repérer les actions qui mènent aux plus fortes récompenses. Pour cela, le joueur doit accepter d'explorer - et donc de se tromper - les différentes options afin d'améliorer sa connaissance du problème. 

Plusieurs modèles de récompenses sont détaillés. Dans le modèle stochastique stationnaire (Section~\ref{sec:stoch-bandits}), chaque bras est lié à une distribution aléatoire. Le but est donc d'évaluer la moyenne de la distribution tout en quantifiant l'information manquante. Dans le modèle adversarial (Section~\ref{sec:adv-bandits}), la récompense est choisie par un adversaire et on cherche donc à repérer les actions les plus prolifiques tout en étant suffisamment imprédictible par l'adversaire. Dans les modèles stochastiques non-stationnaires (Section~\ref{sec:non-stationary}), la distribution associée à chaque bras peut changer au cours du temps ou en fonction des actions choisies. Cela mène à un double problème : d'estimation statistique d'une part (Quelle est la valeur courante ?), et de stratégie d'autre part (Comment puis-je me prémunir contre la non-stationarité ?). Enfin, les bandits contextuels (Section~\ref{sec:contextual}) lient les différentes actions à l'aide d'éléments de contexte. Ces éléments permettent d'envisager un très grand nombre d'actions qui ne seront pas toutes explorées : l'information sur les unes permettant d'extrapoler la valeur des autres. 

Finalement, on présente en Section~\ref{sec:rl} quelques fondamentaux d'apprentissage par renforcement (\emph{reinforcement learning}). Ce modèle est beaucoup plus général que les modèles de bandits car le joueur possède un état qu'il doit contrôler pour rester dans des zones à fortes récompenses. Ce modèle est très riche et permet d'apprendre des taches très complexes. Pour autant, il est très consommateur de données et ses meilleures réussites ont souvent lieu dans des dispositifs ou des données peuvent être simulées facilement.

Le Chapitre~\ref{ch:its} est consacré aux possibilités d'adaptation du modèle de bandits pour un usage éducatif. Lorsqu'on donne une question à un élève, on observe la réponse à cette question, ce qui est très similaire au \emph{feedback} du modèle de bandit. Cependant, nous notons quatre écueils fondamentaux (Section~\ref{sec:shortcomings}) qui entravent un tel usage. Tout d'abord, il est difficile d'associer la réponse d'un élève à une récompense pour l'algorithme. Faut-il privilégier les questions que l'élève réussit ou celles qu'il ne connaît pas ? Une fois la récompense posée, l'algorithme va tenter de la maximiser "brutalement": il faut donc faire très attention au proxy utilisé.




Deuxièmement, les modèles de bandits comparent souvent la performance obtenue à la performance de la "meilleure" action (inconnue). Dans le cas d'une séquence d'exercices, il est assez naturel qu'un exercice puissent être très intéressant à un instant donné, et beaucoup moins intéressant à l'avenir (par exemple, une fois que l'élève le maîtrise parfaitement). Il est donc essentiel de pouvoir se doter de points de comparaison plus intelligents que celui des bandits stationnaires ou adversariaux. 

Troisièmement, le modèle stationnaire suppose que ce qu'on observe ne réagit pas à nos actions. Dans le cas d'un système d'apprentissage, on souhaite que nos actions améliorent les performances de l'élève et que \emph{in fine} cela change les réponses qu'il envoie. Autrement dit, on souhaiterait pouvoir incorporer une modélisation de l'interaction entre l'élève et la machine.

Quatrièmement, les élèves font quelques dizaines de questions dans une séquence. C'est très peu, y compris pour un problème simple comme le problème des bandits stationnaires. Cet écueil s'oppose aux trois autres : là où ces derniers suggéraient une modélisation ambitieuse; ici, on doit se contenter de problèmes très simples.

À la Section~\ref{sec:bandits4ITS}, on détaille l'état de l'art de l'usage des modèles de bandits dans les systèmes d'apprentissage intelligents. Ces études empiriques très poussées s'attaquent à plusieurs objectifs, et montrent le plus souvent les comportements intéressants des algorithmes de bandits dans ces systèmes. Cependant, aucun de ces travaux ne s'attaque directement aux problèmes fondamentaux évoqués précédemment.

Dans le Chapitre~\ref{ch:rested}, on étudie la possibilité de faire travailler un élève sur le sujet le moins connu alors que celui-ci progresse au cours de ses révisions. Ainsi, on associe une récompense positive lorsque l'algorithme trouve une question qui n'est pas connue par l'élève (écueil 1). Au fur et à mesure que l'élève se perfectionne (écueil 3), il y a des moins en moins de récompenses sur le sujet sélectionné. Ce dispositif est donc nommé "bandits pourrissant au repos", ce qui traduit la raréfaction des récompenses et l'absence d'évolution sur les sujets non-sélectionnés. Dans ce dispositif, il est nécessaire de changer de sujet lorsque l'élève a suffisamment progressé sur un chapitre peu connu initialement (écueil 2). L'étude statistique du problème permet de conclure que malgré sa très forte richesse, le problème n'est pas plus dur à apprendre que le problème stationnaire (écueil 4). Ainsi, on montre que le dispositif des bandits au repos apporte une contribution significative aux quatre problèmes fondamentaux détaillés au Chapitre~\ref{ch:its}. Le Chapitre~\ref{ch:restless} étudie un autre dispositif de bandits où la récompense décroît indépendamment des choix d'actions. Ce problème est donc nommé "bandits pourrissant sans repos". Ce modèle est fortement lié au premier, bien qu'il ne soit pas motivé par des applications éducatives. 

Dans le Chapitre~\ref{ch:pomdp}, on considère à nouveau que l'élève progresse lors de ses révisions, mais l'objectif n'est plus de maximiser la récompense cumulée. On définit un seuil de maîtrise et on cherche à atteindre ce seuil sur tous les chapitres. Ce problème se place donc dans le cadre plus large des Problèmes de Décisions Markoviens Partiellement Observés (POMDP). La récompense est beaucoup plus implicite qu'elle ne l'est dans le cas des bandits classiques. À l'aide de quelques hypothèses bien choisies, on montre que la meilleure politique oracle est de choisir le chapitre le mieux connu sous le seuil de maîtrise. Cela contraste fortement avec les bandits décroissants: on cherche désormais à viser les chapitres les plus faciles jusqu'à ce qu'ils soient maîtrisés. Fort de notre très bonne compréhension du problème oracle, on propose une heuristique pour la politique apprenante.

Les résultats fondamentaux sur les bandits se concentrent dans les Chapitres~\ref{ch:rested} et~\ref{ch:restless}. Le Chapitre~\ref{ch:rested} se décompose de la manière suivante. La Section~\ref{sec:rested-model} présente le modèle et les deux principaux travaux préliminaires de manière exhaustive. \citet{heidari2016tight} ont étudié le problème avec une récompense non-bruitée tandis que \citet{levine2017rotting} ont étudié le problème bruité. Ils proposent \SWA, un algorithme avec une borne de regret indépendante du problème $\tcO\pa{T^{2/3}}$.

Cet algorithme utilise un mécanisme de fenêtre glissante de taille fixe. Nous avons proposé deux algorithmes (Section~\ref{sec:algo}), \FEWA et \RAWUCB, qui utilisent pour chaque bras des statistiques balayant toutes les fenêtres possibles. \FEWA utilise ces statistiques pour filtrer les bras les moins bons en partant des échantillons les plus récents.\RAWUCB calcule de multiples indices UCB pour chaque bras et utilise le plus petit pour comparer les bras entre eux. L'étude de ces algorithmes (Section~\ref{sec:theory}) a permis de prouver une borne de regret indépendante du problème $\tcO\pa{\sqrt{T}}$ et une borne dépendante du problème $\cO\pa{\log{T}}$. Ces bornes sont comparables avec le problème stationnaire ce qui nous permet de suggérer que ce problème - bien que plus général - n'est pas plus dur que le problème stationnaire. 

La performance empirique des algorithmes est testée sur des données simulées (Section~\ref{sec:rested-experiment}). Non seulement \RAWUCB et \FEWA obtiennent les meilleurs résultats, mais le détail des expériences montrent en plus des différence qualitatives notables dans la forme du regret comparé à \SWA. En particulier, nos algorithmes sont agnostiques à tous les paramètres du problème à l'exception du niveau de bruit $\sigma$. C'est une forte amélioration par rapport à \SWA qui doit connaître en plus la décroissance maximum ainsi que l'horizon de temps pour obtenir ses meilleures performances. 

Nos algorithmes utilisent $\cO\pa{T}$ statistiques par tour et souffrent donc d'une complexité algorithmique prohibitive (en temps et en espace). Cependant, on montre à la Section~\ref{sec:fast} qu'il est possible de réduire les statistiques utilisées (et la complexité afférente) à $\cO\pa{K\log{T}}$ par tour, tout en retrouvant les mêmes bornes de regret que pour les algorithmes originaux. C'est une meilleure complexité que celle de \SWA ($\cO\pa{T^{2/3}}$), bien que, en pratique, \SWA soit beaucoup plus rapide. Nous avons essayé d'étendre nos résultats aux bandits linéaires. Cependant, nous avons pu montrer que le problème proposé n'était pas apprenable, même en l'absence de bruit. En effet, la non-stationnarité au repos se comporte mal avec le contexte vectoriel.


Le Chapitre~\ref{ch:restless} commence par une revue de la littérature sur les bandits sans repos (Section~\ref{sec:restless-model}). Nous reprenons les deux modèles les plus étudiés dans la littérature : les bandits stationnaires par morceaux ($\Upsilon_T$ morceaux) et les bandits avec budgets d'évolution globaux ($V_T$ budget). On rappelle que les bornes minimax sont respectivement $\tcO\pa{\sqrt{K\Upsilon_T T}}$ et $\tcO\pa{K^{1/3}V_T^{1/3} T^{2/3}}$. À la Section~\ref{sec:restless-theory}, nous montrons que \FEWA et \RAWUCB, sans aucune modification par rapport au chapitre précédent, sont capables d'atteindre ces taux sans connaître les paramètres $T$, $V_T$ et $\Upsilon_T$. Plus important encore, ces algorithmes sont capables d'obtenir une borne de regret logarithmique dépendante du problème. Cette borne est inatteignable dans le cas ou la récompense peut croître. En effet, \citet{garivier2011upper-confidence-bound} montre que les algorithmes minimax ont une borne inférieure en $\cO\pa{\sqrt{T}}$ sur tous les problèmes stationnaires. On conclut donc que l'hypothèse de décroissance permet de simplifier les bandits sans repos.

La Section~\ref{sec:yahoo} propose une évaluation empirique sur des données réelles issues du journalisme en ligne. C'est un jeu de données très utilisé pour les problèmes de bandits non-stationnaires. Cette expérience permet de confirmer nos découvertes théoriques : le regret montre une courbe logarithmique sur les portions stationnaires du problème. Finalement, nous proposons une modélisation avec une non-stationnarité croisée sans repos et avec repos (Section~\ref{sec:general_decreasing_MAB_framework}). Cependant, comme dans le cas linéaire, les deux problèmes sont incompatibles puisque l'on peut montrer une borne inférieur $\cO\pa{T}$.

Le Chapitre~\ref{ch:pomdp} propose un problème d'exploration-exploitation assez original, intermédiaire entre le RL et les bandits. Il ne contient qu'un seul résultat technique : il s'agit de la preuve de l'optimalité de la politique oracle. Ce résultat est surprenant dans la mesure ou cette politique oracle n'utilise pas sa connaissance de l'opérateur de transition. Comme dans le cas des bandits étudiés aux chapitres précédents, seule la connaissance des valeurs courantes est nécessaire pour se comporter optimalement. Bien que ce chapitre ne contient pas d'analyse complète d'une politique apprenante, il nous paraît être une perspective prometteuse de travaux futurs.

\part{Introduction}
\input{1Literature/0Afterclasse}
\input{1Literature/1Bandits}
\input{1Literature/2ITS}

