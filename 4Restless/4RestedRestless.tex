%!TEX root = ../main.tex
\section{Restless and rested rotting bandits}
\label{sec:general_decreasing_MAB_framework}
\subsection{The general case}
\begin{assumption}\label{assum:general}
For each arm $i$, any number of pulls $n$, and time $t$, the functions $\mu_i(t,\cdot)$ and $\mu_i(\cdot,n)$ are non-increasing.
\end{assumption}

In Section~\ref{sec:restless-theory}, we highlight that the main guarantee of our algorithms - Lemma~\ref{lem:core-full} -  holds in the general case of Assumption~\ref{assum:general}. Is it enough to show that our algorithms are near-optimal in this extended setup?

Like in Chapter~\ref{ch:rested} and~\ref{ch:restless}, we define the regret with respect to the best oracle.
\begin{equation*}
R_T(\pi, \mu) \triangleq \argmax_{\pi^\star_T \in \PiO}  J_T(\pi^\star_T, \mu) - J_T(\pi).
\end{equation*}

Like in the linear rested rotting bandits (Section~\ref{sec:linear-rotting}), we can show that not only the greedy oracle suffers linear regret but no learning policy can get a sublinear regret rate in the worst-case. 

\begin{proposition}
\label{prop:general-rotting-unlearnable}
In the no noise setting ($\sigma = 0$), there exists a rotting 2-arms bandits problem (satisfying Assumption~\ref{assum:general}) with reward value in $\left[0,1\right]$, with one rested arm and one restless arm, and with at most one change-point before $T$ each, such that the greedy oracle strategy $\pi_O$ suffers a regret 
\[R_T\pa{\pi_O} \geq \floor{\frac{T}{4}}.\]
Moreover, for any learning strategy  $\pi_S$, there exists a rotting 2-arms bandits problem (satisfying Assumption~\ref{assum:general}) with reward value in $\left[0,1\right]$, with one rested arm and one restless arm, and with at most one change-point before $T$ each, such that 
\[R_T\pa{\pi_S} \geq \floor{\frac{T}{8}}.\]
\end{proposition}

Notice that the two reward functions of the constructed difficult problems are simple: either rested or restless, bounded, and with at most one break-point. If we consider a 2-arm setup with one rested arm and one restless arm, a good strategy may be to select the restless arm even when its current value is the worst. Indeed, this value is only available now, while the good value of the rested arm will still be available in the future. Whether the restless rewards are interesting to the learner depends on the future behavior of the (currently best) rested arm. On the first hand, if it decays below the current value of the restless arm before $T$ pulls, then the learner should profit from the restless reward available right now. On the other hand, if the rested arm stays optimal until the end of the game then the learner should ignore the restless arm and follows the greedy oracle strategy. However, the learner does not know in advance if (and how much) an arm will decay and any anticipation she makes will turn to be bad in the worst case. We formalize these ideas in the proof at the end of the section.

\subsection{Rested rotting bandits with a restless envelope}
\begin{assumption}
\label{assum:envelop}
We consider the following reward functions, 
\[
\mu_i(t,n) = P(t) f_i(n) + S(t),
\]
where $P: \NN^* \rightarrow \R_+$, $\left\{f_i : \NN \rightarrow \R\right\}_{i \in \arms}$ and $S: \NN^* \rightarrow \R$ are non-increasing functions. 
\end{assumption}

Notice that all the arms have the same product $P$ and sum $S$ functions, the only difference is the rested evolution $f_i$. That is why we call this setup the rested rotting bandits with a restless envelope.

With this assumption, we can show that the greedy oracle is optimal.
\begin{proposition}
\label{prop:envelop}
For any reward functions $\left\{\mu_i\right\}_{i \in \arms}$ verifying Assumption~\ref{assum:envelop} and any horizon $T$, $\GO \in \argmax_{\pi \in \PiO} J_T(\pi)$.
\end{proposition}

We leave as an open problem to analyze the aforementioned algorithms in this setup. A first step would be to characterize the performance of the greedy bandit policy in the absence of noise (as we did for the rested problem, see Subsection~\ref{ss:rested-noiseless-online}). We may not recover the $\cO\pa{K}$ bound as in the rested setup. Indeed, the adversary can use the variation of $P$ and $S$ to trick the greedy bandit policy several times for each arm. Moreover, the order of the pull do matter in this problem: the cumulative reward is not a function of $\left\{\NiT\right\}_{i \in \arms}$ anymore.

\subsection{Proofs}
\label{ss:rested-restless-proofs}
\begin{proof}[Proof of Proposition~\ref{prop:general-rotting-unlearnable}]
Let $\mu^{0}$ and $\mu^{1}$, two decreasing 2-arms bandits problems such that:
\begin{align*}
 &\mu^{0}_1(t,n) = \mu_1(n) = 1 \text{ if } n<\frac{T}{2} \text{ else } 0\,,\\
 &\mu^{1}_1(t,n) = 1\,, \\
 &\mu^{0}_2(t,n) = \mu^{1}_2(t,n) = \mu_2(t) = 1/2 \text{ if } t<\frac{T}{2} \text{ else } 0.
\end{align*}
Problem $\mu^{1}$ only evolves according to time. Hence, the oracle greedy policy $\pi_O$ is optimal for this problem and collects
\begin{equation}
\label{eq:regret1-piO}
J_T\pa{\pi_O, \mu^1} = T.
\end{equation}
On $\mu^{0}$, $\pi_O$ selects arm $1$ during $\floor{\frac{T}{2}}$ rounds and then both arms yield $0$ reward. Thus, $\pi_O$ collects 
\[J_T\pa{\pi_O, \mu^0} = \floor{\frac{T}{2}}.\]
However, let $\pi_0$ the policy which selects arm 2 for $\floor{\frac{T}{2}}$ rounds and arm 1 afterwards. Thus, $\pi_0$ collects
\begin{equation}
\label{eq:regret0-pi0}
  J_T\pa{\pi_0, \mu^0} = \pa{3/2} \floor{\frac{T}{2}}.  
\end{equation}
Hence, we conclude the first part of our proposition, 
\[R_T\pa{\pi_O, \mu^0} = J_T\pa{\pi^\star_T, \mu^0} - J_T\pa{\pi_O, \mu^0} \geq J_T\pa{\pi_0, \mu^0} - J_T\pa{\pi_O, \mu^0} \geq  \floor{\frac{T}{4}}.\]
Now, we consider any learning policy $\pi_S$ and we call $\EEempty_j\big[N_{i,t}(\pi_S)\big]$ the (expected, if the policy is random) number of pulls of arm $i$ at any round $t$ by $\pi_S$ on problem $j$. Note that the leaner will receive the same rewards for both problems until at least $\floor{\frac{T}{2}}$. Therefore, we have that 

\[ \forall t \leq \floor{\frac{T}{2}}, \pi\big(\mathcal{H}_t\pa{\mu^0}\big) = \pi\big(\mathcal{H}_t\pa{\mu^1}\big) \implies \EEempty_0\Big[N_{2,\floor{\frac{T}{2}}}(\pi_S)\Big] = \EEempty_1\Big[N_{2,\floor{\frac{T}{2}}}(\pi_S)\Big] \triangleq n_2.\]

On problem $\mu^{1}$, $\pi_S$ collects a reward of at most,
\begin{equation}
\label{eq:regret1-piS}
    J_T\pa{\pi_S, \mu^1} = \EEempty_1[N_{1,T}(\pi_S)] + \frac{n_2}{2} = T - \EEempty_1[N_{2,T}(\pi_S)] + \frac{n_2}{2} \leq T - \frac{n_2}{2}\CommaBin
\end{equation}
because $n_2 = \EEempty_1\Big[N_{2,\floor{\frac{T}{2}}}(\pi_S)\Big] \leq \EEempty_1[N_{2,T}(\pi_S)]$. Using Equations~\ref{eq:regret1-piO} and~\ref{eq:regret1-piS}, we can lower bound the regret of $\pi_S$, 
\[ R_T\pa{\pi_S, \mu^1} = J_T\pa{\pi_O, \mu^1} - J_T\pa{\pi_S, \mu^1} \geq  \frac{n_2}{2}\cdot \]


On problem $\mu^{0}$, $\pi_S$ collects a reward of at most,
\begin{equation}
\label{eq:regret0-piS}
    J_T\pa{\pi_S, \mu^0} = \min\pa{\EEempty_1[N_{1,T}(\pi_S)],\floor{\frac{T}{2}}} + \frac{n_2}{2} \leq \floor{\frac{T}{2}} + \frac{n_2}{2}\cdot
\end{equation}
Using Equations~\ref{eq:regret0-pi0} and~\ref{eq:regret0-piS}, we can lower bound the regret of $\pi_S$, 
\[ R_T\pa{\pi_S, \mu^0} = J_T\pa{\pi_O, \mu^0} - J_T\pa{\pi_S, \mu^0} \geq  \frac{\floor{T/2} - n_2}{2}\cdot \]

Hence, the worst case regret on the two setups is bounded by 
\[R_T(\pi_S) \geq \max\pa{\frac{n_2}{2}, \frac{\floor{\frac{T}{2}} - n_2}{2}} \geq \floor{\frac{T}{8}}\!\cdot \]
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:envelop}]
At any round $t$, we have,
\[
\GO(t) \in \argmax_{i \in \arms} \pa{P(t)f_i(\Nit) + S(t)} = \argmax_{i \in \arms} f_i \pa{\Nit}.
\] 
Therefore, at round $t$, collects the $t$ largest values of $\ev{f_i(n)}_{i\in \arms, n\leq T}$, \textit{i.e.} 
\[
\forall i \in \arms, \ \forall n_i \geq \Nit, \ \mu_{\GO(t)}\pa{N_{\GO(t),\,t}} \geq\mu_{i}\pa{\Nit}  \geq \mu_i(n_i).
\]

The first inequality is due to the selection rule of the policy; the second is due to the decreasing reward functions. 

A direct consequence is that, at the round $t$, $\GO$ selects the $t$-th largest value of $\left\{f_i(n)\right\}_{i \in \arms, n \leq T}$. Hence, at the round $T$, it has selected the $T$ largest value in the decreasing order. Since $P(t)$ is non-increasing and positive, an other policy which selects smaller values of $\left\{f_i(n)\right\}_{i \in \arms, n \leq T}$, or the same values but in an other order, have a smaller or equal cumulative reward than $\GO$.
\end{proof}
