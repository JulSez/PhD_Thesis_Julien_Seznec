%!TEX root = ../main.tex
\chapterimage{chapter_head/2_147romantisme.jpg} 
\chapter{Exploration in online learning}
\vspace{-2.5cm}
{\emph{Il était tard lorsque K. arriva. Une neige épaisse couvrait le village. La colline était cachée par la brume et par la nuit, nul
rayon de lumière n’indiquait le grand Château. K. resta longtemps sur le pont de bois qui menait de la grand-route au village, les yeux levés vers ces hauteurs qui semblaient vides.} \\ \vspace{-1.2cm}
\begin{flushright}
\emph{Franz Kafka,} Le Château, \emph{Chapitre Premier}.
\end{flushright}

\label{ch:exploration}
\section{The multi-armed bandits model}

The multi-armed bandits (MAB) model is a sequential decision process in which the machine learner faces many possible actions. At each round $t \in \left\{ 1, \dots, T\right\}$, it selects one of these actions $i_t \in \arms$ (also called "arms") and receives an observation  $r_{i_t, t}$ which measures the benefits of this action (also called "reward"). A common goal is to maximize the sum of the rewards collected at the last round,
\[
J_T = \sum_{t=1}^T r_{i_t,t}.
\]

In order to do so, the learner should try the different options and discover which action yields the largest rewards. The more the learner try different actions, the more they will be accurate in the future. Yet, there is an inherent cost of "trying" options. Multi-armed bandits methods focus on solving this \textit{exploration-exploitation dilemma}. 

The model was first studied in 1933 by \citet{thompson1933likelihood}. The denomination "multi-armed bandits" was coined in the 80's in reference to the slot machines. Indeed, in a casino, a gambler may face several machines and wonder which one is the most profitable. Of course, the model aims at optimizing more interesting or useful trial and error processes like clinical trials \citep{villar2015bandit}, recommender systems \citep{traca2015regulating} or intelligent tutoring systems \citep{clement2015multi, pikeburke2019phd}.

Yet, before going any further in the modelization, we should stress the assumptions that we already made. First, what we observe is connected to the action we took. In particular, we don't observe the reward associated with other actions. This is known as \emph{bandits feedback}. Second, the observation is revealed just after the action choice. Third, the observation measures how good the action is. Last, the sum of the observations is our final objective. It means that rewards are exchangeable, we can trade-off reward in the present for reward in the future.

\section{Stochastic bandits}
\label{sec:stoch-bandits}
\subsection{Regret minimization}
Up to this point, we did not precise how the environment generates rewards. A popular assumption associates each arm $i$ to a stochastic distribution with mean $\mu_i$. Each time an action is selected, the environment outputs an independent reward sample from the arm's distribution. The mean of the distribution can be seen as the intrinsic value of the action. This intrinsic value is only accessible to the learner through the noisy reward.

For instance, in clinical trials, consider many patients which are affected by the same disease. The different actions are the different drugs that could heal the patients. The goal could be to cure as many patients as possible. The learner observes if a patient heals or not.  Each drug has its own probability of success that we don't know \emph{before testing}. 

If the learner would know in advance the means, he would select the arm with the largest $\mu_i$ to maximize the cumulative reward in expectation. How the learner can compare to this oracle strategy? In order to answer to this question, we define the (expected) regret after $T$ rounds, which is the expected difference between the cumulative reward of the oracle strategy and the cumulative reward gathered by the learner,
\[
R_T(\pi) \triangleq \EE{\sum_{t=1}^T \mu_\star - \mu_{i_t}}
\]
with $\mu_\star = \max_{i\in \arms} \mu_i$. The expected regret is positive, as the oracle policy obtains the best possible performance in expectation. 

How small the regret of the learner can be? In fact, a policy that selects always arm $1$ will have zero regret as soon as arm $1$ is optimal. However, this policy suffers a regret which scales linearly with the number of rounds $T$ when arm $1$ is suboptimal. Thus, this kind of policy is not adaptive at all.  

What do we mean by \emph{adaptive}? In fact, for any arms' distributions set, we would like the policy to make fewer and fewer mistakes as it receives feedback. More formally, the regret per round should decrease with the number of rounds. Or, equivalently, the regret should scale sublinearly with the horizon $T$. A policy is called \emph{consistent} when it has a sublinear regret rate on any possible bandits game. 

What is the cost of being adaptive? \citet{lai1985asymptotically} and \citet{burnetas1996optimal} show that the expected regret per suboptimal arm $i$ for consistent policies is lower bounded asymptotically by $\cO\pa{\frac{\log{T}}{\pa{\mu_\star -\mu_i}}}$. This is the minimal cost on each bandit game to be quite good (\ie consistent) on every one. This is a \emph{problem-dependent} bound because it depends on the value of the arms' means. Later, we will describe famous policies that are proven to get this logarithmic rate (asymptotically) on each bandit game. These policies are called asymptotic optimal because one cannot get better asymptotic performance on any bandit game without suffering very large regret on another problem. 

This logarithmic rate is optimal only asymptotically. In fact, when the gap $\Delta_i = \mu_\star -\mu_i$ tends to zero, the rate diverges at finite horizon $T$. Yet, we cannot have infinite regret as we cannot do more than $T$ mistakes of size $\Delta_i$, \ie at most $T\Delta_i$ regret for arm $i$. Hence, when $\Delta_i$ tends to zero, the regret at finite-time also tends to zero. When $\Delta_i$ is large, the cost of each mistake is large, but a good learner can quickly learn from these mistakes and reach the logarithmic asymptotic regime. In between, we have difficult problems, where the learner struggles to detect significant differences between arms and yet suffers a rather large error at each mistake. 

What is the worst possible regret a good learner can get for finite-horizon $T$?  \citet{auer2002nonstochastic} give a quantitative version of the last argument. With $K$ arms, they design a bandit problem where the best arm's mean is separated from the others by a distance of $\cO\pa{\sqrt{\nicefrac{K}{T}}}$. Then, they show that this difference is small enough so the learner does not see significant differences between arms. Hence, in expectation, the optimal arm cannot be pulled a lot more than the others, which is $\nicefrac{T}{K}$ times. Thus, we do roughly $\cO\pa{T}$ mistakes of size $\cO\pa{\sqrt{\nicefrac{K}{T}}}$ in this setting, \ie a worst-case (or \emph{problem-independent}) regret rate of at least $\cO\pa{\sqrt{KT}}$. 

We will later present some algorithms which match this rate in the worst-case (with an increased constant factor compared to the lower bound). These algorithms are called minimax optimal.  The denomination "minimax" comes from game theory, where a player tries to maximize its performance knowing that its adversary will later try to minimize it. Here, the adversary is the environment, which chooses the worst possible gaps between arms. 

We presented two types of performance criteria, one which depends on the specific parameter of the bandits we are considering and the other which holds in the worst case. Another point of view is to consider the weighted average performance across multiple bandits games. The weight used in the average is called the \emph{prior} probability distribution across bandit games. This prior represents how likely a bandit game is according to our belief before the game has started. One may recognize the language of Bayesian statistics, and this objective measure is called the Bayesian regret. Bayesian regret is weaker than the problem-dependent bound in the sense that we can deduce a Bayesian regret bound from the problem-dependent bound by averaging. Also, the worst-case regret upper bounds the Bayesian regret.


\subsection{Upper confidence bound methods}
In stochastic bandits, we know that arms have intrinsic values. Each time we pull an arm, we get an observation which is useful in two different ways: first, it is an instantaneous reward; second, it brings some information about the intrinsic value of the arm. The ultimate goal is the cumulative reward the learner gathers, so we would like to estimate how much the extra information is worth in terms of future reward. With this estimation, we could estimate the value of pulling an arm by adding the reward with the value of information. 

We call \emph{index policies}, the policies which computes a value for each arm based on the arm's history and selects the arm with the largest value. The \UCB algorithm uses as index an upper confidence bound on the value of the arm. For instance, if arms are gaussians with known variance $\sigma^2$, \UCB computes the following indexes,
\begin{equation}
\label{eq:ucb}
\text{ind}(i) = \hmu_{i,t} + \sqrt{\frac{2\sigma^2 \log{\nicefrac{1}{\delta}}}{N_{i,t}}}.
\end{equation}
with $\hmu_{i,t}$ the average of the $N_{i,t}$ values of arm $i$ at each round $t$.  The average can be seen as the estimate of the instantaneous reward we should get, and the Hoeffding confidence bound term as the value we are willing to pay for the information that the $N_{i,t}+1$-th reward sample should bring. 

Yet, this estimated value depends on a parameter $\delta$: how should we tune it? It is possible to show that \UCB with $\delta = \nicefrac{1}{t}$ is asymptotic optimal in the case of gaussian arms with known variance. However, for other distributions, how should we set $\sigma$ in Equation~\ref{eq:ucb}? One possibility is to upper-bound the variance. For instance, for Bernoulli distribution, we can use Equation~\ref{eq:ucb} with $\sigma^2 = \nicefrac{1}{4}$. By doing so, we can get a near-optimal logarithmic regret rate, \ie a regret rate which is at a constant factor of the \citet{lai1985asymptotically}'s lower bound.

Indeed, upper-bounding the variance means that we "buy" new information at a higher price than what it is worth. For instance, for Bernoulli distribution with a small probability $p\sim 0$, the variance is $p(1-p) \sim p \sim 0$ which is much smaller than $\nicefrac{1}{4}$ when $p=\nicefrac{1}{2}$. \UCBV \citep{audibert2009ucbv} is an extension of \UCB which estimates the variance empirically. While \UCBV shows improved results over classical \UCB in the general case, it is not asymptotic optimal. 

In order to get the asymptotic optimal rate, we need better statistical tools. \KLUCB \citep{cappe2013klucb} uses the Kullback-Leibler divergence which measures how plausible is a distribution $p'$ given that data are generated with an other distribution $p$. More precisely, it computes as index of an arm, 
\begin{equation}
\label{eq:klucb}
\text{ind}(i) = \sup \left\{\mu \in[0,1]\ \bigg{|}\  \mathbb{KL}\left(\hat{\mu}_{i,t}, \mu\right) \leqslant \frac{\log\pa{t} + c\log\pa{\log\pa{t}}}{N_{i,t}} \right\}
\end{equation}
 The expression of the KL-divergence depends on the family of distributions which are considered. When the distributions are gaussians with fixed and known variance, \KLUCB is equivalent to \UCB. Yet, in general, the index of \KLUCB cannot be computed with a closed formula, and we need to use standard optimization software to approximate the index.

\KLUCB is asymptotic optimal but only near-minimax optimal (even for the simple gaussian bandits' case) as we can show a minimax regret rate of $\cO\pa{\sqrt{KT\log T}}$. The extra $\sqrt{\log T}$ factor has a clear interpretation: \UCB buys information at a $\cO\pa{\sqrt{\log t}}$ price. This cost will be paid off asymptotically, but at finite-time, when arms are too close to each other to be distinguished, this information is rather useless. In the early work of \citet{lai1987adaptive}, they suggest to use a refined confidence level $\delta = \nicefrac{N_{i,t}}{t}$ in the ucb such that we do not buy information for the most pulled arms. Yet, when the $K$ arms are close to each other $N_{i,t} \sim \nicefrac{t}{K}$, so we still buy information at a $\cO\pa{\sqrt{\log K}}$ cost. 

The Minimax Optimal Stochastic Strategy \MOSS \citep{audibert2009minimax, degenne2016anytime} suggests to use $\delta = \nicefrac{K N_{i,t}}{t}$. As its name suggests, \MOSS is minimax optimal. It is also asymptotic optimal for the gaussian case \citep{lattimore2020banditbook}. \citet{menard2017klucb++} suggested \KLUCBpp, an algorithm which is minimax and asymptotic optimal for many famous distributions (the single-parameter exponential family). This algorithm uses the tuning of the confidence levels of \MOSS with the KL divergence upper-confidence bound of \KLUCB.

\citet{lattimore2018refining} suggests that asymptotic and minimax optimality may not be enough. When there are many arms, but only one suboptimal arm is close to the optimal value (with a distance $\Delta$), it is effectively a two-arm bandit problem. The other arms weigh very little in terms of both regret and number of pulls (for a good policy).  Yet, \MOSS tunes $\delta$ with $K$. In particular, the exploration bonus is canceled after $\nicefrac{T}{K}$ pulls, which only guarantees (with high probability) to pull the optimal arm $\cO\pa{\nicefrac{T}{K}}$ times at the beginning of the game. By contrast, \UCB keeps exploring the two arms such that they are pulled $\nicefrac{T}{2}$ at the beginning of the game. During this starting phase, the two arms' values are not well identified by the algorithm, and the expected regret is linear. This linear phase ends once each arm has been pulled $\cO\pa{\nicefrac{1}{\Delta^2}}$, hence it is $K$ times longer for \MOSS than for \UCB. In fact, at the end of this phase for \MOSS, its expected regret is $K$ times larger than \UCB. That is why \citet{lattimore2018refining} suggests the sub-UCB criteria, which ensures that the policy is at a constant factor of the performance of \UCB at any round $t$. He also suggests the policy \ADAUCB, which computes for each arm the number of other arms that are "competing" with this arm, and they plug this number instead of $K$ in the confidence level tuning. \ADAUCB is proven to be sub-ucb, asymptotic optimal, and minimax optimal. 

We have discussed how optimistic strategies based on upper-confidence bound indexes can achieve multiple optimality criteria. However, the main advantage of \UCB could be its simplicity. Indeed, it is a deterministic algorithm, that is, an algorithm that outputs always the same action given the same data. Arguably, this is a desirable property for explainability as well as for an implementation purpose. It is worth noticing that one of the most quoted paper \citep{auer2002finite} in the bandit literature studies a suboptimal version of \UCB (namely \UCBone)  with $\delta = \nicefrac{1}{t^4}$. It gives a simple proof that leads to a finite-time and problem-dependent regret bound which holds with high-probability and from which we can derive near-optimal minimax and asymptotic bounds. From a research perspective, this simplicity is desirable as it gives a simple starting point when one studies a more complex setup than the stochastic stationary multi-armed bandits.


\subsection{Bayesian methods}
\label{ss:bayes}

In his early work, \citet{thompson1933likelihood} suggests pulling an arm according to its probability of being the best given the data. It is difficult to compute this probability directly. Hence, we compute for each arm the probability of the parametric distribution beyond it given the data and a prior. Then, we sample a model for each arm according to this distribution and we select the arm with the best mean according to this sampling. This procedure is known as Thompson Sampling (\TS).

Though \TS is very old, it was only shown recently \citep{kaufmann2012ts, agrawal2013finite} that it is asymptotic optimal (when it is fed with an uninformative prior). Borrowing the idea of canceling the exploration for arms with $N_{i,t} = \nicefrac{T}{K}$ from \MOSS, \citet{jin2020mots} suggested the Minimax Optimal Thompson Sampling (\MOTS) which clipped the posterior distribution at a quantile $\delta = \nicefrac{T}{KN_{i,t}}$. \MOTS is minimax and asymptotic optimal. 

\BayesUCB \citep{kaufmann2012bayesian} is another asymptotic optimal Bayesian algorithm. It computes an optimistic index based on an optimistic quantile of the posterior. \BayesUCB and \TS have empirical performance very similar to \KLUCB.

The posterior distribution can sometimes be computed explicitly, for instance with the Beta distribution for Bernoulli reward. When it is not possible, one can use Markov-Chain Monte-Carlo (MCMC, \citet{andrieu2003introduction}). This technique can sample from a probability distribution $p$, if we know the probability ratio $p(x)/p(y)$ for all $x$ and $y$. Indeed, when we use the Bayes rules, we often have an unknown normalization factor which can be hard to compute.
 

\section{Adversarial bandits}
\label{sec:adv-bandits}
\subsection{Pseudo-regret}
Another popular assumption is to consider the environment fully adversarial \citep{auer2002nonstochastic}, which means that rewards are generated by an adversary who wants to maximize our regret. But how do we define the regret in this setting? In adversarial bandits, it is not possible to compete with the oracle who would know in advance what reward is beyond each arm at every step. Indeed, let's consider an adversary who rewards one arm uniformly at random at every step, and set the reward of the other arms to zero. An oracle can select the right arm at every step, but a learning policy can only try to guess what is the right arm. "Guessing" an independent random variable cannot be improved with past feedback (by definition of independent), and hence the learner suffers a linear regret rate compared to the best possible sequence.

Thus,  we will target a more reasonable objective: we will compare to the best arm in hindsight, \ie we take as reference the best policy (for this reward sequence) among the ones which select always the same arm. Formally, with $r_{i,t}$ the reward of arm $i$ at each round $t$, we define the pseudo-regret,
\[
R_T(\pi) = \max_{i \in \arms} \left( \sum_{t=1}^T  r_{i,t} - r_{i_t,t}\right).
\]

The adversarial multi-armed bandit may look much harder than the stochastic bandits due to the latitude the adversary has to trick us. However, \citet{auer2002nonstochastic} have designed \EXP (Exponential weight for exploration-exploitation), an algorithm with a proven worst-case regret upper bound of $\cO\pa{\sqrt{KT\log\pa{K}}}$. This rate was further refined by \INF \citep{audibert2009minimax} to $\cO\pa{\sqrt{KT}}$. It shows that stochastic bandits are not much easier than adversarial bandits from the minimax perspective. More recently, \citet{zimmert2018tsallis} designed a variant \TsallisINF which is minimax optimal in both adversarial and stochastic settings and near-asymptotic optimal in the stochastic setting. They also show relevant results in intermediate settings. It tends to show that we can have simultaneously the best of both worlds (without knowing in advance in which world the learner is).  Yet, we emphasize that \TsallisINF is not completely asymptotic optimal as it does not recover the right multiplicative constant in the regret rate. 

The adversarial bandit framework is a bit odd: on the one hand, the learner tries to compare to the best arm in hindsight; on the other hand, there is no mechanism beyond the reward generation of each arm which guarantees any coherence in the sequence. Let's go back to the casino: if the gambler acknowledges that slot machines are just some black boxes the casino uses to diminish its performance, why would they care about comparing to the best machine in hindsight? 

There is no fully satisfying answer to this question. An important point is that the learner has to believe in something (because they will suffer linear regret in the worst-case if they compare to any possible sequence of actions), and the meaning of this belief is not included in the model. A popular extension of the adversarial bandits computes the regret against the best policy in a predefined set of $E$ experts.  \citet{auer2002nonstochastic} suggests \EXPfour which is proven to achieve a regret rate of $\cO\pa{\sqrt{\min\pa{K, E} T\log E}}$. Notice the logarithmic dependence with the number of experts: we can have a rather high number of experts, but if we consider all the possible sequences of choices, \ie $ E = K^T$, the upper bound rate becomes linear with $T$. 

The learner may believe that there is an inner mechanism beyond each arm, such that it makes sense to compare to the best arm. In an old-time casino, each machine may have an independent non-stochastic mechanism such that one is more rewarding than the others. Yet, the mechanism may be complex to model and the learner may be lazy and assume the reward adversarial. The aforementioned "best of both worlds" results may encourage him in that way. However, one should be cautious: low regret compared to bad policies can mean low reward. For instance, if the arms have periodic and synchronous rewards (the reward of arm $1$ is low when the reward of arm $2$ is high) competing against the best fixed-arm policy may be much less rewarding than competing against experts which are aware of the periodicity.


\subsection{Adversarial methods}

Adversarial games are very different from stochastic games. In the stochastic setting, when we observe the reward for all the actions (a.k.a the full information setting), the learner can follow the actions with the largest current average reward. Indeed, the learner does not need to explore like in the bandit setting, and \emph{Follow the Leader} (\FTL) is guaranteed to do less than a constant regret (with respect to $T$). Yet, in the adversarial full-information setting, \FTL suffers a linear regret. Indeed, the adversary can alternate the reward between two arms such as the current "leader" is never rewarded. 

In fact, in the adversarial setting, every deterministic policy (like \UCB) would fail because a good adversary may know our strategy. Hence, it can set to zero the reward of the action we select. Hence, we need to design probabilistic strategies that output a probability distribution across actions. We already presented \TS, a probabilistic policy. Yet, this policy suffers linear regret in the adversarial setting. Indeed, it is fairly easy to trick optimistic strategies: during the first quarter of the game, we may reward only one arm such that an optimistic stationary policy is very confident that it is the best arm. Then, the adversary can increase the reward of another arm. This arm will be pulled only at a logarithmic pace and even when it is pulled the high reward will be averaged with older lower rewards such that it will take a very long time to realize that something has changed. Recently, \citet{zimmert2018tsallis} empirically show that \TS suffers near-linear regret even in an intermediate setup called "stochastically constrained adversarial regime". In this setup, the rewards are generated stochastically but the probability distributions beyond arms change a few times during the game without changing the best arm identity. Once again, the key is to exploit the "inertia" of this stationary bandit policy, which average rewards from different distributions.

In adversarial games, the output probability distribution needs to take into account the data while being sufficiently unpredictable for the adversary. This is the spirit of the Follow the Regularized Leader (\FTRL) policy. This full-information policy selects the probability distribution which maximizes the expected performance (according to the current data) plus a regularization term that penalized probability distributions that are too concentrated. More formally, with $p_t$ the output probability distribution on arms at each round $t$, $D_t$ the sum of the observed reward for each arm at each round $t$, and $L$ a regularizing function,

\begin{equation}
\label{eq:ftrl}
p_t \in \argmax_p \left\{ < p | D_t > + L(p)\right\}.
\end{equation}

In the bandit setting, we do not have access to $D_t$, the sum of the reward for each arm from the beginning of the game to round $t$. We can estimate $D_t$ with importance weighted estimator, that is, we add $\hat{r}_{i,t} = \mathbbm{1}\left[i_t =i \right] \nicefrac{r_{i,t}}{p_{i,t}}$ to the sum at each round. This quantity is equal to zero for all the arms which are not selected and for which we don't know $r_{i,t}$. For the arm which is selected, the reward $r_{i,t}$ is normalized by the probability of selecting the arm. This weighting strategy is unbiased in the sense that $\EE{\hat{r}_{i,t}} = r_{i,t}$ (the expectation is taken on the algorithm randomization conditionally on the observed history before round $t$).

This estimator is unbiased but has a large variance when $p_{i,t}$ is small and $r_{i,t}$ is large. Indeed, in this case, $\hat{r}_{i,t}$ will have a very different value depending on whether we pull arm $i$ at the round $t$ or not. This variance will be transmitted to $\hat{D}_{t,i} = \sum_{s=1}^t \hat{r}_{i,s}$ that we want to use instead of $D_{t,i}$ in Equation~\ref{eq:ftrl}. It means that when we observe a good reward for an arm that is pulled with low probability, it can squash all the other probabilities to almost zero. Then, the algorithm may never recover because it will keep selecting this arm and adding a positive weighted reward to $\hat{D}_{t,i}$. Yet, if the algorithm did not pull the arm $i$ at the round $t$ in the first place, it would have very different behavior for the same data sequences generated by the adversary. 

The solution is to work with losses instead of rewards. We can define the losses $l_{i,t} = 1 - r_{i,t}$, the importance-weighted estimator of the losses $\hat{l}_{i,t}  = \mathbbm{1}\left[i_t =i \right] \nicefrac{l_{i,t}}{p_{i,t}}$ and the estimated sum of reward $\hat{D}_{t,i} = \sum_{s=1}^t 1 - \hat{l}_{i,s}$. In that case, the variance can also be high but when $p_{i,t}$ is small and $r_{i,t}$ is small. If we select arm $i$ at a round $t$, $\hat{l}_{i,t}$ will be very large and it will reduce $\hat{D}_{t,i} = \sum_{s=1}^t 1 - \hat{l}_{i,s}$. Hence, according to Equation~\ref{eq:ftrl}, it will squash $p_{i,t+1}$ to zero. This is arguably better for the stability of the algorithm than squashing all the other probabilities to zero. Moreover, arm $i$ may recover after few rounds because $\hat{D}_{t,i}$ is increased by $1$ every time arm $i$ is not pulled. 

Up to this point, we did not precise what regularizer $L$ we should use. A good $L$ will penalize probability vectors which are too predictable. In information theory, a classical measure of how predictable is a probability distribution is its (Shannon) entropy: $- \sum_{i \in \arms} p_i \log\pa{p_i}$. The larger is the entropy the more unpredictable it is. Hence, we could use the negentropy as a regularizer. 

We now have all the ingredients beyond the aforementioned \EXP algorithm \citep{auer2002nonstochastic}. \EXP is equivalent to \FTRL (see Equation~\ref{eq:ftrl}) where we use the loss-based importance weighted estimator $\hat{D}_{t,i} = \sum_{s=1}^t 1 - \hat{l}_{i,s}$ and the unormalized negentropy as regularizer $F(p) = \sum_{i \in \arms} p_i \log\pa{p_i} - p_i$. Notice that with this regularizer, there exists a closed-form formula for $p_t$ instead of the implicit formulation in Equation~\ref{eq:ftrl}. This expression is useful for implementation but it hides the main ideas beyond \EXP.

Most of the recent adversarial algorithms use slight (but powerful) modifications of the aforementioned ideas. For instance, we already advertised $\TsallisINF$ \citep{zimmert2018tsallis}, which improves over \EXP from the minimax adversarial perspective and recovers logarithmic asymptotic bound for the stochastic stationary bandits' case. \TsallisINF uses Online Mirror Descent (\OMD) instead of \FTRL. Without going into the details, the two algorithms share deep similarities. In fact, they are even equivalent for some regularizers \citep{mcmahan2011ftrl}. They also use a different regularizing function known as Tsallis entropy \citep{tsallis1988possible} and they finally discussed another unbiased estimation scheme of the losses.

\section{Non-stationary bandits}
\label{sec:non-stationary}
Since the early stages of the research in bandits \citep{thompson1933likelihood,whittle1980multi}, one of the most desirable properties for a learner would be to adapt to actions whose \textit{value changes over time} \citep{whittle1988restless}, as it happens in non-stationary environments. In fact, from applications in medical trials (where the patient can become more resistant to antibiotics) to a modern applications in recommender systems \citep{chapelle2011empirical,traca2015regulating}, assuming that the environment is \textit{stationary is very limiting}. 

In the adversarial bandit setting, rewards do not have to be generated by a stationary stochastic process. However, the objective is strongly stationary as the pseudo-regret definition competes against a \emph{fixed} set of policies (e.g. the stationary policies which select always the same arm). As in stationary bandits, we would like to define the regret against the best oracle, or, at least, a good enough one. Indeed, depending on the non-stationarity, it can be challenging to compute the best oracle (also called the offline policy), especially when the choice of the learner impacts the non-stationarity. 

With bandit feedback, it can be meaningful to assume that the arms evolve only when they are pulled. In that \emph{rested} case, the learner observes (often with noise) every value. \citet{bouneffouf2016multi-armed} consider the case where all the arms are evolving with a known trend which depends on the number of pulls. They compare to the greedy oracle which selects the largest available reward at each round. They design a variant of \UCB which uses as index the product of the classical ucb index by the trend. This algorithm achieves a logarithmic asymptotic bound similar to \UCB's on stationary bandits. \citet{heidari2016tight} consider the two monotonic rested cases without noise in the observation. \citet{levine2017rotting} consider the parametric and non-parametric decreasing (or rotting) rested case with noise. We give a detailed review of their result on the non-parametric rotting case in Chapter~\ref{ch:rested}. 

In the \emph{restless} setting, the arms can evolve even when they are not pulled. Hence, the learner does not know the last reward state beyond each arm. \citet{whittle1988restless} first consider a very general restless setting: arms are associated with Markov chains with different transition probabilities depending on whether an arm is selected. While \citet{whittle1988restless} suggested a heuristic known as the \emph{Whittle's index} policy, the restless bandits problem was later shown to be \texttt{PSPACE}-hard even to approximate \citep{papadimitriou1994complexity}. 

Assuming that the transition probabilities do not depend on the action of the user simplifies the restless setup. Indeed, in that case, the optimal oracle is straightforward: one should pull the arm with the current largest expected reward. When the evolution is deterministic, \ie the Markov chains are replaced by functions of the round, it is possible to approximate this optimal oracle with an online bandit policy when the changes are either not too frequent \citep{garivier2011upper-confidence-bound} or not too big \citep{besbes2014stochastic}. We give a detailed literature review of the restless bandits with independent evolution in Chapter~\ref{ch:restless}.

While the general restless bandits are unlearnable, some authors studied some specific instances of the restless bandits. For instance,  \citet{immorlica2018recharging, pikeburke2019recovering, cella2020stochastic} studied different models of recharging bandits, where arms' rewards decrease when they are selected, and increase back when the arm has not been pulled for a while. In these problems, the optimal oracle policy for the full horizon regret is hard to compute and the authors often consider approximated oracle policies \citep{immorlica2018recharging, cella2020stochastic} or weakened regret definition \citep{pikeburke2019recovering}.

\section{Contextual bandits}
\label{sec:contextual}
The contextual bandits framework \citep{tewari2017contextual} assumes that at each round contextual information is given to the learner. The reward is associated with the action and context such that an action can be good for a given context and bad in another one. Of course, if we have to explore from scratch every time we receive a new context, it can be quite expensive. 

A classical assumption is that actions and contexts can be embedded in a vector space such that the reward is smooth enough in that space - e.g. it is a linear form \citep{abe1999associative, auer2002using, abbasi2011improved, lattimore2017end} though more complex structures were also considered \citep{filippi2010parametric, valko2013finite, valko2014spectral}. It allows for a potentially infinite number of contexts and actions, as long as there is a finite number of unknown parameters that determine the rewards. Interestingly, while optimistic strategies were shown to perform quite well in this setting \citep{abbasi2011improved}, \citet{lattimore2017end} recently advocates that they could not reach asymptotic optimality, unlike in the multi-armed bandits setup. 

\section{Beyond bandits: Reinforcement Learning}
\label{sec:rl}
Reinforcement learning \citep{sutton1998book} extends the former \emph{contextual bandits} so that the context (renamed "state") is controlled by the learner through the actions. The goal is not only to find and exploit the function which relates states and actions to rewards but also to discover the relation between actions and states.

Formally, the Markov Decision Process (MDP, \cite{howard1960dynamic}) models this situation as a quadruplet $\left\{\mathcal{S}, \mathcal{A}, \mathcal{T},  \mathcal{R}\right\}$ where $\mathcal{S}$ is the states space, $\mathcal{A}$ the actions space, $\mathcal{T}(s, a)$ the transition operator which associates an origin state and an action to a probability density over the destination states, $\mathcal{R}(s, a ,s')$ which associates a probability density over the reward to a transition from $s$ to $s'$ after choosing action $a$. It is often easier and more meaningful to consider the discounted cumulative reward rather than the cumulative reward at finite-horizon $T$. Indeed, in some setups, the termination rule may not be known and the learner may discount the reward to take into account the probability of termination.

The exact state of the learner may be observed partially. For instance, the knowledge of students revising on an intelligent tutoring system is not directly observable: each answer gives only limited information about what they know or do not know. In order to model this situation, Partially Observable Markov Decision Process (POMDP, \cite{astrom1965optimal}) adds a set of observation ($\Omega$) and a probability distribution over this observation set for each states-action transition ($\mathcal{O}(s,a,s')$).

% Planning
Unlike in stationary bandits, finding an optimal policy when we know the MDP parameters is not straightforward. Dynamic programming \citep{bellman1966dynamic} is a general method which uses a recursive relation -  the Bellman equation - on the state value, which is the cumulative value that the agent can expect in a given state if s/he follows a given policy.  

%Learning criteria
Reinforcement learning aims at finding the optimal policy when the reward function $\mathcal{R}$ and transition probabilities $\mathcal{T}$ are not known. There are several quantitative objectives associated with the maximization of the reward. As in the bandit case, we can define the regret for the optimal policy. However, this objective is ambitious in RL as in some problems a single mistake can send irreversibly the learner in a sub-optimal region of the state space. A weaker objective is to minimize the sample complexity, which is the number of rounds after which the policy behaves near optimally with high probability. 

%Limitation for bandits
The framework of RL models much more complex situations than bandits. However, it is possible to adapt the \UCB strategy and its optimistic paradigm for RL. \UCRLtwo \citep{auer2009near} selects the most optimistic model in a confidence region built around the empirical means and then uses classical dynamic programming method to get the optimal policy. Then, it runs this policy for a while and restarts the procedure. Being optimistic about the transition probabilities is not as straightforward as it is for the reward parameters. Indeed, for the reward, we can simply increase the reward with the confidence bound. For the transition, we cannot simply increase each transition, because, 1) the probabilities would not be normalized; and 2) increasing the probability to reach a low reward region of the state space is a pessimistic choice. Yet, we can find with an optimization software (with complexity $\cO\pa{|\mathcal{S}|}$) an optimistic MDP whose transition probabilities lies within a confidence band around the empirical average and maximize the reward that the learner can get given the optimistic estimation of the reward function. If the diameter of the MDP is finite - that is, one can (with the right policy) reach any state from any other state in a finite expected number of rounds - \UCRLtwo recovers near-optimal regret bounds. 

\UCRLtwo models the environment by fitting the environment (that is, $\mathcal{R}$ and $\mathcal{T}$). This type of method is called \emph{model-based} RL. By contrast, model-free reinforcement learning directly tries to fit the policy without modeling the environment. For instance, \QLearning \citep{watkins1989learning} plays a behavior policy and tries to measure the total values that the learner can get from selecting each action in each state (the q-value function). At the end of the learning phase, it outputs a target policy that selects in each state the action with the largest q-value. The fact that the policy which is used in the learning phase differs from the output one is called \emph{off-policy learning}. The value of a state-action pair is the sum of the expected instantaneous reward - for which we get a noisy yet objective sample - and the value of the destination state when we play the optimal policy. Of course, the optimal policy is unknown but we can approximate the aforementioned value by considering the maximal state-action value we have estimated for the destination state. The fact that we reinforce our estimated values with other estimated values is called \emph{bootstrapping}. Notice that at the beginning of the learning, the values are just a random guess, and bootstrapping may propagate the error to other nodes. Yet, the q-values converge with high probability under mild conditions on the learning rate \citep{watkins1992q}.

There exist many other policies than the two we have quickly described. Yet, none of them can learn anything but toy models without extra assumptions. Indeed, there are a priori $|\mathcal{A}||\mathcal{S}|^2$ transition parameters and $|\mathcal{A}||\mathcal{S}|$ reward parameters. When the state space is not very small, it is much more than the $K = |\mathcal{A}|$ reward parameters in the $K$-armed bandit case ($|\mathcal{S}| = 1$). Hence, it will take thousands of rounds to \UCRLtwo to get a basic understanding of a fairly small environment with ten states and ten actions. The problem is even worse for \emph{model-free} methods like \QLearning. Indeed, model-based methods use every sample to estimate the model. In model-free learning, samples are forgotten either because they were only used to evaluate one policy (\emph{on-policy} learning) or because bootstrapping updates by using the (inaccurate) current belief. Hence, they need multiple visits of each state-action pair to converge. 

Deep Reinforcement Learning tries to mitigate this issue by using deep neural networks to generalize the experience the learner obtains. For instance, Deep Q-network \citep{mnih2013playing} uses deep networks to learn the q-value function. Yet, using supervised predictive methods in an online active environment is nothing but straightforward. Indeed, in the supervised learning setting, we learn a function that maps observations $X$ to results $Y$. The way $X$ is generated is assumed to be stationary between the training and production phases. Moreover, $Y$ is assumed to be an objective value that is given to the learner. In the online setting, the observations $X$ are heavily dependent on the policy which is played. With off-policy learning methods, if the policy which is used in the learning phase output a very different $X$ proportion than the optimal policy, then it will bias the neural network. On the other hand, with bootstrapping, the target values $Y$ do not correspond to purely objective values. Indeed, they are constructed using the current belief of the model on the value destination state. The combination of bootstrapping, off-policy learning, and (supervised) function approximation was called \emph{the deadly triad} because it can lead to unstable algorithms that do not converge to the optimal policy.

Surprisingly, using sophisticated deep networks instead of more classical and simple supervised models leads to more stable algorithms. Indeed, deep networks are trained with mini-batches: we only use a subset of the data to estimate the network's parameters gradient. This is arguably a key feature for online learning applications where incoming data are natural mini-batches for continuous training. However, this feature alone is not sufficient to solve the whole deadly triad problem. 

We will not review in detail all the ideas (experience replay, double-Q-network ...) which have improved the stability of DRL methods. Yet, we advertise that this line of work led to superhuman performances in many complex games such as the board game of Go \citep{silver2016mastering, silver2017mastering} or Starcraft II \citep{vinyals2019alphastar}. It shows that given a potentially infinite source of data - and enough computational power to process it - DRL methods can learn very complex tasks. However, these methods are still sample-inefficient: AlphaZero \citep{silver2017mastering} played several million games before reaching superhuman performance in both Chess and Go. For many real-life applications, one may not have a simulator that can produce a tremendous amount of accurate and cheap data.

Improving the sample efficiency is a hot research topic \citep{yu2018towards, yarats2019improving}, and there exist more efficient methods than the ones which have been designed for applications with accurate simulators. However, one should notice that the interaction of planning and exploration makes the methods much more data-intensive than in bandits.  In a small data situation, it is preferable to frame a given problem as a bandit than to rely on the too general reinforcement learning paradigm.