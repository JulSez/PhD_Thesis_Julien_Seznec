%!TEX root = ../main.tex
\part{Rotting bandits}
\section*{Decreasing reward}
In Subsection~\ref{ss:less-known}, we presented a line of work that aims at asking questions from the least known subject to a student. In the multi-armed bandits' formulation, it associates positive reward to failed questions. Yet, none of these works consider the impact of the questions on the knowledge of the student. When the answer is given to the student after her trial, questions are a powerful learning tool. Therefore, the more the student work on a topic, the better he becomes, and the smaller is the reward for this topic.

Other situations can be modeled with decreasing rewards caused by the repetition of an action. For instance, the more we recommend an item to a user in a recommender system, the more he might get bored \citep{warlop2018fighting}. In medicine, the efficiency of antibiotics is diminishing with the overall use due to bacteria's mutation \citep{ventola2015antibiotic, ventola2015antibiotic2}.

In microeconomics, the law of diminishing marginal utility states that the utility associated with each unit of goods is decreasing with the number of goods a consumer holds. It is an \emph{ad hoc} explanation to justify that rational consumers, who maximize their total utility, may select different goods. In production theory, the law of diminishing returns \citep{canan1892origin} states that the increment of production caused by the increment of a factor of production (labor, capital) by one unit is decreasing. Again, there is the idea that repeating always the same action - buying one good, investing in a project - may become suboptimal even though the returns were high at the beginning. 

Motivated by these broad applications, \citet{heidari2016tight, levine2017rotting} study this non-stationarity with bandits feedback.  \citet{heidari2016tight} study the case where the rewards are directly observed without noise under the name \emph{decaying bandits}. \citet{levine2017rotting} study the problem with noisy rewards under the name \emph{rotting bandits}. In the following, we call this problem the \emph{rested rotting bandits} to emphasize that actions cause the rewards' decay. We also mention the works of \citet{warlop2018fighting, immorlica2018recharging, pikeburke2019recovering} which model boredom effects in recommender systems as a rested decaying bandit problem but with restless recharging effects. 

In Chapter~\ref{ch:rested}, we synthesized our contributions to the rested rotting bandits problem \citep{seznec2019rotting, seznec2020single}: we present new algorithms and we prove that for an unknown horizon $T$, and without any knowledge on the decreasing behavior of the $K$ arms, these algorithms achieve problem-dependent regret bound of $\tcO(\log{(T)}),$ and a problem-independent one of $\tcO(\sqrt{KT})$. Our result substantially improves over the algorithm of~\citet{levine2017rotting}, which suffers regret $\tcO(K^{1/3}T^{2/3})$. These bounds are at a polylog factor of the optimal bounds on the stationary problem; hence our conclusion: rotting bandits are not harder than stationary ones. 

Another decaying setup is when the reward decreases no matter what the agent is doing. It models different situations such as the aging of content in recommender systems. \citet{louedec2016algorithme} models obsolescence of appearing arms (e.g. piece of news) with a known exponential rate. \citet{komiyama2014time-decaying} study a parametric decay in restless bandits where rewards are linear combinations of known decaying functions. However, the rotting assumption was not studied in the well-studied non-parametric restless bandit setting\citep{garivier2011upper-confidence-bound, auer2019adaptively,chen2019new, cheung2019new, russac2019weighted, besson2019generalized, liu2018change-detection, cao2019nearly, besbes2014stochastic}. That is why we consider the  \emph{restless rotting bandits} problem in Chapter~\ref{ch:restless} which is adapted from \citet{seznec2020single}.  We show that the rotting algorithms designed for the rested case match the problem-independent lower bound and a problem-dependent $\cO(\log{T})$. The latter was shown to be unachievable in the general case where rewards can increase. We conclude: the rotting assumption makes the restless bandits easier. 

Since the same algorithms work in both setups, we investigate in Section~\ref{sec:general_decreasing_MAB_framework} the joint setup where the reward can decrease with the number of pulls and the rounds. Yet, we show that the optimal oracle policy cannot be approached at a nontrivial rate by a learning policy.

\chapterimage{chapter_head/4_94tag.jpg} 
\chapter{Rested rotting bandits are not harder than stationary ones}
\label{ch:rested}
\vspace{-2.8cm}
\begin{flushright}
\emph{This rested rotting bandit seems quite stationary to me.}
\end{flushright}
\vspace{.85cm}
\blfootnote{Chapter header: Photo of a tag by Nemo, Paris, Rue Le Brun.}
\input{2.1Rested/2Model}
\input{2.1Rested/3Algorithm}
\input{2.1Rested/4Theory}
\input{2.1Rested/5Simulation}
\input{2.1Rested/6Efficient}
\input{2.1Rested/7HowHarder}
\input{2.1Rested/8Linear}
