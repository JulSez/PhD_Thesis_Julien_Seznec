%!TEX root = ../main.tex 

\section{Linear rotting bandits are impossible to learn}
\label{sec:linear-rotting}
\subsection{Linear bandits}
\label{ss:linear}
In Section~\ref{sec:contextual}, we presented the contextual bandit, a line of work in which the learner is given a context on which depends the action's reward. The linear bandit is a special case where the reward is a noisy linear form of a context-action vector.

Unlike the classical multi-armed bandits, the feedback associated with one action can be used to learn the other actions' efficiency. In fact, the number of actions can be very large or even potentially infinite, as soon as the representation has a finite dimension. 

This shared knowledge is interesting in the context of education. Indeed, a popular model to predict a student's answer is the Item Response Theory \citep{hambleton2013item}. It models the answer with a logistic function filled with a student and question-related parameters. In its most simple form, it can be seen as logistic regression, a generalized linear model. Generalized linear bandits were studied by \citep{filippi2010parametric} as an extension of the linear bandit.  

In the following, we will introduce formally the linear bandit. We will then discuss the possibility of extending our multi-armed rested rotting bandits result in the linear case. 



\subsubsection{Model} At each round, the learner chooses an action $A_t$ in a fixed set of embedded actions $\mathcal{A} \subset \R^d$ and receives,
 \begin{equation*}
\label{eq:linear-feedback}
o_{t} \triangleq \langle \mu | A_t \rangle + \noise_t
 \;\; \text{with}\; \EE{ \noise_t | \historyt }= 0 \;\; \text{and} \; \forall \lambda \in \R, \; \EE{ e^{\lambda\noise_t}} \leq e^{\frac{\subgaussian\lambda^2}{2}},
\end{equation*}
with $\mu \in \R^d$ a reward vector unknown to the learner. In this model, the learner might face a huge number of actions (or even infinite), but they only has $d$ independent parameters to estimate to take the right decision.  The goal of the learner is still to maximize the cumulative reward, 
\begin{equation*}
J_T(\pi) = \sum_{t=1}^T \langle \mu | \pi(t) \rangle. 
\end{equation*}
The optimal oracle strategy selects $\pi^\star(t) = A^\star \triangleq \argmax_{A_t \in \mathcal{A}} \langle \mu | A_t \rangle$. Thus, we can define the regret, 
\[R_T(\pi) = J_T(\pi^\star) - J_T(\pi) = \langle \mu | \sum_{t=1}^T A^\star - \pi(t) \rangle. \]

\subsection{Linear rested rotting bandits}
We want to extend the rested rotting bandit to actions with a linear embedding. We introduce $d$ non-increasing and $L$-Lipschitz functions $\mu_i : \R^+ \rightarrow \R$. While there were $K$ reward functions defined on $\NN$ in the rotting MAB model, we now have $d$ functions defined on $\R^+$. Indeed, we now have $d$ (and not $K$) reward parameters. Moreover, for the rested rotting MAB, the reward is evolving according to the number (in $\NN$) of pulls of arm $i$. In the linear model, we cannot simply count the number of pulls along each direction because $A_t$ has possibly components along all the directions. We will need to find a quantifier of the pulling intensity along direction $i$. This is not surprising that this quantifier will have value in $\R^+$ because we select direction $i$ with intensity $A_{t,i} \in \R$. We suggest using, 
\[
N_{t,i} \triangleq \sum_{t'=1}^t A_{t',i}.
\]

Hence, we define the feedback as, 
\[
o_t = \sum_{i\leq d} \int_{N_{t-1,i}}^{N_{t,i}} \mu_i(x)dx +\noise_t  = \int_{N_{t-1}}^{N_{t}} \langle \mu(n) |   dn \rangle  + \noise_t,
\]
with $\{\noise_t\}$ a sequence of independent $\sigma$-subgaussian random variables. We also define the cumulative reward, 
\[
 J_T(\pi) = \int_{0}^{N_{T}} \langle \mu(n) |  dn \rangle.
 \]

Like in the rotting MAB model, the total reward depends only on the cumulative pulling intensity $N_{T}$. This property is very useful, as it allows to compare the performance of two policies only with their pulling differences (the overpulled/underpulled arms) and not with the specific order of the pulls, \ie, 
\[
 J_T(\pi_2) - J_T(\pi_1) = \int_{N_T^{\pi_1}}^{N_{T}^{\pi_2}} \langle \mu(n) |  dn \rangle.
\]

Last, we restrain the action vector to be in the positive quadrant, \ie $\mathcal{A} \subset \R^{+d}$. Indeed, we want the pulling intensity to be non-decreasing with time $t$ such that the reward associated to a vector $A\in \R^{+d} $ is non-increasing, 

\[
t_1 < t_2 \implies  N_{t_1-1} \leq N_{t_2-1}  \implies \int_{N_{t_1-1}}^{N_{t_1-1} + A} \langle \mu(n) |   dn \rangle \geq \int_{N_{t_2-1}}^{N_{t_2-1} + A} \langle \mu(n) |   dn \rangle.
\]

This model correctly extends linear bandits and rested rotting bandits. On the first hand, when reward functions are constant, we recover the linear bandit model of Subsection~\ref{ss:linear}. Indeed, we have,
\begin{align*}
&o_t =  \int_{N_{t-1}}^{N_{t}} \langle \mu |   dn \rangle  + \noise_t =  \langle \mu | N_t - N_{t-1} \rangle  + \noise_t = \langle \mu | A_t \rangle  + \noise_t,\\
& J_T(\pi) = \int_{0}^{N_{T}} \langle \mu |  dn \rangle = \langle \mu | N_T\rangle =  \sum_{t=1}^T \langle \mu | A_t \rangle .
\end{align*}


On the other hand, when the actions sets $\mathcal{A}$ are the $d$ canonical basis vectors at every round, we recover the rested rotting bandits setting of Section~\ref{sec:rested-model}. Indeed, if we call $\mu_i^{MAB}(n) \triangleq \int_{n}^{n+1} \mu_i(n) dn$, we have,
\begin{align*}
&N_{t,i} = \sum_{t=1}^T \mathbbm{1}(\pi(t) = i),\\
&o_t =  \int_{N_{t-1}}^{N_{t}} \langle \mu(n) |   dn \rangle  + \noise_t = \mu_{i_t}^{MAB}(N_{t-1, i_t}) + \noise_t,\\
& J_T(\pi) = \int_{0}^{N_{T}} \langle \mu(n) |  dn \rangle = \sum_{i\leq d} \sum_{n=0}^{N_{T, i}-1} \mu_{i}^{MAB}(n).
\end{align*}

To conclude the argument, we notice that the constructed functions $\mu_i^{MAB}$ are in $\rewardSet$ (see Definition~\ref{def:rew-bounded-decay}). Indeed, 
\begin{align*}
&\mu_i^{MAB}(n) -\mu_i^{MAB}(n+1) = \int_{n}^{n+1} \pa{\mu_i(x) - \mu_i(x+1)} dx \geq 0,\\
&\mu_i^{MAB}(n) -\mu_i^{MAB}(n+1) = \int_{n}^{n+1} \pa{\mu_i(x) - \mu_i(x+1)} dx \leq L.
\end{align*}
The first line follows by the non-increasing property of $\mu_i$. The second line is justified because $\mu_i$ is $L$-lipschitz.
 
\begin{remark}
We choose to measure the pulling intensity as $N_{t,i} \triangleq \sum_{t'=1}^t A_{t',i}$. In the (stationary) linear bandit problem, the number of pulls is replaced by the matrix $\sum_{t'=1}^t A_{t'}A_{t'}^\intercal$ which is used in the least-square regression and in the confidence ellipsoid computation. Hence, from an information-theoretic point of view, the natural identification is $N_{t,i} \triangleq \sum_{t'=1}^t A_{t',i}^2 $  (which follows when the actions embeddings are the canonical basis vectors). Yet, our choice is motivated because we want a linear dependence between  $N_{t,i}$ and the collected reward along direction $i$ in the stationary case.
\end{remark} 

\subsection{The offline problem}
\label{ss:linear-rotting-offline}
In the rested rotting bandits model, \citet{heidari2016tight} show that the greedy oracle policy which selects the arm with the highest upcoming reward is anytime optimal. Unfortunately, this result does not hold in the linear rotting bandit model. 

\begin{proposition}
\label{prop:linear-rotting-greedy}
Let's consider the simple $d=2$ case. For any horizon $T\geq 2$, there exists an $L$-lipschitz reward vector function $\mu$ bounded in $[0,L]$, a fixed vector arms set $\mathcal{A}$, and a strategy $\pi$ such that the greedy oracle $\GO$ suffers,
\[
J_T(\pi) - J_T(\GO) \geq \frac{L(T-2)}{8}\cdot 
\]
\end{proposition}

We notice that the worst regret for reward values in $[0,L]$ and arms in $[0,1]^d$ is $LT$. Hence, Proposition~\ref{prop:linear-rotting-greedy} shows that $\GO$ is unable to learn as its problem-independent performance is at a constant ratio of the worst possible rate. We will prove precisely this statement in Subsection~\ref{ss:linear-rotting-proofs} but we give here the main intuition. Let's consider a vector reward function such that the first direction decays from $L$ to $0$ in the middle of the game and the second one has a constant reward value $L/2$. In the MAB case - when arms are orthogonal - the greedy oracle $\GO$ stops pulling the first direction when the associated reward decreases. However, when arms are not orthogonal (e.g. $(1,0)$ and $(\nicefrac{1}{2}, \nicefrac{1}{2})$), the greedy policy will collect quickly all the reward associated with the first direction by selecting the first arm. Then, it selects arm $2$ which still pulls a fraction of the first direction, even though the reward has decayed. A better oracle strategy would be to notice that the good reward associated with the first direction will be gathered at the end of the game anyway, and focus on maximizing the reward associated with the other direction.

Notice that this better strategy needs to see at least up to the middle of the game to see that the reward will decay. Otherwise, it will not behave consistently on the problems on which the reward does not decay. Refining this argument, we can show in Proposition~\ref{prop:linear-rotting-short-sighted} that any short-sighted oracle - \ie a policy that can only see the next $T_O$ rewards for any combination of $T_O$ arms - suffers a regret which scales at least with $T-T_O$. It means that the offline problem is a planning problem where we need full knowledge of the reward function and the horizon $T$.

\begin{proposition}
\label{prop:linear-rotting-short-sighted}
Let $\pi$ a short-sighted policy which sees the future rewards associated to any combination of $T_O$ arms. For $T \geq T_O + 23$, there exists a problem $\mu$ and a policy $\pi'$ such that,
\[
J_T(\pi') - J_T(\pi) \geq \frac{L\pa{T-T_O}}{20}\cdot
\]
\end{proposition}

\subsection{The noise-free online problem}
A direct consequence of Proposition~\ref{prop:linear-rotting-short-sighted} is that any learning policy - a special case of short-sighted policies with $T_O =0$ - suffers a worst-case linear regret, even in the absence of noise. Hence, we conclude that the rotting linear setup is not learnable. 
\begin{corollary}
For any learning policy $\pi$ and $T\geq 23$, there exists a problem $\mu$ and a policy $\pi'$
\[
J_T(\pi') - J_T(\pi)  \geq  \frac{LT}{20}\cdot
\]
\end{corollary}

Again, we highlight the deep contrast with the MAB setting, where a simple greedy policy is guaranteed to make at most $K-1$ mistakes. The key argument is that when a direction decreases we need to be able to stop pulling that direction and focus on other directions.  

\subsection{Proofs}
\label{ss:linear-rotting-proofs}
\label{ss:linear-rotting-proof}

\begin{proof}[Proof of Proposition~\ref{prop:linear-rotting-greedy}]
For $d\! =\! 2$, $\mathcal{A} \!=\! \left\{ A_1, A_2 \right\}$ with $A_1 \!=\! (1,0)^\intercal$ and $A_2 \!=\! (\nicefrac{1}{2},\nicefrac{1}{2})^\intercal$. For any horizon $T\geq2$, we consider the following $L$-lipschitz reward functions,
\[
\mu_1(x) = \begin{cases}
L &\text{ if } x < \frac{T-1}{2} \\
L(1- x + \frac{T-1}{2}) &\text{ if }  \frac{T-1}{2} \leq x \leq \frac{T+1}{2}\\
0 &\text{ else}
\end{cases}
\qquad \text{and} \qquad  \mu_2(x) = \frac{L}{2}\cdot
\]

The greedy oracle strategy first selects $A_1$ for $\floor{\frac{T}{2}}$ rounds and then $A_2$ for the remaining $\ceil{\frac{T}{2}}$. Hence,
\begin{align*}
&N_{T,1} = \floor{\frac{T}{2}} + \frac{1}{2} \ceil{\frac{T}{2}}\CommaBin \\
&N_{T,2} = \frac{1}{2} \ceil{\frac{T}{2}} \geq \frac{T+1}{2}\cdot
\end{align*} 
For $T \geq 2$, $N_{T,1} \geq \frac{T+1}{2}$, which means that the greedy policy collects all the $\nicefrac{LT}{2}$ reward along the first direction. We also notice that $N_{T,2} \leq \frac{T+1}{4}$, which means that the total reward is bounded by, 
\[
J_T(\GO) = \int_0^{N_{T,1}} \mu_1(x)dx + \int_0^{N_{T,2}} \mu_2(x)dx  \leq \frac{LT}{2} + \frac{L\pa{T+1}}{8}\cdot
\]
We now consider the policy $\pi_2$ which always selects arm 2. At the end of the game, it gathers the reward,
\[
J_T(\pi_2) = \int_0^{\frac{T}{2}} \mu_1(x)dx + \int_0^{\frac{T}{2}} \mu_2(x)dx = \frac{L\pa{T-\nicefrac{1}{4}} }{2} + \frac{LT}{4}\cdot
\]

Hence, we have that,
\[
J_T(\policy_2) - J_T(\GO) \geq \frac{L(T-2)}{8}\cdot 
\]

\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:linear-rotting-short-sighted}]
We still consider $d = 2$, $\mathcal{A} = \left\{ A_1, A_2 \right\}$ with $A_1 = (1,0)^\intercal$ and $A_2 = (\nicefrac{1}{2},\nicefrac{1}{2})^\intercal$. For any horizon $T$, we consider the ($L$-lipschitz) two reward vector functions $\mu^1$ and $\mu^2$,
\begin{align*}
&\mu^1_1(x) = L  & \text{and} \;  \mu^1_2(x) = \frac{L}{2} \CommaBin\\
&\mu^2_1(x) = \begin{cases}
L &\text{ if } x <\frac{T+T_O-1}{2} \\
L(1- x +  \frac{T +T_O -1}{2}) &\text{ if }  \frac{T +T_O-1}{2} \leq x \leq \frac{T +T_O+1}{2}\\
0 &\text{ else}
\end{cases}
 & \text{and} \; \mu^2_2(x) = \frac{L}{2}\cdot
\end{align*}

Notice that it is not possible to tell the difference between the two setups for the short-sighted oracle $\pi$ before round $t$ such that $ N_{t,1} \geq \frac{T -T_O -1}{2}$. Notice that this round always exists because $N_{T,1} \geq \frac{T}{2}$ for any policy because the pulling intensity of the first direction is always greater than $\nicefrac{1}{2}$ .

On problem $\mu^1$, we will compare $\pi$ to $\pi_1$ which selects always arm $1$. On problem $\mu^2$, we will compare $\pi$ to $\pi_2$ which selects always arm $2$. We give the cumulative reward of the different policies on the two problems,
\begin{align}
&J^1_T(\pi_1) = LT, \label{eq:linear-J1Tpi1}\\ 
&J^2_T(\pi_2) = \frac{L\pa{T -\nicefrac{1}{4}}}{2} + \frac{LT}{4}\CommaBin\label{eq:linear-J2Tpi2}\\
&J^1_T(\pi) = L\pa{T - N^1_{T,2}}, \label{eq:linear-J1Tpi} \\
&J^2_T(\pi) \leq \frac{L\pa{T + N^2_{T,2}}}{2}\CommaBin\label{eq:linear-J2Tpi}
\end{align}
with $N^k_{T,2}$ the pulling intensity in direction $2$ of $\pi$ on problem $k$. The inequality upper-bounds the reward collected in direction $1$ by its maximum $\nicefrac{LT}{2}$. Since we cannot distinguish between the two problems until $N_{t,1} \geq \frac{T - T_O -1}{2} $, we will show that $N^1_{T,2}$ and $N^2_{T,2}$ are loosely related. 

We call $t_u$ the last round such that the two problems are indistinguishable. Therefore, we have that, 
\[
N_{t_u,1}^1 = N_{t_u,1}^2 \triangleq  N_{t_u,1} \text{ and } N_{t_u,2}^1 = N_{t_u,2}^2\triangleq  N_{t_u,2} .
\]

Problems are indistinguishable until,
\[ N_{t_u,1} < \frac{T - T_O -1}{2} \cdot\]

We also know that the problems can be distinguished at the round $t_u+1$, \ie 
\[  \frac{T - T_O -1}{2} \leq N_{t_u+1,1} \leq N_{t_u,1} +1.\] 

The second inequality is because $N_{t_u+1,1} - N_{t_u,1} \leq \max_{i \leq 2} A_{i,1} = 1$. We have bounded $N_{t_u,1}$ but we use $N_{T,2}^k$  in Equations~\ref{eq:linear-J1Tpi} and~\ref{eq:linear-J2Tpi}. We will start by relating $N_{t_u,2}$ to $N_{t_u,1}$, and then we will relate $N_{T,2}^k$ to $N_{t_u,2}$. We call $n_{t,i}$, the number of pulls of arm $i$ at the round $t$. We have that 
\begin{align*}
&N_{t,1} = n_{t,1} + \frac{n_{t,2}}{2}\CommaBin \\
&N_{t,2} = \frac{n_{t,2}}{2}\CommaBin\\\
&n_{t,1} + n_{t,2} = t.
\end{align*}

We can rewrite $N_{t,1} = t - N_{t,2}$. Hence, the above inequations on $N_{t_u,1}$ can be translated for $N_{t_u,2}$. 
\begin{align}
&N_{t_u,2} > \frac{2t_u - T + T_O+1}{2}\CommaBin \nonumber\\
&N_{t_u,2} \leq \frac{2t_u - T +T_O +3}{2} \cdot\label{eq:ntu2}
\end{align}

We can now bound $N_{t_u,2}^k$ for all $k$,
\[
 \frac{2t_u - T + T_O+1}{2} <N_{t_u,2} \leq N_{T,2} ^k \leq \frac{T-t_u}{2} + N_{t_u,2} \leq \frac{t_u +T_O +3}{2}. 
\]

The first and fourth inequality use Equation~\ref{eq:ntu2}. The second uses that $t_u\leq T$ and that the pulling intensity can only grow through the rounds. The third inequality uses that with $N_{t_u,2}$ pulling intensity at the round $t_u$, the learner cannot reach more than $N_{t_u,2} + \pa{T-t_u} \max_{j}A_{j,2} =N_{t_u,2} + \frac{T-t_u}{2}$ (by selecting arm $2$ until the end of the game).

We can use Equations~\ref{eq:linear-J1Tpi1} and~\ref{eq:linear-J1Tpi} (respectively~\ref{eq:linear-J2Tpi2} and~\ref{eq:linear-J2Tpi}) to lower-bound the difference of performance with respect to policy $\pi_1$ (respectively $\pi_2$) on problem $1$ (respectively $2$). 
\begin{align*}
&J_T^1(\pi_1)- J_T^1(\pi) = LN_{T,2}^1 \geq \frac{L}{2} \pa{2t_u - \pa{T - T_O} +1}\\
&J_T^2(\pi_2)- J_T^2(\pi) \geq  \frac{L}{4} \pa{T - 2N_{T,2}^2- \nicefrac{1}{2}} \geq\frac{L}{4} \pa{T -t_u - T_O - 3.5 } .
\end{align*}

The only quantity which depends on the algorithm $\pi$ is $t_u\in \left\{1, \dots, T\right\}$. In a minimax perspective, we lower bound the maximum of these two bounds,
\[
\max\pa{J_T^1(\pi_1)- J_T^1(\pi), J_T^2(\pi_2)- J_T^2(\pi)} \geq \frac{L\pa{T-T_O}}{10} -1.15L.
\]
We conclude the proof by noticing that when $T-T_O\geq 23$, 
\[
\frac{L\pa{T-T_O}}{10} -1.15L \geq \frac{L\pa{T-T_O}}{20}\cdot
\]
\end{proof}

