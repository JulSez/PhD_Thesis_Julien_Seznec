%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Alessandro Lazaric at 2019-02-12 10:52:36 +0100 


%% Saved with string encoding Unicode (UTF-8) 

@misc{SMPyBandits,
    title =   {{SMPyBandits: an Open-Source Research Framework for Single and Multi-Players Multi-Arms Bandits (MAB) Algorithms in Python}},
    author =  {Lilian Besson},
    year =    {2018},
    url =     {https://github.com/SMPyBandits/SMPyBandits/},
    howpublished = {Online at: \url{github.com/SMPyBandits/SMPyBandits}},
    note =    {Code at https://github.com/SMPyBandits/SMPyBandits/, documentation at https://smpybandits.github.io/}
}

@inproceedings{warlop2018fighting,
	Author = {Romain Warlop and Alessandro Lazaric and J{\'{e}}r{\'{e}}mie Mary},
	Bibsource = {dblp computer science bibliography, https://dblp.org},
	Biburl = {https://dblp.org/rec/bib/conf/nips/WarlopLM18},
	Booktitle = {Neural Information Processing Systems},
	Timestamp = {Sun, 16 Dec 2018 17:30:05 +0100},
	Title = {Fighting boredom in recommender systems with linear reinforcement learning},
	Url = {http://papers.nips.cc/paper/7447-fighting-boredom-in-recommender-systems-with-linear-reinforcement-learning},
	Year = {2018},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7447-fighting-boredom-in-recommender-systems-with-linear-reinforcement-learning}}

@inproceedings{bifet2007learning,
	Abstract = {We present a new approach for dealing with distribution change and concept drift when learning from data sequences that may vary with time. We use sliding windows whose size, instead of being fixed a priori, is recomputed online according to the rate of change observed from the data in the window itself. This delivers the user or programmer from having to guess a time-scale for change. Contrary to many related works, we provide rigorous guarantees of performance, as bounds on the rates of false positives and false negatives. Using ideas from data stream algorithmics, we develop a time- and memory-efficient version of this algorithm, called ADWIN2. We show how to combine ADWIN2 with the Naive Bayes (NB) predictor, in two ways: one, using it to monitor the error rate of the current model and declare when revision is necessary and, two, putting it inside the NB predictor to maintain up-to-date estimations of conditional probabilities in the data. We test our approach using synthetic and real data streams.},
	Author = {Bifet, Albert and Gavald{\`{a}}, Ricard},
	Booktitle = {International Conference on Data Mining},
	Title = {{Learning from time-changing data with adaptive windowing}},
	Url = {https://pdfs.semanticscholar.org/fea2/14dd4c4050d96e00fd4bf45b564274efef04.pdf},
	Year = {2007},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/fea2/14dd4c4050d96e00fd4bf45b564274efef04.pdf}}

@article{TrAmBa17,
	Abstract = {We present a new random sampling strategy for k-bandlimited signals defined on graphs, based on determinantal point processes (DPP). For small graphs, ie, in cases where the spectrum of the graph is accessible, we exhibit a DPP sampling scheme that enables perfect recovery of bandlimited signals. For large graphs, ie, in cases where the graph's spectrum is not accessible, we investigate, both theoretically and empirically, a sub-optimal but much faster DPP based on loop-erased random walks on the graph. Preliminary experiments show promising results especially in cases where the number of measurements should stay as small as possible and for graphs that have a strong community structure. Our sampling scheme is efficient and can be applied to graphs with up to {\$}10{\^{}}6{\$} nodes.},
	Archiveprefix = {arXiv},
	Arxivid = {1703.01594},
	Author = {Tremblay, Nicolas and Amblard, Pierre-Olivier and Barthelm{\'{e}}, Simon},
	Eprint = {1703.01594},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Tremblay, Amblard, Barthelm{\'{e}} - 2017 - Graph sampling with determinantal processes(2).pdf:pdf},
	Isbn = {9780992862671},
	Journal = {ArXiv e-prints},
	Title = {{Graph sampling with determinantal processes}},
	Url = {http://arxiv.org/abs/1703.01594},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.01594}}

@inproceedings{Kan13,
	Abstract = {Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive definite matrix that de-fines the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from which we can acquire a sample from the given DPP in sub-cubic time. In ad-dition, we show that this framework can be extended to sampling from cardinality-constrained DPPs. As an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering. There are some crucial errors in the proofs of the theorem which invalidate the theoretical claims of this paper. Please consult the appendix for more details.},
	Author = {Kang, Byungkon},
	Booktitle = {NIPS},
	File = {::},
	Title = {{Fast Determinantal Point Process Sampling with Application to Clustering}},
	Year = {2013}}

@article{DuBa16,
	Abstract = {We propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items. This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items. We apply this new class to modelling text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs. We present an application to document summarization with a DPP on {\$}2{\^{}}{\{}500{\}}{\$} items.},
	Archiveprefix = {arXiv},
	Arxivid = {1610.05925},
	Author = {Dupuy, Christophe and Bach, Francis},
	Eprint = {1610.05925},
	File = {::},
	Journal = {arXiv preprint arXiv:1610.05925},
	Month = {oct},
	Title = {{Learning Determinantal Point Processes in Sublinear Time}},
	Url = {http://arxiv.org/abs/1610.05925},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1610.05925}}

@article{UrBrMoRi17,
	Abstract = {Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the cycle sparsity; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings.},
	Archiveprefix = {arXiv},
	Arxivid = {1703.00539},
	Author = {Urschel, John and Brunel, Victor-Emmanuel and Moitra, Ankur and Rigollet, Philippe},
	Eprint = {1703.00539},
	File = {::},
	Journal = {International Conference on Machine Learning},
	Pages = {3511--3520},
	Title = {{Learning Determinantal Point Processes with Moments and Cycles}},
	Url = {http://proceedings.mlr.press/v70/urschel17a/urschel17a.pdf https://arxiv.org/abs/1703.00539},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/urschel17a/urschel17a.pdf%20https://arxiv.org/abs/1703.00539}}

@article{Bardenet2016,
	Abstract = {We show that repulsive random variables can yield Monte Carlo methods with faster convergence rates than the typical {\$}N{\^{}}{\{}-1/2{\}}{\$}, where {\$}N{\$} is the number of integrand evaluations. More precisely, we propose stochastic numerical quadratures involving determinantal point processes associated with multivariate orthogonal polynomials, and we obtain root mean square errors that decrease as {\$}N{\^{}}{\{}-(1+1/d)/2{\}}{\$}, where {\$}d{\$} is the dimension of the ambient space. First, we prove a central limit theorem (CLT) for the linear statistics of a class of determinantal point processes, when the reference measure is a product measure supported on a hypercube, which satisfies the Nevai-class regularity condition, a result which may be of independent interest. Next, we introduce a Monte Carlo method based on these determinantal point processes, and prove a CLT with explicit limiting variance for the quadrature error, when the reference measure satisfies a stronger regularity condition. As a corollary, by taking a specific reference measure and using a construction similar to importance sampling, we obtain a general Monte Carlo method, which applies to any measure with continuously derivable density. Loosely speaking, our method can be interpreted as a stochastic counterpart to Gaussian quadrature, which, at the price of some convergence rate, is easily generalizable to any dimension and has a more explicit error term.},
	Archiveprefix = {arXiv},
	Arxivid = {1605.00361},
	Author = {Bardenet, R{\'{e}}mi and Hardy, Adrien},
	Eprint = {1605.00361},
	File = {::},
	Month = {may},
	Pages = {48},
	Title = {{Monte Carlo with Determinantal Point Processes}},
	Url = {http://arxiv.org/abs/1605.00361},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1605.00361}}

@article{GoChGr14,
	Abstract = {Video summarization is a challenging problem with great application potential. Whereas prior approaches, largely unsupervised in nature, focus on sampling use-ful frames and assembling them as summaries, we consider video summarization as a supervised subset selection problem. Our idea is to teach the system to learn from human-created summaries how to select informative and diverse subsets, so as to best meet evaluation metrics derived from human-perceived quality. To this end, we propose the sequential determinantal point process (seqDPP), a proba-bilistic model for diverse sequential subset selection. Our novel seqDPP heeds the inherent sequential structures in video data, thus overcoming the deficiency of the standard DPP, which treats video frames as randomly permutable items. Mean-while, seqDPP retains the power of modeling diverse subsets, essential for summa-rization. Our extensive results of summarizing videos from 3 datasets demonstrate the superior performance of our method, compared to not only existing unsuper-vised methods but also naive applications of the standard DPP model.},
	Author = {Gong, Boqing and Chao, Wl and Grauman, Kristen and Sha, F},
	Issn = {10495258},
	Journal = {Nips},
	Keywords = {erse sequential subset selection,for,supervised video summarization},
	Pages = {1--9},
	Title = {{Diverse Sequential Subset Selection for Supervised Video Summarization}},
	Url = {http://papers.nips.cc/paper/5413-large-scale-l-bfgs-using-mapreduce},
	Year = {2014},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5413-large-scale-l-bfgs-using-mapreduce}}

@inproceedings{DeWa17,
	Abstract = {Given a full rank matrix {\$}X{\$} with more columns than rows consider the task of estimating the pseudo inverse {\$}X{\^{}}+{\$} based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times {\$}X{\^{}}+X{\^{}}{\{}+\backslashtop{\}}{\$}. Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of {\$}X{\$}. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from {\$}X{\$}. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels. We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.},
	Address = {Long Beach, CA, USA},
	Archiveprefix = {arXiv},
	Arxivid = {1705.06908},
	Author = {Derezinski, Michal and Warmuth, Manfred K.},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1705.06908},
	File = {::},
	Month = {may},
	Title = {{Unbiased estimates for linear regression via volume sampling}},
	Url = {http://arxiv.org/abs/1705.06908},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.06908}}

@inproceedings{gartrell2016low,
	Abstract = {Determinantal point processes (DPPs) have garnered attention as an elegant probabilistic model of set diversity. They are useful for a number of subset selection tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. In this work we present a new method for learning the DPP kernel from observed data using a low-rank factorization of this kernel. We show that this low-rank factorization enables a learning algorithm that is nearly an order of magnitude faster than previous approaches, while also providing for a method for computing product recommendation predictions that is far faster (up to 20x faster or more for large item catalogs) than previous techniques that involve a full-rank DPP kernel. Furthermore, we show that our method provides equivalent or sometimes better predictive performance than prior full-rank DPP approaches, and better performance than several other competing recommendation methods in many cases. We conduct an extensive experimental evaluation using several real-world datasets in the domain of product recommendation to demonstrate the utility of our method, along with its limitations.},
	Archiveprefix = {arXiv},
	Arxivid = {1602.05436},
	Author = {Gartrell, Mike and Paquet, Ulrich and Koenigstein, Noam},
	Booktitle = {AAAI Conference on Artificial Intelligence},
	Eprint = {1602.05436},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Gartrell, Paquet, Koenigstein - 2017 - Low-Rank Factorization of Determinantal Point Processes for Recommendation.pdf:pdf},
	Title = {{Low-Rank Factorization of Determinantal Point Processes for Recommendation}},
	Url = {http://arxiv.org/abs/1602.05436},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1602.05436}}

@article{HoKoFa14,
	Abstract = {In the period since 2004, many novel sophisticated approaches for generic multi-document summarization have been developed. Intuitive simple approaches have also been shown to perform unexpectedly well for the task. Yet it is practically impossible to compare the existing approaches directly, because systems have been evaluated on different datasets, with different evaluation measures, against different sets of comparison systems. Here we present a corpus of summaries produced by several state-of-the-art extractive summarization systems or by popular baseline systems. The inputs come from the 2004 DUC evaluation, the latest year in which generic summarization was addressed in a shared task. We use the same settings for ROUGE automatic evaluation to compare the systems directly and analyze the statistical significance of the differences in performance. We show that in terms of average scores the state-of-the-art systems appear similar but that in fact they produce very different summaries. Our corpus will facilitate future research on generic summarization and motivates the need for development of more sensitive evaluation measures and for approaches to system combination in summarization.},
	Author = {Hong, Kai and Conroy, John M and Favre, Benoit and Kulesza, Alex and Lin, Hui and Nenkova, Ani},
	File = {::},
	Isbn = {978-2-9517408-8-4},
	Journal = {Lrec2014},
	Keywords = {duc 2004,evaluation,generic summarization},
	Number = {Classy 04},
	Pages = {1608--1616},
	Title = {{A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization}},
	Volume = {65},
	Year = {2014}}

@inproceedings{anari2016monte,
	Abstract = {Strongly Rayleigh distributions are natural generalizations of product and determinantal probability distributions and satisfy strongest form of negative dependence properties. We show that the "natural" Monte Carlo Markov Chain (MCMC) is rapidly mixing in the support of a {\{}$\backslash$em homogeneous{\}} strongly Rayleigh distribution. As a byproduct, our proof implies Markov chains can be used to efficiently generate approximate samples of a {\$}k{\$}-determinantal point process. This answers an open question raised by Deshpande and Rademacher.},
	Archiveprefix = {arXiv},
	Arxivid = {1602.05242},
	Author = {Anari, Nima and Gharan, Shayan Oveis and Rezaei, Alireza},
	Booktitle = {Conference on Learning Theory},
	Eprint = {1602.05242},
	File = {::},
	Keywords = {1,13,2016,algorithms for sampling,determinantal point processes,monte carlo markov chain,r,strongly rayleigh distributions and,vol 49,workshop and conference proceedings},
	Month = {feb},
	Title = {{Monte Carlo Markov Chains for sampling Strongly Rayleigh distributions and Determinantal Point Processes}},
	Url = {http://arxiv.org/abs/1602.05242},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1602.05242}}

@article{LiBi12,
	Abstract = {We introduce a method to learn a mixture of submodular ``shells'' in a large-margin setting. A submodular shell is an abstract submodular function that can be instantiated with a ground set and a set of parameters to produce a submodular function. A mixture of such shells can then also be so instantiated to produce a more complex submodular function. What our algorithm learns are the mixture weights over such shells. We provide a risk bound guarantee when learning in a large-margin structured-prediction setting using a projected subgradient method when only approximate submodular optimization is possible (such as with submodular function maximization). We apply this method to the problem of multi-document summarization and produce the best results reported so far on the widely used NIST DUC-05 through DUC-07 document summarization corpora.},
	Author = {Lin, Hui and Bilmes, Ja},
	Isbn = {9780974903989},
	Journal = {Uncertainty in Artificial Intelligence},
	Title = {{Learning mixtures of submodular shells with application to document summarization}},
	Url = {http://arxiv.org/abs/1210.4871},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1210.4871}}

@inproceedings{TrBaAm17,
	Abstract = {R{\'{e}}sum{\'{e}} -- Nous consid{\'{e}}rons l echantillonnage de signaux sur graph{\`{e}} a bande limit{\'{e}}e k , i . e . , les combinaisons lin{\'{e}}aires des k premiers modes de Fourier du graphe . Il existe k noeuds du graphe qui permettent leur reconstruction parfaite , les trouver n{\'{e}}cessite cependant une diagonalisation partielle de la matrice laplacienne , trop co{\^{u}}teus{\`{e}} a grande dimension . Nous proposons une nouvelle m{\'{e}}thode rapide d echantillonnage bas{\'{e}}e sur des processus d{\'{e}}terminantaux qui permet la reconstructio a partir d ' un nombre d echantillons de l ' ordre de k . Abstract -- We consider the problem of sampling k - bandlimited graph signals , i . e . , linear combinations of the first k graph Fourier modes . We know that a set of k nodes embedding all k - bandlimited signals always exists , thereby enabling their perfect reconstruction after sampling . Unfortunately , to exhibit such a set , one needs to partially diagonalize the graph Laplacian , which becomes prohibitive at large scale . We propose a novel strategy based on determinantal point processes that side - steps partial diagonalisation and enables reconstruction with only O (k) samples .},
	Archiveprefix = {arXiv},
	Arxivid = {1704.02239},
	Author = {Tremblay, Nicolas and Barthelm{\'{e}}, Simon and Amblard, Pierre-Olivier},
	Booktitle = {GRETSI},
	Eprint = {1704.02239},
	File = {::},
	Title = {{{\'{E}}chantillonnage de signaux sur graphes via des processus d{\'{e}}terminantaux}},
	Url = {https://hal.archives-ouvertes.fr/hal-01503736 https://arxiv.org/abs/1704.02239},
	Year = {2017},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-01503736%20https://arxiv.org/abs/1704.02239}}

@inproceedings{gillenwater2012discovering,
	Abstract = {We propose a novel probabilistic technique for modeling and extracting salient struc-ture from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of infor-mation. We are particularly interested in revealing and exploiting relationships be-tween documents. To this end, we focus on extracting diverse sets of threads---singly-linked, coherent chains of important doc-uments. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.},
	Author = {Gillenwater, Jennifer and Kulesza, Alex and Taskar, Ben},
	Booktitle = {Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	File = {::},
	Pages = {710--720},
	Title = {{Discovering Diverse and Salient Threads in Document Collections}},
	Year = {2012}}

@article{AgChCh14,
	Abstract = {In this paper, we compare three initialization schemes for the KMEANS clustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and 3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k needs to be set by the user of the algorithms. (Kang 2013) recently proposed a novel use of determinantal point processes for sampling the initial centroids for the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide any evaluation establishing that KMEANSD++ is better than other algorithms. In this paper, we show that the performance of KMEANSD++ is comparable to KMEANS++ (both of which are better than KMEANSRAND) with KMEANSD++ having an additional that it can automatically approximate the value of k.},
	Archiveprefix = {arXiv},
	Arxivid = {1410.6975},
	Author = {Agarwal, Apoorv and Choromanska, Anna and Choromanski, Krzysztof},
	Eprint = {1410.6975},
	File = {::},
	Month = {oct},
	Title = {{Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering}},
	Url = {http://arxiv.org/abs/1410.6975},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1410.6975}}

@inproceedings{BrMoRi17,
	Abstract = {Determinantal point processes (DPPs) have wide-ranging applications in machine learning, where they are used to enforce the notion of diversity in subset selection problems. Many estimators have been proposed, but surprisingly the basic properties of the maximum likelihood estimator (MLE) have received little attention. In this paper, we study the local geometry of the expected log-likelihood function to prove several rates of convergence for the MLE. We also give a complete characterization of the case where the MLE converges at a parametric rate. Even in the latter case, we also exhibit a potential curse of dimensionality where the asymptotic variance of the MLE is exponentially large in the dimension of the problem.},
	Address = {Amsterdam, Netherlands},
	Author = {Brunel, Victor-Emmanuel and Moitra, Ankur and Rigollet, Philippe and Urschel, John},
	Booktitle = {Conference on Learning Theory},
	Editor = {Kale, Satyen and Shamir, Ohad},
	Pages = {343--345},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Rates of estimation for determinantal point processes}},
	Url = {http://proceedings.mlr.press/v65/brunel17a.html},
	Volume = {65},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v65/brunel17a.html}}

@article{Evans2009,
	Abstract = {In this paper, we use a partition of the links of a network in order to uncover its community structure. This approach allows for communities to overlap at nodes, so that nodes may be in more than one community. We do this by making a node partition of the line graph of the original network. In this way we show that any algorithm which produces a partition of nodes can be used to produce a partition of links. We discuss the role of the degree heterogeneity and propose a weighted version of the line graph in order to account for this.},
	Archiveprefix = {arXiv},
	Arxivid = {0903.2181},
	Author = {Evans, T. S. and Lambiotte, R.},
	Doi = {10.1103/PhysRevE.80.016105},
	Eprint = {0903.2181},
	File = {::},
	Issn = {1539-3755},
	Journal = {Physical Review E},
	Month = {jul},
	Number = {1},
	Pages = {016105},
	Title = {{Line graphs, link partitions, and overlapping communities}},
	Url = {http://arxiv.org/abs/0903.2181},
	Volume = {80},
	Year = {2009},
	Bdsk-Url-1 = {http://arxiv.org/abs/0903.2181},
	Bdsk-Url-2 = {https://doi.org/10.1103/PhysRevE.80.016105}}

@inproceedings{ZeSr16,
	Abstract = {We introduce Divnet, a flexible technique for learning networks with di-verse neurons. Divnet models neuronal diversity by placing a Determi-nantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Di-vnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effec-tive auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.},
	Address = {San Juan, Puerto Rico},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1511.05077v6},
	Author = {Mariet, Zelda and Sra, Suvrit},
	Booktitle = {Conference on Learning Representations},
	Eprint = {arXiv:1511.05077v6},
	File = {::},
	Pages = {1--13},
	Title = {{Diversity Networks}},
	Url = {https://arxiv.org/pdf/1511.05077.pdf},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1511.05077.pdf}}

@inproceedings{LiJeSr17,
	Abstract = {We study dual volume sampling, a method for selecting k columns from an n*m short and wide matrix (n {\textless}= k {\textless}= m) such that the probability of selection is proportional to the volume of the parallelepiped spanned by the rows of the induced submatrix. This method was studied in [3], who motivated it as a promising method for column subset selection. However, the development of polynomial time sampling algorithms -- exact or approximate -- has been since open. We close this open problem by presenting (i) an exact (randomized) polynomial time sampling algorithm; (ii) its derandomization that samples subsets satisfying the desired properties deterministically; and (iii) an efficient approximate sampling procedure using Markov chains that are provably fast mixing. Our algorithms can thus benefit downstream applications of dual volume sampling, such as column subset selection and experimental design.},
	Address = {Long Beach, CA, USA},
	Annote = {NULL},
	Archiveprefix = {arXiv},
	Arxivid = {1703.02674},
	Author = {Li, Chengtao and Jegelka, Stefanie and Sra, Suvrit},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1703.02674},
	File = {::},
	Month = {mar},
	Title = {{Column Subset Selection via Polynomial Time Dual Volume Sampling}},
	Url = {http://arxiv.org/abs/1703.02674},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.02674}}

@article{HKPV06,
	Abstract = {We give a probabilistic introduction to determinantal and permanental point processes. Determinantal processes arise in physics (fermions, eigenvalues of random matrices) and in combinatorics (nonintersecting paths, random spanning trees). They have the striking property that the number of points in a region {\$}D{\$} is a sum of independent Bernoulli random variables, with parameters which are eigenvalues of the relevant operator on {\$}L{\^{}}2(D){\$}. Moreover, any determinantal process can be represented as a mixture of determinantal projection processes. We give a simple explanation for these known facts, and establish analogous representations for permanental processes, with geometric variables replacing the Bernoulli variables. These representations lead to simple proofs of existence criteria and central limit theorems, and unify known results on the distribution of absolute values in certain processes with radially symmetric distributions.},
	Archiveprefix = {arXiv},
	Arxivid = {math/0503110},
	Author = {Hough, J. Ben and Krishnapur, Manjunath and Peres, Yuval and Vir{\'{a}}g, B{\'{a}}lint},
	Doi = {10.1214/154957806000000078},
	Eprint = {0503110},
	File = {::},
	Issn = {1549-5787},
	Journal = {Probability Surveys},
	Pages = {206--229},
	Primaryclass = {math},
	Title = {{Determinantal Processes and Independence}},
	Url = {http://arxiv.org/abs/math/0503110{\%}5Cnhttp://www.arxiv.org/pdf/math/0503110.pdf},
	Volume = {3},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1214/154957806000000078}}

@article{KuTa12,
	Abstract = {Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.},
	Annote = {NULL},
	Archiveprefix = {arXiv},
	Arxivid = {1207.6083},
	Author = {Kulesza, Alex and Taskar, Ben},
	Doi = {10.1561/2200000044},
	Eprint = {1207.6083},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Kulesza, Taskar - 2012 - Determinantal Point Processes for Machine Learning(3).pdf:pdf},
	Issn = {1935-8237},
	Journal = {Foundations and Trends in Machine Learning},
	Month = {jul},
	Number = {2-3},
	Pages = {123--286},
	Title = {{Determinantal Point Processes for Machine Learning}},
	Url = {http://arxiv.org/abs/1207.6083},
	Volume = {5},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1207.6083},
	Bdsk-Url-2 = {https://doi.org/10.1561/2200000044}}

@article{GiKuFo14,
	Abstract = {A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix, or learning weights for a linear combination of DPPs with fixed kernel matrices. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5{\%} in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.},
	Archiveprefix = {arXiv},
	Arxivid = {1411.1088},
	Author = {Gillenwater, Jennifer A and Kulesza, Alex and Fox, Emily and Taskar, Ben},
	Eprint = {1411.1088},
	Journal = {Nips},
	Pages = {3149--3157},
	Title = {{Expectation-Maximization for Learning Determinantal Point Processes}},
	Url = {http://papers.nips.cc/paper/5564-expectation-maximization-for-learning-determinantal-point-processes.pdf{\%}5Cnhttp://papers.nips.cc/paper/5564-expectation-maximization-for-learning-determinantal-point-processes},
	Year = {2014}}

@inproceedings{LiJeSr16,
	Abstract = {In this note we consider sampling from (non-homogeneous) strongly Rayleigh probability measures. As an important corollary, we obtain a fast mixing Markov Chain sampler for Determinantal Point Processes.},
	Archiveprefix = {arXiv},
	Arxivid = {1607.03559},
	Author = {Li, Chengtao and Jegelka, Stefanie and Sra, Suvrit},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1607.03559},
	File = {::},
	Title = {{Fast Sampling for Strongly Rayleigh Measures with Application to Determinantal Point Processes}},
	Year = {2016}}

@article{BaHa16,
	Abstract = {We show that repulsive random variables can yield Monte Carlo methods with faster convergence rates than the typical {\$}N{\^{}}{\{}-1/2{\}}{\$}, where {\$}N{\$} is the number of integrand evaluations. More precisely, we propose stochastic numerical quadratures involving determinantal point processes associated with multivariate orthogonal polynomials, and we obtain root mean square errors that decrease as {\$}N{\^{}}{\{}-(1+1/d)/2{\}}{\$}, where {\$}d{\$} is the dimension of the ambient space. First, we prove a central limit theorem (CLT) for the linear statistics of a class of determinantal point processes, when the reference measure is a product measure supported on a hypercube, which satisfies the Nevai-class regularity condition, a result which may be of independent interest. Next, we introduce a Monte Carlo method based on these determinantal point processes, and prove a CLT with explicit limiting variance for the quadrature error, when the reference measure satisfies a stronger regularity condition. As a corollary, by taking a specific reference measure and using a construction similar to importance sampling, we obtain a general Monte Carlo method, which applies to any measure with continuously derivable density. Loosely speaking, our method can be interpreted as a stochastic counterpart to Gaussian quadrature, which, at the price of some convergence rate, is easily generalizable to any dimension and has a more explicit error term.},
	Annote = {NULL},
	Archiveprefix = {arXiv},
	Arxivid = {1605.00361},
	Author = {Bardenet, R{\'{e}}mi and Hardy, Adrien},
	Eprint = {1605.00361},
	File = {::},
	Month = {may},
	Pages = {48},
	Title = {{Monte Carlo with Determinantal Point Processes}},
	Url = {http://arxiv.org/abs/1605.00361},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1605.00361}}

@article{LaMoRu15,
	Abstract = {Statistical models and methods for determinantal point processes (DPPs) seem largely unexplored. We demonstrate that DPPs provide useful models for the description of spatial point pattern datasets where nearby points repel each other. Such data are usually modelled by Gibbs point processes, where the likelihood and moment expressions are intractable and simulations are time consuming. We exploit the appealing probabilistic properties of DPPs to develop parametric models, where the likelihood and moment expressions can be easily evaluated and realizations can be quickly simulated. We discuss how statistical inference is conducted using the likelihood or moment properties of DPP models, and we provide freely available software for simulation and statistical inference.},
	Archiveprefix = {arXiv},
	Arxivid = {1205.4818},
	Author = {Lavancier, Fr{\'{e}}d{\'{e}}ric and M{\{}$\backslash$o{\}}ller, Jesper and Rubak, Ege},
	Doi = {10.1111/rssb.12096},
	Eprint = {1205.4818},
	File = {::},
	Issn = {14679868},
	Journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	Keywords = {Maximum-likelihood-based inference,Point process density,Product densities,Repulsiveness,Simulation,Spectral approach},
	Number = {4},
	Pages = {853--877},
	Title = {{Determinantal point process models and statistical inference}},
	Volume = {77},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1111/rssb.12096}}

@inproceedings{DeGaLa17,
	Abstract = {Dans cet article, nous {\'{e}}tudions une adaptation des processus ponctuels d{\'{e}}terminantaux au cadre des pixels d'une image. Il s'agit d'un cadre 2D discret, stationnaire et p{\'{e}}riodique. Nous nous int{\'{e}}ressons en particulier aux propri{\'{e}}t{\'{e}}s de r{\'{e}}pulsion d'un tel processus. Nous {\'{e}}tudions d'abord la r{\'{e}}pulsion totale, puis en utilisant le mod{\`{e}}le shot noise, nous caract{\'{e}}risons les cas de r{\'{e}}pulsion maximale et minimale de ces processus pixelliques d{\'{e}}terminantaux.},
	Address = {Juan-les-Pins},
	Author = {Desolneux, Agn{\`{e}}s and Galerne, Bruno and Launay, Claire},
	Booktitle = {GRETSI},
	File = {::},
	Title = {{Etude de la r{\'{e}}pulsion des processus pixelliques d{\'{e}}terminantaux}},
	Url = {https://hal.archives-ouvertes.fr/hal-01548767v2},
	Year = {2017},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-01548767v2}}

@inproceedings{MaSr16,
	Abstract = {Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of {\$}N{\$} items. They have recently gained prominence in several applications that rely on "diverse" subsets. However, their applicability to large problems is still limited due to the {\$}\backslashmathcal O(N{\^{}}3){\$} complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP.},
	Address = {Barcelona, Spain},
	Archiveprefix = {arXiv},
	Arxivid = {1605.08374},
	Author = {Mariet, Zelda and Sra, Suvrit},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1605.08374},
	File = {::},
	Month = {may},
	Pages = {2694----2702},
	Title = {{Kronecker Determinantal Point Processes}},
	Url = {https://papers.nips.cc/paper/6296-kronecker-determinantal-point-processes http://arxiv.org/abs/1605.08374},
	Year = {2016},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/6296-kronecker-determinantal-point-processes%20http://arxiv.org/abs/1605.08374}}

@inproceedings{LiJeSr16a,
	Abstract = {Determinantal Point Processes (DPPs) provide probabilistic models over discrete sets of items that help model repulsion and diversity. Applicability of DPPs to large sets of data is, however, hindered by the expensive matrix operations involved, especially when sampling. We therefore propose a new efficient approximate two-stage sampling algorithm for discrete k-DPPs. As opposed to previous approximations, our algorithm aims at minimizing the variational distance to the original distribution. Experiments indicate that the resulting sampling algorithm works well on large data and yields more accurate samples than previous approaches.},
	Address = {Cadiz, Spain},
	Archiveprefix = {arXiv},
	Arxivid = {1509.01618},
	Author = {Li, Chengtao and Jegelka, Stefanie and Sra, Suvrit},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Eprint = {1509.01618},
	File = {::},
	Pages = {1--14},
	Title = {{Efficient Sampling for k-Determinantal Point Processes}},
	Url = {http://proceedings.mlr.press/v51/li16f.pdf http://arxiv.org/abs/1509.01618},
	Volume = {51},
	Year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v51/li16f.pdf%20http://arxiv.org/abs/1509.01618}}

@article{AvGa13,
	Abstract = {Consider a finite weighted oriented graph. We study a probability measure on the set of spanning rooted oriented forests on the graph. We prove that the set of roots sampled from this measure is a determinantal process, characterized by a possibly non-symmetric kernel with complex eigenvalues. We then derive several results relating this measure to the Markov process associated with the starting graph, to the spectrum of its generator and to hitting times of subsets of the graph. In particular, the mean hitting time of the set of roots turns out to be independent of the starting point, conditioning or not to a given number of roots. Wilson's algorithm provides a way to sample this measure and, in absence of complex eigenvalues of the generator, we explain how to get samples with a number of roots approximating a prescribed integer. We also exploit the properties of this measure to give some probabilistic insight into the proof of an algebraic result due to Micchelli and Willoughby [13]. Further, we present two different related coalescence and fragmentation processes.},
	Author = {Avena, L and Gaudilli{\`{e}}re, A},
	File = {::},
	Journal = {e-prints},
	Keywords = {05C81,05C85 Keywords,15A15,15A18,60J20,Finite networks,MSC 2010,Wilson's algorithm,coalescence and frag-mentation,determinantal processes,hit-ting times,local equilibria,primary,random partitions,random sets,secondary,spanning forests},
	Title = {{On some random forests with determinantal roots}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.740.6173{\&}rep=rep1{\&}type=pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.740.6173%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{AfKuFo13,
	Abstract = {Determinantal point processes (DPPs) are appealing models for subset selection prob-lems where diversity is desired. They offer surprisingly efficient inference, including sam-pling in O(N 3) time and O(N 2) space, where N is the number of base items. However, in some applications, N may grow so large that sampling from a DPP becomes compu-tationally infeasible. This is especially true in settings where the DPP kernel matrix can-not be represented by a linear decomposition of low-dimensional feature vectors. In these cases, we propose applying the Nystr{\"{o}}m ap-proximation to project the kernel matrix into a low-dimensional space. While theoretical guarantees for the Nystr{\"{o}}m approximation in terms of standard matrix norms have been previously established, we are concerned with probabilistic measures, like total variation dis-tance between the DPP and its Nystr{\"{o}}m ap-proximation, that behave quite differently. In this paper we derive new error bounds for the Nystr{\"{o}}m-approximated DPP and present em-pirical results to corroborate them. We then demonstrate the Nystr{\"{o}}m-approximated DPP by applying it to a motion capture summa-rization task.},
	Address = {Scottsdale, AZ, USA},
	Author = {Affandi, Raja Hafiz and Kulesza, Alex and Fox, Emily B and Taskar, Ben},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	File = {::},
	Issn = {15337928},
	Keywords = {dblp},
	Pages = {85--98},
	Title = {{Nystrom Approximation for Large-Scale Determinantal Processes.}},
	Url = {http://jmlr.org/proceedings/papers/v31/affandi13a.html},
	Volume = {31},
	Year = {2013},
	Bdsk-Url-1 = {http://jmlr.org/proceedings/papers/v31/affandi13a.html}}

@article{EvLa10,
	Archiveprefix = {arXiv},
	Arxivid = {0912.4389},
	Author = {Evans, T S and Lambiotte, R},
	Doi = {10.1140/epjb/e2010-00261-8},
	Eprint = {0912.4389},
	File = {::},
	Keywords = {05,40,75,89,community detection,edge partition,fb random walks and,fb structures and organization,hc networks and genealogical,in complex,levy flights,line graphs,overlapping communities,pacs,systems,trees,vertex cover},
	Pages = {1--8},
	Title = {{Line Graphs of Weighted Networks for Overlapping Communities(EPJB正式版).pdf}},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1140/epjb/e2010-00261-8}}

@article{LiJeSr16d,
	Abstract = {In this note we consider sampling from (non-homogeneous) strongly Rayleigh probability measures. As an important corollary, we obtain a fast mixing Markov Chain sampler for Determinantal Point Processes.},
	Annote = {NULL},
	Archiveprefix = {arXiv},
	Arxivid = {1607.03559},
	Author = {Li, Chengtao and Jegelka, Stefanie and Sra, Suvrit},
	Eprint = {1607.03559},
	File = {::},
	Journal = {ArXiv e-prints},
	Title = {{Fast Sampling for Strongly Rayleigh Measures with Application to Determinantal Point Processes}},
	Url = {http://arxiv.org/abs/1607.03559},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1607.03559}}

@article{kulesza2011kdpps,
	Abstract = {Determinantal point processes ( DPPs ) have recently been proposed$\backslash$nas models for set selection problems where diversity is pre- ferred.$\backslash$nFor example, they can be used to select diverse sets of sentences$\backslash$nto form doc- ument summaries, or to find multiple non- overlapping$\backslash$nhuman ...},
	Author = {Kulesza, Alex and Taskar, Ben},
	Journal = {International Conference on Machine Learning},
	Pages = {1193--1200},
	Title = {{k-DPPs: Fixed-Size Determinantal Point Processes}},
	Year = {2011}}

@article{GaPaKo16,
	Abstract = {Determinantal point processes (DPPs) have garnered attention as an elegant probabilistic model of set diversity. They are useful for a number of subset selection tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. In this work we present a new method for learning the DPP kernel from observed data using a low-rank factorization of this kernel. We show that this low-rank factorization enables a learning algorithm that is nearly an order of magnitude faster than previous approaches, while also providing for a method for computing product recommendation predictions that is far faster (up to 20x faster or more for large item catalogs) than previous techniques that involve a full-rank DPP kernel. Furthermore, we show that our method provides equivalent or sometimes better predictive performance than prior full-rank DPP approaches, and better performance than several other competing recommendation methods in many cases. We conduct an extensive experimental evaluation using several real-world datasets in the domain of product recommendation to demonstrate the utility of our method, along with its limitations.},
	Archiveprefix = {arXiv},
	Arxivid = {1602.05436},
	Author = {Gartrell, Mike and Paquet, Ulrich and Koenigstein, Noam},
	Eprint = {1602.05436},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Gartrell, Paquet, Koenigstein - 2017 - Low-Rank Factorization of Determinantal Point Processes for Recommendation.pdf:pdf},
	Pages = {10},
	Title = {{Low-Rank Factorization of Determinantal Point Processes for Recommendation}},
	Url = {http://arxiv.org/abs/1602.05436},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1602.05436}}

@inproceedings{GaBaVa17,
	Abstract = {Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers. We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costly-to-evaluate functions, such as summary extraction in our experiments.},
	Address = {Sydney, Australia},
	Archiveprefix = {arXiv},
	Arxivid = {1705.10498},
	Author = {Gautier, Guillaume and Bardenet, R{\'{e}}mi and Valko, Michal},
	Booktitle = {International Conference on Machine Learning},
	Editor = {Precup, Doina and Teh, Yee Whye},
	Eprint = {1705.10498},
	File = {::},
	Pages = {1223--1232},
	Publisher = {PMLR},
	Title = {{Zonotope hit-and-run for efficient sampling from projection DPPs}},
	Url = {http://proceedings.mlr.press/v70/gautier17a/gautier17a.pdf http://arxiv.org/abs/1705.10498},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/gautier17a/gautier17a.pdf%20http://arxiv.org/abs/1705.10498}}

@article{Affandi2014a,
	Abstract = {Determinantal point processes (DPPs) are well- suited for modeling repulsion and have proven useful in applications where diversity is desired. While DPPs have many appealing properties, learning the parameters of a DPP is difficult, as the likelihood is non-convex and is infeasible to compute in many scenarios. Here we propose Bayesian methods for learning the DPP kernel parameters. These methods are applicable in large- scale discrete and continuous DPP settings, even when the likelihood can only be bounded. We demonstrate the utility of our DPP learning methods in studying the progression of diabetic neuropathy based on the spatial distribution of nerve fibers, and in studying human perception of diversity in images.},
	Archiveprefix = {arXiv},
	Arxivid = {1402.4862},
	Author = {Affandi, Raja Hafiz and Fox, Emily B. and Adams, Ryan P. and Taskar, Ben},
	Eprint = {1402.4862},
	Isbn = {9781634393973},
	Journal = {International Conference on Machine Learning},
	Number = {1},
	Pages = {2967--2981},
	Title = {{Learning the parameters of determinantal point process kernels}},
	Url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84919798123{\&}partnerID=tZOtx3y1},
	Volume = {4},
	Year = {2014},
	Bdsk-Url-1 = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84919798123%7B%5C&%7DpartnerID=tZOtx3y1}}

@inproceedings{LiJeSr16a,
	Abstract = {The Nystr$\backslash$"om method has long been popular for scaling up kernel methods. Its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected. We study landmark selection for Nystr$\backslash$"om using Determinantal Point Processes (DPPs), discrete probability models that allow tractable generation of diverse samples. We prove that landmarks selected via DPPs guarantee bounds on approximation errors; subsequently, we analyze implications for kernel ridge regression. Contrary to prior reservations due to cubic complexity of DPPsampling, we show that (under certain conditions) Markov chain DPP sampling requires only linear time in the size of the data. We present several empirical results that support our theoretical analysis, and demonstrate the superior performance of DPP-based landmark selection compared with existing approaches.},
	Address = {New York, USA},
	Archiveprefix = {arXiv},
	Arxivid = {1603.06052},
	Author = {Li, Chengtao and Jegelka, Stefanie and Sra, Suvrit},
	Booktitle = {International Conference on Machine Learning},
	Eprint = {1603.06052},
	File = {::},
	Month = {mar},
	Title = {{Fast DPP Sampling for Nystr$\backslash$"om with Application to Kernel Methods}},
	Url = {http://proceedings.mlr.press/v48/lih16.html http://arxiv.org/abs/1603.06052},
	Volume = {48},
	Year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v48/lih16.html%20http://arxiv.org/abs/1603.06052}}

@article{KaDe16,
	Abstract = {Subset selection problems ask for a small, diverse yet representative subset of the given data. When pairwise similarities are captured by a kernel, the determinants of submatrices provide a measure of diversity or independence of items within a subset. Matroid theory gives another notion of independence, thus giving rise to optimization and sampling questions about Determinantal Point Processes (DPPs) under matroid constraints. Partition constraints, as a special case, arise naturally when incorporating additional labeling or clustering information, besides the kernel, in DPPs. Finding the maximum determinant submatrix under matroid constraints on its row/column indices has been previously studied. However, the corresponding question of sampling from DPPs under matroid constraints has been unresolved, beyond the simple cardinality constrained k-DPPs. We give the first polynomial time algorithm to sample exactly from DPPs under partition constraints, for any constant number of partitions. We complement this by a complexity theoretic barrier that rules out such a result under general matroid constraints. Our experiments indicate that partition-constrained DPPs offer more flexibility and more diversity than k-DPPs and their naive extensions, while being reasonably efficient in running time. We also show that a simple greedy initialization followed by local search gives improved approximation guarantees for the problem of MAP inference from k- DPPs on well-conditioned kernels. Our experiments show that this improvement is significant for larger values of k, supporting our theoretical result.},
	Archiveprefix = {arXiv},
	Arxivid = {1607.01551},
	Author = {Kathuria, Tarun and Deshpande, Amit},
	Eprint = {1607.01551},
	File = {::},
	Number = {Nips},
	Title = {{On Sampling and Greedy MAP Inference of Constrained Determinantal Point Processes}},
	Url = {http://arxiv.org/abs/1607.01551},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1607.01551}}

@article{BaTi15,
	Abstract = {Determinantal point processes (DPPs) are point process models that naturally encode diversity between the points of a given realization, through a positive definite kernel {\$}K{\$}. DPPs possess desirable properties, such as exact sampling or analyticity of the moments, but learning the parameters of kernel {\$}K{\$} through likelihood-based inference is not straightforward. First, the kernel that appears in the likelihood is not {\$}K{\$}, but another kernel {\$}L{\$} related to {\$}K{\$} through an often intractable spectral decomposition. This issue is typically bypassed in machine learning by directly parametrizing the kernel {\$}L{\$}, at the price of some interpretability of the model parameters. We follow this approach here. Second, the likelihood has an intractable normalizing constant, which takes the form of a large determinant in the case of a DPP over a finite set of objects, and the form of a Fredholm determinant in the case of a DPP over a continuous domain. Our main contribution is to derive bounds on the likelihood of a DPP, both for finite and continuous domains. Unlike previous work, our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator. Through usual arguments, these bounds thus yield cheap variational inference and moderately expensive exact Markov chain Monte Carlo inference methods for DPPs.},
	Archiveprefix = {arXiv},
	Arxivid = {1507.01154},
	Author = {Bardenet, R{\'{e}}mi and Titsias, Michalis K.},
	Eprint = {1507.01154},
	File = {::},
	Month = {jul},
	Title = {{Inference for determinantal point processes without spectral knowledge}},
	Url = {http://arxiv.org/abs/1507.01154},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1507.01154}}

@article{MaSr15,
	Abstract = {Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique.},
	Archiveprefix = {arXiv},
	Arxivid = {1508.00792},
	Author = {Mariet, Zelda and Sra, Suvrit},
	Eprint = {1508.00792},
	File = {::},
	Month = {aug},
	Title = {{Fixed-point algorithms for learning determinantal point processes}},
	Url = {http://arxiv.org/abs/1508.00792},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1508.00792}}

@article{Agarwal2014,
	Abstract = {In this paper, we compare three initialization schemes for the KMEANS clustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and 3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k needs to be set by the user of the algorithms. (Kang 2013) recently proposed a novel use of determinantal point processes for sampling the initial centroids for the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide any evaluation establishing that KMEANSD++ is better than other algorithms. In this paper, we show that the performance of KMEANSD++ is comparable to KMEANS++ (both of which are better than KMEANSRAND) with KMEANSD++ having an additional that it can automatically approximate the value of k.},
	Archiveprefix = {arXiv},
	Arxivid = {1410.6975},
	Author = {Agarwal, Apoorv and Choromanska, Anna and Choromanski, Krzysztof},
	Eprint = {1410.6975},
	File = {::},
	Month = {oct},
	Title = {{Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering}},
	Url = {http://arxiv.org/abs/1410.6975},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1410.6975}}

@inproceedings{LiJeSr16c,
	Abstract = {We study probability measures induced by set functions with constraints. Such measures arise in a variety of real-world settings, where prior knowledge, resource limitations, or other pragmatic considerations impose constraints. We consider the task of rapidly sampling from such constrained measures, and develop fast Markov chain samplers for them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures, for which we present sharp polynomial bounds on the mixing time. As a corollary, this result yields a fast mixing sampler for Determinantal Point Processes (DPPs), yielding (to our knowledge) the first provably fast MCMC sampler for DPPs since their inception over four decades ago. Beyond SR measures, we develop MCMC samplers for probabilistic models with hard constraints and identify sufficient conditions under which their chains mix rapidly. We illustrate our claims by empirically verifying the dependence of mixing times on the key factors governing our theoretical bounds.},
	Address = {Barcelona, Spain},
	Annote = {NULL},
	Archiveprefix = {arXiv},
	Arxivid = {1608.01008},
	Author = {Li, Chengtao and Jegelka, Stefanie and Sra, Suvrit},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1608.01008},
	File = {::},
	Issn = {10495258},
	Number = {2},
	Title = {{Fast Mixing Markov Chains for Strongly Rayleigh Measures, DPPs, and Constrained Sampling}},
	Url = {https://papers.nips.cc/paper/6182-fast-mixing-markov-chains-for-strongly-rayleigh-measures-dpps-and-constrained-sampling http://arxiv.org/abs/1608.01008},
	Year = {2016},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/6182-fast-mixing-markov-chains-for-strongly-rayleigh-measures-dpps-and-constrained-sampling%20http://arxiv.org/abs/1608.01008}}

@article{StVi16,
	Abstract = {A determinantal point process (DPP) over a universe {\$}\backslash{\{}1,\backslashldots,m\backslash{\}}{\$} with respect to an {\$}m \backslashtimes m{\$} positive semidefinite matrix {\$}L{\$} is a probability distribution where the probability of a subset {\$}S \backslashsubseteq \backslash{\{}1,\backslashldots,m\backslash{\}}{\$} is proportional to the determinant of the principal minor of {\$}L{\$} corresponding to {\$}S.{\$} DPPs encapsulate a wide variety of known distributions and appear naturally (and surprisingly) in a wide variety of areas such as physics, mathematics and computer science. Several applications that use DPPs rely on the fact that they are computationally tractable -- i.e., there are algorithms for sampling from DPPs efficiently. Recently, there is growing interest in studying a generalization of DPPs in which the support of the distribution is a restricted family B of subsets of {\$}\backslash{\{}1,2,\backslashldots, m\backslash{\}}{\$}. Mathematically, these distributions, which we call generalized DPPs, include the well-studied hardcore distributions as special cases (when {\$}L{\$} is diagonal). In applications, they can be used to refine models based on DPPs by imposing combinatorial constraints on the support of the distribution. In this paper we take first steps in a systematic study of computational questions concerning generalized DPPs. We introduce a natural class of linear families: roughly, a family B is said to be linear if there is a collection of {\$}p{\$} linear forms that all elements of B satisfy. Important special cases of linear families are all sets of cardinality {\$}k{\$} -- giving rise to {\$}k{\$}-DPPs -- and, more generally, partition matroids. On the positive side, we prove that, when {\$}p{\$} is a constant, there is an efficient, exact sampling algorithm for linear DPPs. We complement these results by proving that, when {\$}p{\$} is large, the computational problem related to such DPPs becomes {\$}\backslash{\#}{\$}P-hard. Our proof techniques rely and build on the interplay between polynomials and probability distributions.},
	Archiveprefix = {arXiv},
	Arxivid = {1608.00554},
	Author = {Straszak, Damian and Vishnoi, Nisheeth K.},
	Eprint = {1608.00554},
	File = {::},
	Month = {aug},
	Title = {{Generalized Determinantal Point Processes: The Linear Case}},
	Url = {http://arxiv.org/abs/1608.00554},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1608.00554}}

@inproceedings{ChZhZh17,
	Abstract = {Recommender systems take the key responsibility to help users discover items that they might be interested in. Many recommenda-tion algorithms are built upon similarity measures, which usually result in low intra-list diversity. The deficiency in capturing the whole range of user interest often leads to poor satisfaction. To solve this problem, increasing attention has been paid on improving the diversity of recommendation results in recent years. In this paper, we propose a novel method to improve the diversity of top-N recommendation results based on the determinantal point process (DPP), which is an elegant model for characterizing the repulsion phenomenon. We propose an acceleration algorithm to greatly speed up the process of the result inference, making our algorithm practical for large-scale scenarios. We also incorporate a tunable parameter into the DPP model which allows the users to smoothly control the level of diversity. More diversity metrics are introduced to better evaluate diversification algorithms. We have evaluated our algorithm on several public datasets, and compared it thoroughly with other reference algorithms. Results show that our proposed algorithm provides a much better accuracy-diversity trade-off with comparable efficiency.},
	Address = {Como, Italy},
	Archiveprefix = {arXiv},
	Arxivid = {1709.05135},
	Author = {Chen, Laming and Zhang, Guoxin and Zhou, Hanning},
	Booktitle = {ACM Conference on Recommender Systems, Large Scale Recommendation System Workshop},
	Eprint = {1709.05135},
	File = {::},
	Title = {{Improving the Diversity of Top-N Recommendation via Determinantal Point Process}},
	Url = {https://arxiv.org/pdf/1709.05135.pdf},
	Volume = {8},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1709.05135.pdf}}

@inproceedings{GiKuTa13,
	Abstract = {Determinantal point processes (DPPs) have recently been proposed as computa-tionally efficient probabilistic models of diverse sets for a variety of applications, including document summarization, image search, and pose estimation. Many DPP inference operations, including normalization and sampling, are tractable; however, finding the most likely configuration (MAP), which is often required in practice for decoding, is NP-hard, so we must resort to approximate inference. This optimization problem, which also arises in experimental design and sensor placement, involves finding the largest principal minor of a positive semidefinite matrix. Because the objective is log-submodular, greedy algorithms have been used in the past with some empirical success; however, these methods only give approximation guarantees in the special case of monotone objectives, which cor-respond to a restricted class of DPPs. In this paper we propose a new algorithm for approximating the MAP problem based on continuous techniques for submod-ular function maximization. Our method involves a novel continuous relaxation of the log-probability function, which, in contrast to the multilinear extension used for general submodular functions, can be evaluated and differentiated exactly and efficiently. We obtain a practical algorithm with a 1/4-approximation guarantee for a more general class of non-monotone DPPs; our algorithm also extends to MAP inference under complex polytope constraints, making it possible to com-bine DPPs with Markov random fields, weighted matchings, and other models. We demonstrate that our approach outperforms standard and recent methods on both synthetic and real-world data.},
	Author = {Gillenwater, Jennifer and Kulesza, Alex and Taskar, Ben},
	Booktitle = {Nips},
	File = {::},
	Isbn = {9781627480031},
	Issn = {10495258},
	Pages = {1--9},
	Title = {{Near-Optimal MAP Inference for Determinantal Point Processes}},
	Url = {http://www.cis.upenn.edu/{~}taskar/pubs/dppmap{\_}nips12.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://www.cis.upenn.edu/%7B~%7Dtaskar/pubs/dppmap%7B%5C_%7Dnips12.pdf}}

@article{KaDeKo16,
	Abstract = {Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function requires training a model which may involve days or even weeks of computation. Most methods for this so-called "Bayesian optimization" only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.},
	Archiveprefix = {arXiv},
	Arxivid = {1611.04088},
	Author = {Kathuria, Tarun and Deshpande, Amit and Kohli, Pushmeet},
	Eprint = {1611.04088},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Kathuria, Deshpande, Kohli - 2016 - Batched Gaussian Process Bandit Optimization via Determinantal Point Processes(2).pdf:pdf},
	Pages = {1--16},
	Title = {{Batched Gaussian Process Bandit Optimization via Determinantal Point Processes}},
	Url = {http://arxiv.org/abs/1611.04088},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1611.04088}}

@inproceedings{ZhKjMa17,
	Abstract = {We study a mini-batch diversification scheme for stochastic gradient descent (SGD). While classi-cal SGD relies on uniformly sampling data points to form a mini-batch, we propose a non-uniform sampling scheme based on the Determinantal Point Process (DPP). The DPP relies on a sim-ilarity measure between data points and gives low probabilities to mini-batches which contain redun-dant data, and higher probabilities to mini-batches with more diverse data. This simultaneously bal-ances the data and leads to stochastic gradients with lower variance. We term this approach Di-versified Mini-Batch SGD (DM-SGD). We show that regular SGD and a biased version of stratified sampling emerge as special cases. Furthermore, DM-SGD generalizes stratified sampling to cases where no discrete features exist to bin the data into groups. We show experimentally that our method results more interpretable and diverse features in unsupervised setups, and in better classification accuracies in supervised setups.},
	Address = {Sydney, Australia},
	Author = {Zhang, Cheng and Kjellstr{\"{o}}m, Hedvig and Mandt, Stephan},
	Booktitle = {Uncertainty in Artificial Intelligence},
	File = {::},
	Title = {{Determinantal Point Processes for Mini-Batch Diversification}},
	Url = {http://auai.org/uai2017/proceedings/papers/69.pdf},
	Year = {2017},
	Bdsk-Url-1 = {http://auai.org/uai2017/proceedings/papers/69.pdf}}

@article{ziebart2008maximum,
	Abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces the problem of learning to recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decisions, while providing the same performance guarantees as existing methods.We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
	Author = {Ziebart, Brian and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
	Editor = {Archer, M},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ziebart et al. - 2008 - Maximum Entropy Inverse Reinforcement Learning.pdf:pdf},
	Isbn = {9781577353683},
	Journal = {Proc AAAI},
	Keywords = {irl},
	Mendeley-Tags = {irl},
	Publisher = {AAAI Press},
	Title = {{Maximum Entropy Inverse Reinforcement Learning}},
	Url = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf}}

@article{ross2010reduction,
	Abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
	Author = {Ross, Stephane and Gordon, Geoffrey J and Bagnell, J Andrew},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ross, Gordon, Bagnell - 2010 - A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning.pdf:pdf},
	Journal = {AISTATS},
	Pages = {627--635},
	Title = {{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}},
	Url = {http://arxiv.org/abs/1011.0686},
	Volume = {15},
	Year = {2010},
	Bdsk-Url-1 = {http://arxiv.org/abs/1011.0686}}

@inproceedings{ng2000algorithms,
	Abstract = {This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behaviour. IRL may be useful for apprenticeship learning to acquire skilled behaviour, and for ascertaining the reward function being optimized by a natural system. We rst characterize the set of all reward functions for which a given policy is optimal. We then derive three algorithms for IRL. The rst two deal with the case where the entire policy is known; we handle tabulated reward functions on a nite state space and linear functional approximation of the reward function over a potentially in- nite state space. The third algorithm deals with the more realistic case in which the policy is known only through a nite set of observed trajectories. In all cases, a key issue is degeneracythe existence of a large set of reward functions for which the observed policy is optimal. To remove...},
	Author = {Ng, Andrew and Russell, Stuart},
	Booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	Doi = {10.2460/ajvr.67.2.323},
	Editor = {{De Sousa}, Jorge Pinho},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ng, Russell - 2000 - Algorithms for inverse reinforcement learning.pdf:pdf},
	Issn = {00029645},
	Pages = {663--670},
	Pmid = {16454640},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Algorithms for inverse reinforcement learning}},
	Url = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
	Year = {2000},
	Bdsk-Url-1 = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
	Bdsk-Url-2 = {https://doi.org/10.2460/ajvr.67.2.323}}

@inproceedings{CSS10,
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Shalev-Shwartz, Shai and Shamir, Ohad},
	Booktitle = {COLT},
	Pages = {218--231},
	Title = {{Online learning of noisy data with kernels}},
	Year = {2010}}

@inproceedings{bagnell2010efficient,
	Abstract = {Imitation Learning, while applied successfully on many large real-world$\backslash$nproblems, is typically addressed as a standard supervised learning$\backslash$nproblem, where it is assumed the training and testing data are i.i.d..$\backslash$nThis is not true in imitation learning as the learned policy influences$\backslash$nthe future test inputs (states) upon which it will be tested. We$\backslash$nshow that this leads to compounding errors and a regret bound that$\backslash$ngrows quadratically in the time horizon of the task. We propose two$\backslash$nalternative algorithms for imitation learning where training occurs$\backslash$nover several episodes of interaction. These two approaches share$\backslash$nin common that the learner's policy is slowly modified from executing$\backslash$nthe expert's policy to the learned policy. We show that this leads$\backslash$nto stronger performance guarantees and demonstrate the improved performance$\backslash$non two challenging problems: training a learner to play 1) a 3D racing$\backslash$ngame (Super Tux Kart) and 2) Mario Bros.; given input images from$\backslash$nthe games and corresponding actions taken by a human expert and near-optimal$\backslash$nplanner respectively.},
	Author = {Bagnell, J Andrew and Ross, St{\'{e}}phane},
	Booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010},
	Pages = {661--668},
	Title = {{Efficient Reductions for Imitation Learning}},
	Volume = {9},
	Year = {2010}}

@inproceedings{valko2012semi-supervised,
	Abstract = {In apprenticeship learning we aim to learn a good policy by observing the behavior of an expert or a set of experts. In particular, we consider the case where the expert acts so as to maximize an unknown reward function defined as a linear combination of a set of state features. In this paper, we consider the setting where we observe many sample trajectories (i.e., sequences of states) but only one or a few of them are labeled as experts' trajectories. We investigate the conditions under which the remaining unlabeled trajectories can help in learning a policy with a good performance. In particular, we define an extension to the max-margin inverse reinforcement learning proposed by Abbeel and Ng (2004) where, at each iteration, the max-margin optimization step is replaced by a semi-supervised optimization problem which favors classifiers separating clusters of trajectories. Finally, we report empirical results on two grid-world domains showing that the semi-supervised algorithm is able to output a better policy in fewer iterations than the related algorithm that does not take the unlabeled trajectories into account.},
	Author = {Valko, Michal and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
	Booktitle = {The 24th Journal of Machine Learning Research Proceedings of the 10th European Workshop on Reinforcement Learning},
	Month = {jun},
	Pages = {131--241},
	Publisher = {Sparc},
	Title = {{Semi-Supervised Apprenticeship Learning}},
	Url = {http://researchers.lille.inria.fr/{~}valko/hp/serve.php?what=publications/valko2012semi-supervised.pdf},
	Volume = {24},
	Year = {2012},
	Bdsk-Url-1 = {http://researchers.lille.inria.fr/%7B~%7Dvalko/hp/serve.php?what=publications/valko2012semi-supervised.pdf}}

@inproceedings{judah2012active,
	Author = {Judah, Kshitij and Fern, Alan Paul and Dietterich, Thomas Glenn},
	Booktitle = {AAAI Fall Symposium: Robots Learning Interactively from Human Teachers},
	Keywords = {dblp},
	Publisher = {AAAI},
	Series = {AAAI Technical Report},
	Title = {{Active Imitation Learning via Reduction to I.I.D. Active Learning.}},
	Url = {http://dblp.uni-trier.de/db/conf/aaaifs/aaaifs2012-07.html{\#}JudahFD12},
	Volume = {FS-12-07},
	Year = {2012},
	Bdsk-Url-1 = {http://dblp.uni-trier.de/db/conf/aaaifs/aaaifs2012-07.html%7B%5C#%7DJudahFD12}}

@inproceedings{abbeel2004apprenticeship,
	Author = {Abbeel, Pieter and Ng, Andrew},
	Booktitle = {Proceedings of the 21st international conference on machine learning},
	Doi = {http://doi.acm.org/10.1145/1015330.1015430},
	Isbn = {1-58113-838-5},
	Keywords = {irl},
	Mendeley-Tags = {irl},
	Title = {{Apprenticeship learning via inverse reinforcement learning}},
	Url = {http://www.eecs.harvard.edu/{~}parkes/cs286r/spring06/papers/abeelng.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.eecs.harvard.edu/%7B~%7Dparkes/cs286r/spring06/papers/abeelng.pdf},
	Bdsk-Url-2 = {http://doi.acm.org/10.1145/1015330.1015430}}

@article{nino-nora2010computing,
	Author = {Nino-Mora, J},
	Doi = {10.1287/ijoc.1100.0398},
	Issn = {10919856},
	Journal = {INFORMS Journal on Computing},
	Keywords = {accepted may 2010,accepted winfried grassmann,advance,analysis algorithms,area editor computational,bandits,computational complexity,dynamic programming,finite horizon,history,index policies,march 2009,markov,may 2010,probability analysis,published online articles,received,revised january 2010},
	Number = {2},
	Pages = {254--267},
	Title = {{Computing a Classic Index for Finite-Horizon Bandits}},
	Url = {http://joc.journal.informs.org/cgi/doi/10.1287/ijoc.1100.0398},
	Volume = {23},
	Year = {2010},
	Bdsk-Url-1 = {http://joc.journal.informs.org/cgi/doi/10.1287/ijoc.1100.0398},
	Bdsk-Url-2 = {https://doi.org/10.1287/ijoc.1100.0398}}

@inproceedings{cesa-bianchi2013gang,
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio and Zappella, Giovanni},
	Booktitle = {Neural Information Processing Systems},
	Title = {{A gang of bandits}},
	Url = {https://papers.nips.cc/paper/5006-a-gang-of-bandits.pdf},
	Year = {2013},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/5006-a-gang-of-bandits.pdf}}

@phdthesis{bubeck2010jeux,
	Author = {Bubeck, S{\'{e}}bastien},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	School = {Universit{\{}{\'{e}}{\}} des Sciences et des Technologies de Lille 1},
	Title = {{Jeux de bandits et fondations du clustering}},
	Year = {2010}}

@inproceedings{thrun2003,
	Address = {Vancouver, British Columbia, Canada},
	Booktitle = {Proceedings of the 17th conference on advances in Neural Information Processing Systems},
	Editor = {Thrun, Sebastian and Saul, Lawrence K and Sch{\"{o}}lkopf, Bernhard},
	Isbn = {0-262-20152-6},
	Month = {dec},
	Publisher = {MIT Press},
	Series = {NIPS '03},
	Title = {{No Title}},
	Year = {2003}}

@article{russo2014learning,
	Author = {Russo, Daniel and {Van Roy}, Benjamin},
	Journal = {Mathematics of Operations Research},
	Title = {{Learning to Optimize Via Posterior Sampling}},
	Year = {2014}}

@article{bouneffouf2016multi-armed,
	Author = {Bouneffouf, Djallel and F{\'{e}}raud, Raphael},
	Doi = {10.1016/j.neucom.2016.02.052},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Number = {C},
	Pages = {16--21},
	Title = {{Multi-armed bandit problem with known trend}},
	Volume = {205},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neucom.2016.02.052}}

@inproceedings{buccapatnam2014stochastic,
	Author = {Buccapatnam, Swapna and Eryilmaz, Atilla and Shroff, Ness B.},
	Booktitle = {International Conference on Measurement and Modeling of Computer Systems},
	Title = {{Stochastic bandits with side observations on networks}},
	Url = {https://www.orie.cornell.edu/orie/research/groups/multheavytail/upload/mabSigfinal.pdf},
	Year = {2014},
	Bdsk-Url-1 = {https://www.orie.cornell.edu/orie/research/groups/multheavytail/upload/mabSigfinal.pdf}}

@article{may2012optimistic,
	Author = {May, Benedict C. and Korda, Nathaniel and Lee, Anthony and Leslie, David S.},
	Journal = {Journal of Machine Learning Research},
	Number = {1},
	Pages = {2069--2106},
	Title = {{Optimistic Bayesian sampling in contextual-bandit problems}},
	Url = {http://www.jmlr.org/papers/volume13/may12a/may12a.pdf},
	Volume = {13},
	Year = {2012},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume13/may12a/may12a.pdf}}

@inproceedings{bartlettadaptive,
	Author = {Bartlett, Peter L and Hazan, Elad and Rakhlin, Alexander},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {65--72},
	Title = {{Adaptive Online Gradient Descent.}}}

@article{dudik2011efficient,
	Abstract = {We address the problem of learning in an online setting where the learner repeatedly observes features, selects among a set of actions, and receives reward for the action taken. We provide the first efficient algorithm with an optimal regret. Our algorithm uses a cost sensitive classification learner as an oracle and has a running time mathrmpolylog(N), where N is the number of classification rules among which the oracle might choose. This is exponentially faster than all previous algorithms that achieve optimal regret in this setting. Our formulation also enables us to create an algorithm with regret that is additive rather than multiplicative in feedback delay as in all previous work.},
	Author = {Dudik, Miroslav and Hsu, Daniel and Kale, Satyen and Karampatziakis, Nikos and Langford, John and Reyzin, Lev and Zhang, Tong},
	Journal = {Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence},
	Title = {{Efficient Optimal Learning for Contextual Bandits}},
	Url = {http://arxiv.org/abs/1106.2369},
	Year = {2011},
	Bdsk-Url-1 = {http://arxiv.org/abs/1106.2369}}

@article{awerbuch2008online,
	Address = {Orlando, FL, USA},
	Author = {Awerbuch, Baruch and Kleinberg, Robert D},
	Issn = {0022-0000},
	Journal = {Journal of Computer Systems and Science},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {feb},
	Number = {1},
	Pages = {97--114},
	Publisher = {Academic Press, Inc.},
	Title = {{Online linear optimization and adaptive routing}},
	Volume = {74},
	Year = {2008}}

@inproceedings{singla2015information,
	Abstract = {How should we gather information in a network, where each node's visibility is limited to its local neighborhood? This problem arises in numerous real-world applications, such as surveying and task routing in social networks, team formation in collaborative networks and experimental design with dependency constraints. Often the informativeness of a set of nodes can be quantified via a submodular utility function. Existing approaches for submodular optimization, however, require that the set of all nodes that can be selected is known ahead of time, which is often unrealistic. In contrast, we propose a novel model where we start our exploration from an initial node, and new nodes become visible and available for selection only once one of their neighbors has been chosen. We then present a general algorithm NetExp for this problem, and provide theoretical bounds on its performance dependent on structural properties of the underlying network. We evaluate our methodology on various simulated problem instances as well as on data collected from social question answering system deployed within a large enterprise.},
	Author = {Singla, Adish and Horvitz, Eric and Kohli, Pushmeet and White, Ryen and Krause, Andreas},
	Booktitle = {International Joint Conferences on Artificial Intelligence},
	Title = {{Information gathering in networks via active exploration}},
	Year = {2015}}

@inproceedings{lafferty2010,
	Address = {Vancouver, British Columbia, Canada},
	Booktitle = {Proceedings of the 24th conference on advances in Neural Information Processing Systems},
	Editor = {Lafferty, John D and Williams, Chris K I and Shawe-Taylor, John and Zemel, Richard S and Culotta, Aron},
	Month = {dec},
	Series = {NIPS '10},
	Title = {{No Title}},
	Year = {2010}}

@inproceedings{agrawal2011analysis,
	Author = {Agrawal, Shipra and Goyal, Navin},
	Booktitle = {Conference on Learning Theory},
	Title = {{Analysis of Thompson sampling for the multi-armed bandit problem}},
	Url = {http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf}}

@inproceedings{defreitas2012exponential,
	Abstract = {This paper analyzes the problem of Gaussian process (GP) bandits with deterministic observations. The analysis uses a branch and bound algorithm that is related to the UCB algorithm of (Srinivas et al, 2010). For GPs with Gaussian observation noise, with variance strictly greater than zero, Srinivas et al proved that the regret vanishes at the approximate rate of {\$}O(1/\backslashsqrt{\{}t{\}}){\$}, where t is the number of observations. To complement their result, we attack the deterministic case and attain a much faster exponential convergence rate. Under some regularity assumptions, we show that the regret decreases asymptotically according to {\$}O(e{\^{}}{\{}-\backslashfrac{\{}\backslashtau t{\}}{\{}(\backslashln t){\^{}}{\{}d/4{\}}{\}}{\}}){\$} with high probability. Here, d is the dimension of the search space and tau is a constant that depends on the behaviour of the objective function near its global maximum.},
	Archiveprefix = {arXiv},
	Arxivid = {1206.6457},
	Author = {de Freitas, Nando and Smola, Alex and Zoghi, Masrour},
	Booktitle = {International Conference on Machine Learning},
	Eprint = {1206.6457},
	Isbn = {978-1-4503-1285-1},
	Title = {{Exponential regret bounds for Gaussian process bandits with deterministic observations}},
	Url = {http://arxiv.org/abs/1206.6457},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1206.6457}}

@inproceedings{kocsis2006bandit,
	Abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
	Author = {Kocsis, Levente and Szepesv{\'{a}}ri, Csaba},
	Booktitle = {European Conference on Machine Learning},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Bandit-based Monte-Carlo planning}},
	Url = {http://ggp.stanford.edu/readings/uct.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://ggp.stanford.edu/readings/uct.pdf}}

@unpublished{honda2010asymptotically,
	Annote = {arXiv:0905.2776},
	Author = {Honda, Junya and Takemura, Akimichi},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{An Asymptotically Optimal Policy for Finite Support Models in the Multiarmed Bandit Problem}},
	Year = {2010}}

@inproceedings{bnaya2013bandit,
	Author = {Bnaya, Zahy and Puzis, Rami and Stern, Roni and Felner, Ariel},
	Booktitle = {International Conference on Social Computing},
	Keywords = {Data mining,Educational institutions,Equations,Heuristic algorithms,Heuristics Search,Mathematical model,Multi-armed bandit,Social Network Intelligence,Social network services,Tin,VMAB,bandit algorithms,multiarmed bandit problem with volatile arms,profiles matching,query processing,search criterion,search engines,social network crawler,social network queries,social networking (online),targeted crawling},
	Title = {{Bandit algorithms for social network queries}},
	Year = {2013}}

@inproceedings{billsus2000learning,
	Author = {Billsus, Daniel and Pazzani, Michael J. and Chen, James},
	Booktitle = {International Conference on Intelligent User Interfaces},
	Title = {{A learning agent for wireless news access}},
	Url = {https://www.ics.uci.edu/{~}pazzani/Publications/billsuspazzanichen.pdf},
	Year = {2000},
	Bdsk-Url-1 = {https://www.ics.uci.edu/%7B~%7Dpazzani/Publications/billsuspazzanichen.pdf}}

@article{Gopalan2013a,
	Abstract = {We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms' rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets.},
	Author = {Gopalan, Aditya and Mannor, Shie and Mansour, Yishay},
	Month = {nov},
	Title = {{Thompson Sampling for Complex Bandit Problems}},
	Url = {http://arxiv.org/abs/1311.0466},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1311.0466}}

@phdthesis{bubeck2010bandits,
	Author = {Bubeck, S{\'{e}}bastien},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	School = {Universit{\{}{\'{e}}{\}} de Lille 1},
	Title = {{Bandits Games and Clustering Foundations}},
	Year = {2010}}

@inproceedings{chu2011contextual,
	Author = {Chu, Lei and Li, Lihong and Reyzin, Lev and Schapire, Robert E},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Chu et al. - 2011 - Contextual Bandits with Linear Payoff Functions.pdf:pdf},
	Title = {{Contextual bandits with linear payoff functions}},
	Url = {http://proceedings.mlr.press/v15/chu11a/chu11a.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v15/chu11a/chu11a.pdf}}

@inproceedings{bubeck2009pure,
	Abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of strategies that perform an online exploration of the arms. The strategies are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. The main result is that the required explorationexploitation trade-offs are qualitatively different, in view of a general lower bound on the simple regret in terms of the cumulative regret.},
	Annote = {From Duplicate 1 ( Pure exploration in multi-armed bandits problems - Bubeck, S{\'{e}}bastien; Munos, R{\'{e}}mi; Stoltz, Gilles )

From Duplicate 1 ( Pure exploration in multi-armed bandits problems - Bubeck, S{\'{e}}bastien; Munos, R{\'{e}}mi; Stoltz, Gilles )





From Duplicate 4 ( Pure Exploration in Multi-armed Bandits Problems - Bubeck, S{\'{e}}bastien; Munos, R{\'{e}}mi; Stoltz, Gilles )
},
	Author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles},
	Booktitle = {Algorithmic Learning Theory},
	Keywords = {bandits,computational,information theoretic learning with statistics,theory {\&} algorithms},
	Mendeley-Tags = {bandits},
	Title = {{Pure exploration in multi-armed bandits problems}},
	Url = {https://arxiv.org/abs/0802.2655},
	Year = {2009},
	Bdsk-Url-1 = {https://arxiv.org/abs/0802.2655}}

@inproceedings{dani2008price,
	Author = {Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
	Booktitle = {Neural Information Processing Systems},
	Issn = {00368075},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Publisher = {MIT Press},
	Title = {{The Price of Bandit Information for Online Optimization}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.4607{\&}rep=rep1{\&}type=pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.4607%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{auer2007improved,
	Author = {Auer, Peter and Ortner, Ronald and Szepesv{\'{a}}ri, Csaba},
	Booktitle = {Conference on Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Improved rates for the stochastic continuum-armed bandit problem}},
	Url = {https://pdfs.semanticscholar.org/0bd8/cbe79bd1b6e408d916bcdf4cbed5c9ed58c6.pdf},
	Year = {2007},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/0bd8/cbe79bd1b6e408d916bcdf4cbed5c9ed58c6.pdf}}

@article{hazan2007logarithmic,
	Author = {Hazan, Elad and Agarwal, Amit and Kale, Satyen},
	Journal = {Machine Learning},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Number = {2-3},
	Pages = {169--192},
	Title = {{Logarithmic Regret Algorithms for Online Convex Optimization}},
	Volume = {69},
	Year = {2007}}

@article{rusmevichientong2010linearly,
	Address = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
	Author = {Rusmevichientong, Paat and Tsitsiklis, John N},
	Journal = {Math. Oper. Res.},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {may},
	Number = {2},
	Pages = {395--411},
	Publisher = {Informs},
	Title = {{Linearly Parameterized Bandits}},
	Volume = {35},
	Year = {2010}}

@article{burnetas1997optimal,
	Address = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
	Author = {Burnetas, Apostolos N and Katehakis, Micha{\"{e}}l N},
	Issn = {0364-765X},
	Journal = {Mathematics of Operations Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {feb},
	Number = {1},
	Pages = {222--255},
	Publisher = {INFORMS},
	Title = {{Optimal adaptive policies for Markov decision processes}},
	Volume = {22},
	Year = {1997}}

@inproceedings{garivier2016maximin,
	Abstract = {We study an original problem of pure exploration in a strategic bandit model motivated by Monte Carlo Tree Search. It consists in identifying the best action in a game, when the player may sample random outcomes of sequentially chosen pairs of actions. We propose two strategies for the fixed-confidence setting: Maximin-LUCB, based on lower-and upper-confidence bounds; and Maximin-Racing, which operates by successively eliminating the sub-optimal actions. We discuss the sample complexity of both methods and compare their performance empirically. We sketch a lower bound analysis, and possible connections to an optimal algorithm.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1602.04676v1},
	Author = {Garivier, Aur{\'{e}}lien and Kaufmann, Emilie and Koolen, Wouter M},
	Booktitle = {Conference on Learning Theory},
	Eprint = {arXiv:1602.04676v1},
	Title = {{Maximin action identification: A new bandit framework for games}},
	Year = {2016}}

@article{alon2014nonstochastic,
	Abstract = {We present and study a partial-information model of online learning, where a decision maker repeatedly chooses from a finite set of actions, and observes some subset of the associated losses. This naturally models several situations where the losses of different actions are related, and knowing the loss of one action provides information on the loss of other actions. Moreover, it generalizes and interpolates between the well studied full-information setting (where all losses are revealed) and the bandit setting (where only the loss of the action chosen by the player is revealed). We provide several algorithms addressing different variants of our setting, and provide tight regret bounds depending on combinatorial properties of the information feedback structure.},
	Archiveprefix = {arXiv},
	Arxivid = {1409.8428},
	Author = {Alon, Noga and Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio and Mannor, Shie and Mansour, Yishay and Shamir, Ohad},
	Eprint = {1409.8428},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Alon et al. - 2014 - Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback.pdf:pdf},
	Journal = {SIAM Journal on Computing},
	Number = {6},
	Pages = {1785--1826},
	Title = {{Nonstochastic multi-armed bandits with graph-structured feedback}},
	Url = {https://arxiv.org/abs/1409.8428},
	Volume = {46},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1409.8428}}

@article{tu2015bandit,
	Author = {Tu, Shi-tao and Zhu, Lan-juan},
	File = {:Users/miki/Downloads/art{\%}3A10.1007{\%}2Fs12204-015-1618-7.pdf:pdf},
	Journal = {Journal of Shanghai Jiaotong University (Science)},
	Number = {5},
	Pages = {535--539},
	Title = {{A bandit method using probabilistic matrix factorization in recommendation}},
	Volume = {20},
	Year = {2015}}

@inproceedings{ben-david2009agnostic,
	Author = {Ben-David, Shai and P{\'{a}}l, D{\'{a}}vid and Shalev-Shwartz, Shai},
	Booktitle = {22th annual conference on learning theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Agnostic Online Learning}},
	Year = {2009}}

@inproceedings{yue2012hierarchical,
	Address = {New York, NY, USA},
	Author = {Yue, Yisong and Hong, Sue A and Guestrin, Carlos},
	Booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
	Editor = {Langford, John and Pineau, Joelle},
	Pages = {1895--1902},
	Publisher = {ACM},
	Title = {{Hierarchical Exploration for Accelerating Contextual Bandits}},
	Url = {http://icml.cc/2012/papers/933.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://icml.cc/2012/papers/933.pdf}}

@article{bartok2011minimax,
	Abstract = {In a partial monitoring game, the learner repeatedly chooses an action, the environment responds with an outcome, and then the learner suffers a loss and receives a feedback signal, both of which are fixed functions of the action and the outcome. The goal of the learner is to minimize his regret, which is the difference between his total cumulative loss and the total loss of the best fixed action in hindsight. Assuming that the outcomes are generated in an i.i.d. fashion from an arbitrary and unknown probability distribution, we characterize the minimax regret of any partial monitoring game with finitely many actions and outcomes. It turns out that the minimax regret of any such game is either zero, {\&}z.Theta;̃(√T), {\&}z.Theta;(T2/3), or {\&}z.Theta;(T). We provide a computationally efficient learning algorithm that achieves the minimax regret within logarithmic factor for any game. {\textcopyright} 2011 G. Bart{\'{o}}k, D. P{\'{a}}l {\&} C. Szepesv{\'{a}}ri.},
	Author = {Bart{\'{o}}k, G{\'{a}}bor and P{\'{a}}l, D{\'{a}}vid and Szepesv{\'{a}}ri, Csaba},
	Journal = {Conference on Learning Theory},
	Keywords = {Imperfect feedback,Online learning,Regret analysis},
	Title = {{Minimax regret of finite partial-monitoring games in stochastic environments}},
	Year = {2011}}

@inproceedings{ortneronline,
	Author = {Ortner, Ronald},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {123--137},
	Title = {{Online Regret Bounds for Markov Decision Processes with Deterministic Transitions}}}

@article{auer2010ucb,
	Abstract = {In the stochastic multi-armed bandit problem we consider a modification of the UCB algorithm of Auer et al. 4. For this modified algorithm we give an improved bound on the regret with respect to the optimal reward. While for the original UCB algorithm the regret in K-armed bandits after T trials is bounded by const K log(T)/Delta, where Delta measures the distance between a suboptimal arm and the optimal arm, for the modified UCB algorithm we show an upper bound on the regret of const K log (T/Delta 2) Delta.},
	Author = {Auer, Peter and Ortner, Ronald},
	Journal = {Periodica Mathematica Hungarica},
	Keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
	Title = {{UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem}},
	Url = {http://personal.unileoben.ac.at/rortner/Pubs/UCBRev.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://personal.unileoben.ac.at/rortner/Pubs/UCBRev.pdf}}

@article{munos2002variable,
	Author = {Munos, R{\'{e}}mi and Moore, Andrew},
	Journal = {Machine Learning},
	Pages = {291--323},
	Title = {{Variable Resolution Discretization in Optimal Control}},
	Volume = {49},
	Year = {2002}}

@article{beygelzimer2010contextual,
	Abstract = {We address the problem of learning in an online, bandit setting where the learner must repeatedly select among K actions, but only receives partial feedback based on its choices. We establish two new facts: First, using a new algorithm called Exp4.P, we show that it is possible to compete with the best in a set of N experts with probability {\$}1-delta while incurring regret at most O(sqrtKTln(N/delta)) over T time steps. The new algorithm is tested empirically in a large-scale, real-world dataset. Second, we give a new algorithm called VE that competes with a possibly infinite set of policies of VC-dimension d while incurring regret at most O(sqrtT(dln(T) + ln (1/delta))) with probability {\$}1-delta. These guarantees improve on those of all previous algorithms, whether in a stochastic or adversarial environment, and bring us closer to providing supervised learning type guarantees for the contextual bandit setting.},
	Author = {Beygelzimer, Alina and Langford, John and Li, Lihong and Reyzin, Lev and Schapire, Robert E},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Beygelzimer et al. - 2010 - Contextual Bandit Algorithms with Supervised Learning Guarantees.pdf:pdf},
	Journal = {Machine Learning},
	Pages = {14},
	Title = {{Contextual Bandit Algorithms with Supervised Learning Guarantees}},
	Url = {http://arxiv.org/abs/1002.4058},
	Volume = {15},
	Year = {2010},
	Bdsk-Url-1 = {http://arxiv.org/abs/1002.4058}}

@inproceedings{gentile2017context,
	Author = {Gentile, Claudio and Li, Shuai and Kar, Purushottam and Karatzoglou, Alexandros and Zappella, Giovanni and Etrue, Evans},
	Booktitle = {International Conference on Machine Learning},
	Month = {jul},
	Title = {{On context-dependent clustering of bandits}},
	Url = {http://proceedings.mlr.press/v70/gentile17a/gentile17a.pdf},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/gentile17a/gentile17a.pdf}}

@article{cesa-bianchi2005minimizing,
	Abstract = {We investigate label efficient prediction, a variant, proposed by Helmbold and Panizza, of the problem of prediction with expert advice. In this variant, the forecaster, after guessing the next element of the sequence to be predicted, does not observe its true value unless he asks for it, which he cannot do too often. We determine matching upper and lower bounds for the best possible excess prediction error, with respect to the best possible constant predictor, when the number of allowed queries is fixed. We also prove that Hannan consistency, a fundamental property in game-theoretic prediction models, can be achieved by a forecaster issuing a number of queries growing to infinity at a rate just slightly faster than logarithmic in the number of prediction rounds.},
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Lugosi, G{\'{a}}bor and Stoltz, Gilles},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Individual sequences,Label efficient prediction,On-line learning,Prediction with expert advice},
	Number = {6},
	Pages = {2152--2162},
	Title = {{Minimizing regret with label efficient prediction}},
	Volume = {51},
	Year = {2005}}

@inproceedings{zinkevich2003online,
	Author = {Zinkevich, Martin},
	Booktitle = {Proceedings of the 20th International Conference on Machine Learning},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {928--936},
	Title = {{Online Convex Programming and Generalized Infinitesimal Gradient Ascent}},
	Year = {2003}}

@inproceedings{audibert2009minimax,
	Author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
	Booktitle = {Conference on Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Minimax policies for adversarial and stochastic bandits}},
	Url = {https://hal-enpc.archives-ouvertes.fr/hal-00834882/document},
	Year = {2009},
	Bdsk-Url-1 = {https://hal-enpc.archives-ouvertes.fr/hal-00834882/document}}

@inproceedings{koller2008,
	Address = {Vancouver, British Columbia, Canada},
	Booktitle = {Proceedings of the 22nd conference on advances in Neural Information Processing Systems},
	Editor = {Koller, Daphne and Schuurmans, Dale and Bengio, Yoshua and Bottou, L{\'{e}}on},
	Month = {dec},
	Publisher = {MIT Press},
	Series = {NIPS '08},
	Title = {{No Title}},
	Year = {2008}}

@inproceedings{yue2009k,
	Author = {Yue, Yisong and Broder, J and Kleinberg, R and Joachims, T},
	Booktitle = {22th annual conference on learning theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{The K-armed Dueling Bandits Problem}},
	Year = {2009}}

@inproceedings{neu2013efficient,
	Author = {Neu, Gergely and Bart{\'{o}}k, G{\'{a}}bor},
	Booktitle = {Algorithmic Learning Theory},
	Title = {{An efficient algorithm for learning with semi-bandit feedback}},
	Year = {2013}}

@article{Gopalan2013b,
	Abstract = {We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms' rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets.},
	Author = {Gopalan, Aditya and Mannor, Shie and Mansour, Yishay},
	Month = {nov},
	Title = {{Thompson Sampling for Complex Bandit Problems}},
	Url = {http://proceedings.mlr.press/v32/gopalan14.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v32/gopalan14.pdf}}

@inproceedings{danyluk2009,
	Address = {Montreal, Quebec, Canada},
	Booktitle = {Proceedings of the 26th International Conference on Machine Learning},
	Editor = {Danyluk, Andrea Pohoreckyj and Bottou, L{\'{e}}on and Littman, Michael L},
	Isbn = {978-1-60558-516-1},
	Month = {jun},
	Publisher = {ACM},
	Series = {ICML '09, ACM International Conference Proceeding Series},
	Title = {{No Title}},
	Volume = {382},
	Year = {2009}}

@inproceedings{freund2008,
	Address = {Budapest, Hungary},
	Booktitle = {Proceedings of the 19th international conference on Algorithmic Learning Theory},
	Editor = {Freund, Yoav and Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and Tur{\'{a}}n, Gy{\"{o}}rgy and Zeugmann, Thomas},
	Isbn = {978-3-540-87986-2},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {oct},
	Publisher = {Springer},
	Series = {ALT '08, Lecture Notes in Computer Science},
	Title = {{No Title}},
	Volume = {5254},
	Year = {2008}}

@article{mannor2004sample,
	Author = {Mannor, Shie and Tsitsiklis, John N},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {623--648},
	Title = {{The Sample Complexity of Exploration in the Multi-Armed Bandit Problem}},
	Volume = {5},
	Year = {2004}}

@inproceedings{kleinberg2005nearly,
	Author = {Kleinberg, Robert D},
	Booktitle = {Neural Information Processing Systems},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Nearly tight bounds for the continuum-armed bandit problem}},
	Url = {https://papers.nips.cc/paper/2634-nearly-tight-bounds-for-the-continuum-armed-bandit-problem.pdf},
	Year = {2005},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/2634-nearly-tight-bounds-for-the-continuum-armed-bandit-problem.pdf}}

@misc{snapnets,
	Author = {Leskovec, Jure and Krevl, Andrej},
	Howpublished = {$\backslash$url{\{}http://snap.stanford.edu/data{\}}},
	Month = {jun},
	Title = {{{\{}SNAP Datasets{\}}: {\{}Stanford{\}} Large Network Dataset Collection}},
	Year = {2014}}

@article{chan2018cbt,
	Abstract = {The infinite arms bandit problem was initiated by Berry et al. (1997). They derived a regret lower bound of all solutions for Bernoulli rewards, and proposed various bandit strategies based on success runs, but which do not achieve this bound. We propose here a confidence bound target (CBT) algorithm that achieves extensions of their regret lower bound for general reward distributions and distribution priors. The algorithm does not require information on the reward distributions, for each arm we require only the mean and standard deviation of its rewards to compute a confidence bound. We play the arm with the smallest confidence bound provided it is smaller than a target mean. If the confidence bounds are all larger, then we play a new arm. We show how the target mean can be computed from the prior so that the smallest asymptotic regret, among all infinite arms bandit algorithms, is achieved. We also show that in the absence of information on the prior, the target mean can be determined empirically, and that the regret achieved is comparable to the smallest regret. Numerical studies show that CBT is versatile and outperforms its competitors.},
	Archiveprefix = {arXiv},
	Arxivid = {1805.11793},
	Author = {Chan, Hock Peng and Hu, Shouri},
	Eprint = {1805.11793},
	File = {::},
	Title = {{Infinite arms bandit: Optimality via confidence bounds}},
	Url = {http://arxiv.org/pdf/1805.11793.pdf},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/pdf/1805.11793.pdf}}

@inproceedings{dasgupta2009,
	Address = {Montreal,Quebec, Canada},
	Booktitle = {Proceedings of the 22nd annual Conference On Learning Theory},
	Editor = {Dasgupta, Sanjot and Klivans, Adam},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jun},
	Series = {COLT '09},
	Title = {{No Title}},
	Year = {2009}}

@inproceedings{mannor2011from,
	Author = {Mannor, Shie and Shamir, Ohad},
	Booktitle = {Neural Information Processing Systems},
	Title = {{From bandits to experts: On the value of side-observations}},
	Url = {https://papers.nips.cc/paper/4366-from-bandits-to-experts-on-the-value-of-side-observations.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/4366-from-bandits-to-experts-on-the-value-of-side-observations.pdf}}

@inproceedings{lu2010contextual,
	Annote = {From Duplicate 1 ( Contextual Multi-Armed Bandits - Lu, Tyler; P{\'{a}}l, David; P{\'{a}}l, Martin )
And Duplicate 4 ( Contextual Multi-Armed Bandits - Lu, Tyler; P{\'{a}}l, David; P{\'{a}}l, Martin )

From Duplicate 1 ( Contextual Multi-Armed Bandits - Lu, Tyler; P{\'{a}}l, David; P{\'{a}}l, Martin )
},
	Author = {Lu, Tyler and P{\'{a}}l, D{\'{a}}vid and P{\'{a}}l, Martin},
	Booktitle = {Proceedings of the 13th international conference on Artificial Intelligence and Statistics},
	Editor = {Teh, Yee Whye and Titterington, Mike},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Lu, P{\'{a}}l, P{\'{a}}l - 2010 - Contextual Multi-Armed Bandits.pdf:pdf},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {485--492},
	Title = {{Contextual Multi-Armed Bandits}},
	Volume = {9},
	Year = {2010}}

@inproceedings{kawale2015efficient,
	Author = {Kawale, Jaya and Bui, Hung Hai and Kveton, Branislav and Tran-Thanh, Long and Chawla, Sanjay},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Efficient Thompson sampling for online matrix-factorization recommendation}},
	Year = {2015}}

@inproceedings{kveton2015tight,
	Author = {Kveton, Branislav and Wen, Zheng and Ashkan, Azin and Szepesvari, Csaba},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{Tight regret bounds for stochastic combinatorial semi-bandits}},
	Url = {http://proceedings.mlr.press/v38/kveton15.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v38/kveton15.pdf}}

@article{bubeck2013bandits,
	Author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicol{\`{o}} and Lugosi, G{\'{a}}bor},
	Doi = {10.1109/TIT.2013.2277869},
	Issn = {0018-9448},
	Journal = {Information Theory, IEEE Transactions on},
	Keywords = {Electronic mail,Equations,Heavy-tailed distributions,Indexes,Probability distribution,Random variables,Robustness,Standards,regret bounds,robust estimators,stochastic multi-armed bandit},
	Number = {11},
	Pages = {7711--7717},
	Title = {{Bandits With Heavy Tail}},
	Volume = {59},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIT.2013.2277869}}

@inproceedings{kalai2010,
	Booktitle = {Proceedings of the 23rd annual Conference On Learning Theory},
	Editor = {Kalai, Adam Tauman and Mohri, Mehryar},
	Isbn = {978-0-9822529-2-5},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jun},
	Publisher = {Omnipress},
	Title = {{No Title}},
	Year = {2010}}

@article{robbins1952some,
	Author = {Robbins, Herbert},
	Journal = {Bulletin of the American Mathematics Society},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {527--535},
	Title = {{Some aspects of the sequential design of experiments}},
	Volume = {58},
	Year = {1952}}

@inproceedings{wang2014bayesian,
	Abstract = {Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.},
	Archiveprefix = {arXiv},
	Arxivid = {1402.7005},
	Author = {Wang, Ziyu and Shakibi, Babak and Jin, Lin and de Freitas, Nando},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Eprint = {1402.7005},
	File = {::},
	Issn = {15337928},
	Month = {feb},
	Title = {{Bayesian Multi-Scale Optimistic Optimization}},
	Url = {http://arxiv.org/abs/1402.7005},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1402.7005}}

@inproceedings{desautels12parallelizing,
	Author = {Desautels, Thomas and Krause, Andreas and Burdick, Joel},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization}},
	Url = {https://icml.cc/2012//papers/602.pdf},
	Year = {2012},
	Bdsk-Url-1 = {https://icml.cc/2012//papers/602.pdf}}

@article{burnetas1996optimal,
	Author = {Burnetas, Apostolos N. and Katehakis, Micha{\"{e}}l N.},
	Journal = {Advances in Applied Mathematics},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {122--142},
	Title = {{Optimal adaptive policies for sequential allocation problems}},
	Volume = {17(2)},
	Year = {1996}}

@article{agrawal1995continuum,
	Author = {Agrawal, Rajeev},
	Journal = {SIAM Journal on Control and Optimization},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {1926--1951},
	Title = {{The continuum-armed bandit problem}},
	Url = {https://epubs.siam.org/doi/pdf/10.1137/S0363012992237273},
	Volume = {33},
	Year = {1995},
	Bdsk-Url-1 = {https://epubs.siam.org/doi/pdf/10.1137/S0363012992237273}}

@inproceedings{bubeck2012towards,
	Author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicol{\`{o}} and Kakade, Sham M.},
	Booktitle = {Conference on Learning Theory},
	Title = {{Towards minimax policies for online linear optimization with bandit feedback}},
	Url = {http://proceedings.mlr.press/v23/bubeck12a/bubeck12a.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v23/bubeck12a/bubeck12a.pdf}}

@inproceedings{combes2014unimodal,
	Author = {Combes, Richard and Prouti{\`{e}}re, Alexandre},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Unimodal bandits: Regret lower bounds and optimal algorithms}},
	Url = {http://proceedings.mlr.press/v32/combes14.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v32/combes14.pdf}}

@article{li2010contextual,
	Abstract = {Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5{\%} click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.},
	Author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2010 - A Contextual-Bandit Approach to Personalized News Article Recommendation.0146:0146},
	Institution = {ACM},
	Journal = {International World Wide Web Conference},
	Keywords = {contextual bandit,exploitation dilemma,exploration,personalization,recommender sys,tems,web service},
	Publisher = {ACM Press},
	Title = {{A contextual-bandit approach to personalized news article recommendation}},
	Url = {http://rob.schapire.net/papers/www10.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://rob.schapire.net/papers/www10.pdf}}

@inproceedings{abernethy2008competing,
	Author = {Abernethy, Jacob D and Hazan, Elad and Rakhlin, Alexander},
	Booktitle = {Conference on Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Competing in the dark: An efficient algorithm for bandit linear optimization.}},
	Url = {http://web.eecs.umich.edu/{~}jabernet/123-Abernethy.pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://web.eecs.umich.edu/%7B~%7Djabernet/123-Abernethy.pdf}}

@inproceedings{guillou2016scalable,
	Author = {Guillou, Fr{\'{e}}d{\'{e}}ric and Gaudel, Romaric and Preux, Philippe},
	Booktitle = {Pacific Asia Conference on Information Systems},
	Title = {{Scalable explore-exploit collaborative filtering}},
	Year = {2016}}

@inproceedings{heidari2016tight,
	Abstract = {We consider a variant of the well-studied multi-armed bandit problem in which the reward from each action evolves monotonically in the number of times the decision maker chooses to take that action. We are motivated by settings in which we must give a series of homogeneous tasks to a finite set of arms (workers) whose performance may improve (due to learning) or decay (due to loss of interest) with repeated trials. We assume that the arm-dependent rates at which the rewards change are unknown to the decision maker, and propose algorithms with provably optimal policy regret bounds, a much stronger notion than the often-studied external regret. For the case where the rewards are increasing and concave, we give an algorithm whose policy regret is sublinear and has a (provably necessary) dependence on the time re-quired to distinguish the optimal arm from the rest. We illustrate the behavior and performance of this algorithm via simulations. For the decreasing case, we present a simple greedy approach and show that the policy regret of this algorithm is constant and upper bounded by the number of arms.},
	Author = {Heidari, Hoda and Kearns, Michael and Roth, Aaron},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{Tight policy regret bounds for improving and decaying bandits}},
	Url = {https://www.cis.upenn.edu/{\%}7B{~}{\%}7Daaroth/Papers/decayingbandits.pdf},
	Year = {2016}}

@phdthesis{maillard2011apprentissage,
	Author = {Maillard, Odalric-Ambrym},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	School = {Universit{\{}{\'{e}}{\}} des Sciences et des Technologies de Lille 1},
	Title = {{Apprentissage s{\'{e}}quentiel: Bandits, Statistique et Renforcement}},
	Year = {2011}}

@inproceedings{agrawal2013further,
	Abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that simultaneously proves both the optimal problem-dependent bound of {\$}(1+\backslashepsilon)\backslashsum{\_}i \backslashfrac{\{}\backslashln T{\}}{\{}\backslashDelta{\_}i{\}}+O(\backslashfrac{\{}N{\}}{\{}\backslashepsilon{\^{}}2{\}}){\$} and the first near-optimal problem-independent bound of {\$}O(\backslashsqrt{\{}NT\backslashln T{\}}){\$} on the expected regret of this algorithm. Our near-optimal problem-independent bound solves a COLT 2012 open problem of Chapelle and Li. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are conceptually simple, easily extend to distributions other than the Beta distribution, and also extend to the more general contextual bandits setting [Manuscript, Agrawal and Goyal, 2012].},
	Author = {Agrawal, Shipra and Goyal, Navin},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{Further optimal regret bounds for Thompson sampling}},
	Url = {http://proceedings.mlr.press/v31/agrawal13a.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v31/agrawal13a.pdf}}

@article{audibert2009exploration,
	Author = {Audibert, Jean-Yves and Munos, R{\'{e}}mi and Szepesv{\'{a}}ri, Csaba},
	Journal = {Theoretical Computer Science},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {1876--1902},
	Title = {{Exploration-exploitation trade-off using variance estimates in multi-armed bandits}},
	Volume = {410},
	Year = {2009}}

@book{cesa-bianchi2006prediction,
	Annote = {From Duplicate 1 ( Prediction, Learning, and Games - Cesa-Bianchi, Nicolo; Lugosi, Gabor )
},
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Lugosi, G{\'{a}}bor},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Publisher = {Cambridge University Press},
	Title = {{Prediction, learning, and games}},
	Url = {http://www.ii.uni.wroc.pl/{~}lukstafi/pmwiki/uploads/AGT/Prediction{\_}Learning{\_}and{\_}Games.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://www.ii.uni.wroc.pl/%7B~%7Dlukstafi/pmwiki/uploads/AGT/Prediction%7B%5C_%7DLearning%7B%5C_%7Dand%7B%5C_%7DGames.pdf}}

@incollection{chapelle2011empirical,
	Abstract = {Thompson sampling is one of oldest heuristic to address the exploration ex- ploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
	Author = {Chapelle, Olivier and Li, Lihong},
	Booktitle = {Neural Information Processing Systems},
	Title = {{An empirical evaluation of Thompson sampling}},
	Url = {https://arxiv.org/pdf/1605.08722.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1605.08722.pdf}}

@inproceedings{bubeck2010open,
	Author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi},
	Booktitle = {Conference on Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Open-loop optimistic planning}},
	Year = {2010}}

@inproceedings{qin2017ttei,
	Abstract = {The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.},
	Archiveprefix = {arXiv},
	Arxivid = {1705.10033},
	Author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1705.10033},
	Pages = {5381--5391},
	Title = {{Improving the expected improvement algorithm}},
	Url = {http://arxiv.org/abs/1705.10033},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.10033}}

@inproceedings{platt2007,
	Address = {Vancouver, British Columbia, Canada},
	Booktitle = {Proceedings of the 21st conference on advances in Neural Information Processing Systems},
	Editor = {Platt, John C and Koller, Daphne and Singer, Yoram and Roweis, Sam T},
	Month = {dec},
	Publisher = {MIT Press},
	Series = {NIPS '07},
	Title = {{No Title}},
	Year = {2007}}

@article{bubeck2012regret,
	Abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the Thirties, exploration-exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this survey, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
	Archiveprefix = {arXiv},
	Arxivid = {1204.5721},
	Author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicol{\`{o}}},
	Eprint = {1204.5721},
	Journal = {Foundations and Trends in Machine Learning},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {1--122},
	Title = {{Regret analysis of stochastic and nonstochastic multi-armed bandit problems}},
	Url = {http://arxiv.org/abs/1204.5721},
	Volume = {5},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1204.5721}}

@article{Auer1995,
	Author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E},
	Keywords = {EXP3,adversarial bandits,bandit problem,game theory,matrix game,multi-armed bandit problem,rate of convergence,slot machines,stochastic games,well-behaved stochastic process},
	Mendeley-Tags = {EXP3,adversarial bandits},
	Month = {oct},
	Pages = {322},
	Title = {{Gambling in a rigged casino: The adversarial multi-armed bandit problem}},
	Url = {http://dl.acm.org/citation.cfm?id=795662.796294},
	Year = {1995},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=795662.796294}}

@inproceedings{katariya2017bernoulli,
	Abstract = {The probability that a user will click a search result depends both on its relevance and its position on the results page. The position based model explains this behavior by ascribing to every item an attraction probability, and to every position an examination probability. To be clicked, a result must be both attractive and examined. The probabilities of an item-position pair being clicked thus form the entries of a rank-{\$}1{\$} matrix. We propose the learning problem of a Bernoulli rank-{\$}1{\$} bandit where at each step, the learning agent chooses a pair of row and column arms, and receives the product of their Bernoulli-distributed values as a reward. This is a special case of the stochastic rank-{\$}1{\$} bandit problem considered in recent work that proposed an elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret scales linearly with the number of rows and columns on "benign" instances. These are the instances where the minimum of the average row and column rewards {\$}\backslashmu{\$} is bounded away from zero. The issue with Rank1Elim is that it fails to be competitive with straightforward bandit strategies as {\$}\backslashmu \backslashrightarrow 0{\$}. In this paper we propose Rank1ElimKL which simply replaces the (crude) confidence intervals of Rank1Elim with confidence intervals based on Kullback-Leibler (KL) divergences, and with the help of a novel result concerning the scaling of KL divergences we prove that with this change, our algorithm will be competitive no matter the value of {\$}\backslashmu{\$}. Experiments with synthetic data confirm that on benign instances the performance of Rank1ElimKL is significantly better than that of even Rank1Elim, while experiments with models derived from real data confirm that the improvements are significant across the board, regardless of whether the data is benign or not.},
	Author = {Katariya, Sumeet and Kveton, Branislav and Szepesv{\'{a}}ri, Csaba and Vernade, Claire and Wen, Zheng},
	Booktitle = {International Joint Conference on Artificial Intelligence},
	File = {::},
	Title = {{Bernoulli rank-1 bandits for click feedback}},
	Year = {2017}}

@article{hager1989updating,
	Author = {Hager, W W},
	Journal = {SIAM review},
	Pages = {221--239},
	Publisher = {JSTOR},
	Title = {{Updating the inverse of a matrix}},
	Year = {1989}}

@article{thompson1933likelihood,
	Author = {Thompson, William R.},
	Journal = {Biometrika},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {285--294},
	Title = {{On the likelihood that one unknown probability exceeds another in view of the evidence of two samples}},
	Url = {https://www.jstor.org/stable/2332286},
	Volume = {25},
	Year = {1933},
	Bdsk-Url-1 = {https://www.jstor.org/stable/2332286}}

@article{auer2002using,
	Author = {Auer, Peter},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {397--422},
	Title = {{Using confidence bounds for exploitation-exploration trade-offs}},
	Url = {http://www.jmlr.org/papers/volume3/auer02a/auer02a.pdf},
	Volume = {3},
	Year = {2002},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume3/auer02a/auer02a.pdf}}

@article{bubeck2011pure,
	Abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of forecasters that perform an on-line exploration of the arms. These forecasters are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. One of the main results in the case of a finite number of arms is a general lower bound on the simple regret of a forecaster in terms of its cumulative regret: the smaller the latter, the larger the former. Keeping this result in mind, we then exhibit upper bounds on the simple regret of some forecasters. The paper ends with a study devoted to continuous-armed bandit problems; we show that the simple regret can be minimized with respect to a family of probability distributions if and only if the cumulative regret can be minimized for it. Based on this equivalence, we are able to prove that the separable metric spaces are exactly the metric spaces on which these regrets can be minimized with respect to the family of all probability distributions with continuous mean-payoff functions. ?? 2010 Elsevier B.V. All rights reserved.},
	Author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles},
	Journal = {Theoretical Computer Science},
	Keywords = {Continuous-armed bandits,Efficient exploration,Multi-armed bandits,Simple regret},
	Number = {19},
	Pages = {1832--1852},
	Title = {{Pure exploration in finitely-armed and continuous-armed bandits}},
	Url = {https://hal.archives-ouvertes.fr/hal-00257454v6/document},
	Volume = {412},
	Year = {2011},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-00257454v6/document}}

@inproceedings{menard2017minimax,
	Abstract = {We propose the kl-UCB ++ algorithm for regret minimization in stochastic bandit models with exponential families of distributions. We prove that it is simultaneously asymptotically optimal (in the sense of Lai and Robbins' lower bound) and minimax optimal. This is the first algorithm proved to enjoy these two properties at the same time. This work thus merges two different lines of research with simple and clear proofs.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1702.07211v2},
	Author = {M{\'{e}}nard, Pierre and Garivier, Aur{\'{e}}lien and Hanneke, Steve and Reyzin, Lev},
	Booktitle = {Algorithmic Learning Theory},
	Eprint = {arXiv:1702.07211v2},
	Keywords = {Stochastic multi-armed bandits,asymptotic optimality,mini-max optimality,regret analysis,upper confidence bound (UCB)},
	Title = {{A minimax and asymptotically optimal algorithm for stochastic bandits}},
	Year = {2017}}

@article{azuma1967weighted,
	Author = {Azuma, Kazuoki},
	Journal = {Tohoku Mathematical Journal},
	Keywords = {bound,math},
	Number = {3},
	Pages = {357--367},
	Title = {{Weighted sums of certain dependent random variables}},
	Url = {https://www.jstage.jst.go.jp/article/tmj1949/19/3/19{\_}3{\_}357/{\_}pdf},
	Volume = {19},
	Year = {1967},
	Bdsk-Url-1 = {https://www.jstage.jst.go.jp/article/tmj1949/19/3/19%7B%5C_%7D3%7B%5C_%7D357/%7B%5C_%7Dpdf}}

@inproceedings{abeille2017linear,
	Author = {Abeille, Marc and Lazaric, Alessandro},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	File = {::},
	Title = {{Linear Thompson sampling revisited}},
	Url = {http://proceedings.mlr.press/v54/abeille17a/abeille17a.pdf},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/abeille17a/abeille17a.pdf}}

@inproceedings{gavald`a2009,
	Address = {Porto, Portugal},
	Booktitle = {Proceedings of the 20th international conference on Algorithmic Learning Theory},
	Editor = {Gavald{\`{a}}, Ricard and Lugosi, G{\'{a}}bor and Zeugmann, Thomas and Zilles, Sandra},
	Isbn = {978-3-642-04413-7},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {oct},
	Publisher = {Springer},
	Series = {ALT '09, Lecture Notes in Computer Science},
	Title = {{No Title}},
	Volume = {5809},
	Year = {2009}}

@inproceedings{servedio2008,
	Address = {Helsinki, Finland},
	Booktitle = {Proceedings of the 21st annual Conference On Learning Theory},
	Editor = {Servedio, Rocco A and Zhang, Tong},
	Month = {jul},
	Publisher = {Omnipress},
	Series = {COLT '08},
	Title = {{No Title}},
	Volume = {80},
	Year = {2008}}

@inproceedings{lazarichybrid,
	Author = {Lazaric, Alessandro and Munos, R{\'{e}}mi},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Hybrid Stochastic-Adversarial Online Learning}}}

@article{bnaya2013social,
	Abstract = {In many cases the best way to find a profile or a set of profiles matching some criteria in a socialnetwork is via targeted crawling. An important challenge in targeted crawling is choosing the next profileto explore. Existing heuristics for targeted crawling are usually tailored for specific search criterionand could lead to short-sighted crawling decisions. In this paper we propose and evaluate a generic ap- proach for guiding targeted crawling which is based on recent developments in Artificial Intelligence. Ourapproach, based on the recently introduced variant of the Multi-Armed Bandit problem with volatile arms(VMAB), aims to provide a proper balance between exploration and exploitation during the crawling process. Unlike other heuristics which are hand tailored for specific type of search queries, our approach isgeneral-purpose. In addition, it provides provable performance guarantees. Experimental results indicate that our approach compares favorably with the best existing heuristics on two different domains.},
	Author = {Bnaya, Zahy and Puzis, Rami and Stern, Roni and Felner, Ariel},
	Journal = {Human Journal},
	Number = {2},
	Pages = {84--98},
	Title = {{Social network search as a volatile multi-armed bandit problem}},
	Volume = {2},
	Year = {2013}}

@inproceedings{shawe-taylor2004,
	Address = {Banff, Canada},
	Booktitle = {Proceedings of the 17th annual Conference On Learning Theory},
	Editor = {Shawe-Taylor, John and Singer, Yoram},
	Isbn = {3-540-22282-0},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jul},
	Publisher = {Springer},
	Series = {COLT '04, Lecture Notes in Computer Science},
	Title = {{No Title}},
	Volume = {3120},
	Year = {2004}}

@inproceedings{gu2014online,
	Author = {Gu, Quanquan and Han, Jiawei},
	Booktitle = {International Conference on Data Mining},
	Title = {{Online spectral learning on a graph with bandit feedback}},
	Url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023409},
	Year = {2014},
	Bdsk-Url-1 = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023409}}

@article{harisson1978,
	Author = {Harrison, D and Rubinfeld, D L},
	Journal = {J. Environ. Economics {\&} Management},
	Pages = {81--102},
	Title = {{Hedonic prices and the demand for clean air}},
	Volume = {5},
	Year = {1978}}

@inproceedings{garivier2011upper-confidence-bound,
	Abstract = {Many problems, such as cognitive radio, parameter control of a scanning tunnelling microscope or internet advertisement, can be modelled as non-stationary bandit problems where the distributions of rewards changes abruptly at unknown time instants. In this paper, we analyze two algorithms designed for solving this issue: discounted UCB (D-UCB) and sliding-window UCB (SW-UCB). We establish an upper-bound for the expected regret by upper-bounding the expectation of the number of times suboptimal arms are played. The proof relies on an interesting Hoeffding type inequality for self normalized deviations with a random number of summands. We establish a lower-bound for the regret in presence of abrupt changes in the arms reward distributions. We show that the discounted UCB and the sliding-window UCB both match the lower-bound up to a logarithmic factor. Numerical simulations show that D-UCB and SW-UCB perform significantly better than existing soft-max methods like EXP3.S.},
	Author = {Garivier, Aur{\'{e}}lien and Moulines, Eric},
	Booktitle = {Algorithmic Learning Theory},
	Isbn = {978-3-642-24412-4},
	Title = {{On upper-confidence-bound policies for switching bandit problems}},
	Url = {https://arxiv.org/pdf/0805.3415.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://arxiv.org/pdf/0805.3415.pdf}}

@article{abernethy2008efficient,
	Abstract = {We introduce an efficient algorithm for the problem of online linear optimization in the bandit setting which achieves the optimal O (T regret. The setting is a natural generalization of the non-stochastic multi-armed bandit problem, and the existence of an efficient optimal algorithm has been posed as an open problem in a number of recent papers. We show how the difficulties encountered by previous approaches are overcome by the use of a self-concordant potential function. Our approach presents a novel connection between online learning and interior point methods.},
	Author = {Abernethy, Jacob Duncan and Hazan, Elad and Rakhlin, Alexander},
	Doi = {10.1080/09544820500115717},
	Institution = {EECS Department, University of California, Berkeley},
	Issn = {09544828},
	Journal = {Online},
	Number = {3},
	Pages = {540--543},
	Publisher = {Citeseer},
	Title = {{An Efficient Algorithm for Bandit Linear Optimization}},
	Url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1080/09544820500115717{\&}magic=crossref},
	Volume = {1},
	Year = {2008},
	Bdsk-Url-1 = {http://www.informaworld.com/openurl?genre=article%7B%5C&%7Ddoi=10.1080/09544820500115717%7B%5C&%7Dmagic=crossref},
	Bdsk-Url-2 = {https://doi.org/10.1080/09544820500115717}}

@techreport{garivier2018klucb,
	Abstract = {In the context of K-armed stochastic bandits with distribution only assumed to be supported by [0, 1], we introduce a new algorithm, KL-UCB-switch, and prove that it enjoys simultaneously a distribution-free regret bound of optimal order $\backslash$sqrt{\{}KT{\}} and a distribution-dependent regret bound of optimal order as well, that is, matching the $\backslash$kappa $\backslash$ln T lower bound by Lai and Robbins (1985) and Burnetas and Katehakis (1996).},
	Archiveprefix = {arXiv},
	Arxivid = {1805.05071},
	Author = {Garivier, Aur{\'{e}}lien and Hadiji, H{\'{e}}di and Menard, Pierre and Stoltz, Gilles},
	Eprint = {1805.05071},
	Month = {may},
	Title = {{KL-UCB-switch: optimal regret bounds for stochastic bandits from both a distribution-dependent and a distribution-free viewpoints}},
	Url = {http://arxiv.org/abs/1805.05071},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1805.05071}}

@book{Audibert2007a,
	Address = {Berlin, Heidelberg},
	Author = {Audibert, Jean-Yves and Munos, R{\'{e}}mi and Szepesv{\'{a}}ri, Csaba},
	Editor = {Hutter, Marcus and Servedio, Rocco A. and Takimoto, Eiji},
	Issn = {0302-9743},
	Month = {oct},
	Pages = {150--165},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {{Algorithmic Learning Theory}},
	Url = {http://dl.acm.org/citation.cfm?id=1422422.1422442},
	Volume = {4754},
	Year = {2007},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1422422.1422442}}

@inproceedings{preux2014bandits,
	Abstract = {We consider function optimization as a sequential decision making problem under the budget constraint. Such constraint limits the number of objective function evaluations allowed during the optimization. We consider an algorithm inspired by a continuous version of a multi-armed bandit problem which attacks this optimization problem by solving the tradeoff between exploration (initial quasi-uniform search of the domain) and exploitation (local optimization around the potentially global maxima). We introduce the so-called Simultaneous Optimistic Optimization (SOO), a deterministic algorithm that works by domain partitioning. The benefit of such an approach are the guarantees on the returned solution and the numerical eficiency of the algorithm. We present this machine learning rooted approach to optimization, and provide the empirical assessment of SOO on the CEC'2014 competition on single objective real-parameter numerical optimization testsuite.},
	Author = {Preux, Philippe and Munos, R{\'{e}}mi and Valko, Michal},
	Booktitle = {Congress on Evolutionary Computation},
	Title = {{Bandits attack function optimization}},
	Url = {https://hal.inria.fr/hal-00978637/document},
	Year = {2014},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-00978637/document}}

@article{silver2016mastering,
	Abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	Author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	Journal = {Nature},
	Number = {7587},
	Pages = {484--489},
	Shorttitle = {Nature},
	Title = {{Mastering the game of Go with deep neural networks and tree search}},
	Volume = {529},
	Year = {2016}}

@article{lai1985asymptotically,
	Author = {Lai, Tze L. and Robbins, Herbert},
	Journal = {Advances in Applied Mathematics},
	Keywords = {bandit,bandits},
	Mendeley-Tags = {bandits},
	Number = {1},
	Pages = {4--22},
	Publisher = {Elsevier},
	Title = {{Asymptotically efficient adaptive allocation rules}},
	Url = {https://ac.els-cdn.com/0196885885900028/1-s2.0-0196885885900028-main.pdf?{\_}tid=6ded14a5-1fe6-4c09-a1e3-9738a40b46d4{\&}acdnat=1539373065{\_}3220aa4053ab6e1f5db385fd4ef37e61},
	Volume = {6},
	Year = {1985},
	Bdsk-Url-1 = {https://ac.els-cdn.com/0196885885900028/1-s2.0-0196885885900028-main.pdf?%7B%5C_%7Dtid=6ded14a5-1fe6-4c09-a1e3-9738a40b46d4%7B%5C&%7Dacdnat=1539373065%7B%5C_%7D3220aa4053ab6e1f5db385fd4ef37e61}}

@techreport{lehrer2003wide,
	Author = {Lehrer, Ehud and Rosenberg, Dinah},
	Institution = {EconWPA},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{A Wide Range No-Regret Theorem}},
	Type = {Game Theory and Information},
	Year = {2003}}

@inproceedings{carpentier2014extreme,
	Abstract = {In many areas of medicine, security, and life sciences, we want to allocate limited resources to different sources in order to detect extreme values. In this paper, we study an efficient way to allocate these resources sequentially under limited feedback. While sequential design of experiments is well studied in bandit theory, the most commonly optimized property is the regret with respect to the maximum mean reward. However, in other problems such as network intrusion detection, we are interested in detecting the most extreme value output by the sources. Therefore, in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail. We propose the ExtremeHunter algorithm, provide its analysis, and evaluate it empirically on synthetic and real-world experiments.},
	Author = {Carpentier, Alexandra and Valko, Michal},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Carpentier, Valko - 2014 - Extreme bandits.pdf:pdf},
	Title = {{Extreme bandits}},
	Year = {2014}}

@inproceedings{xia2015budgeted,
	Abstract = {Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution. To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is {\$}O(\backslashln B){\$}, where {\$}B{\$} denotes the budget. By introducing a Bernoulli trial, we further extend this algorithm to the setting that the rewards (costs) are drawn from general distributions, and prove that its regret bound remains almost the same. Our simulation results demonstrate the effectiveness of the proposed algorithm.},
	Archiveprefix = {arXiv},
	Arxivid = {1505.00146},
	Author = {Xia, Yingce and Li, Haifang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
	Booktitle = {International Joint Conference on Artificial Intelligence},
	Eprint = {1505.00146},
	Isbn = {9781577357384},
	Title = {{Thompson sampling for budgeted multi-armed bandits}},
	Url = {http://arxiv.org/pdf/1505.00146.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/pdf/1505.00146.pdf}}

@inproceedings{kanade2009sleeping,
	Author = {Kanade, Varun and McMahan, H Brendan and Bryan, Brent},
	Booktitle = {Proceedings of the 12th international conference on Artificial Intelligence and Statistics},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Number = {5},
	Pages = {272--279},
	Series = {AI{\&}Stats '09},
	Title = {{Sleeping Experts and Bandits with Stochastic Action Availability and Adversarial Rewards}},
	Year = {2009}}

@inproceedings{bartletthigh,
	Author = {Bartlett, Peter L and Dani, Varsha and Hayes, Thomas P and Kakade, Sham M and Rakhlin, Alexander and Tewari, Ambuj},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {335--342},
	Title = {{High-probability Regret Bounds for Bandit Online Linear Optimization}}}

@inproceedings{dani2008stochastic,
	Author = {Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
	Booktitle = {Conference on Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Stochastic linear optimization under bandit feedback}},
	Url = {https://repository.upenn.edu/cgi/viewcontent.cgi?article=1501{\&}context=statistics{\_}papers},
	Year = {2008},
	Bdsk-Url-1 = {https://repository.upenn.edu/cgi/viewcontent.cgi?article=1501%7B%5C&%7Dcontext=statistics%7B%5C_%7Dpapers}}

@article{kawaguchi2016global,
	Abstract = {This paper considers global optimization with a black-box unknown objective function that can be non-convex and non-differentiable. Such a difficult optimization problem arises in many real-world applications, such as parameter tuning in machine learning, engineering design problem, and planning with a complex physics simulator. This paper proposes a new global optimization algorithm, called Locally Oriented Global Optimization (LOGO), to aim for both fast convergence in practice and finite-time error bound in theory. The advantage and usage of the new algorithm are illustrated via theoretical analysis and an experiment conducted with 11 benchmark test functions. Further, we modify the LOGO algorithm to specifically solve a planning problem via policy search with continuous state/action space and long time horizon while maintaining its finite-time error bound. We apply the proposed planning method to accident management of a nuclear power plant. The result of the application study demonstrates the practical utility of our method.},
	Archiveprefix = {arXiv},
	Arxivid = {1607.04817},
	Author = {Kawaguchi, Kenji and Maruyama, Yu and Zheng, Xiaoyu},
	Doi = {10.1613/jair.4742},
	Eprint = {1607.04817},
	Issn = {10769757},
	Journal = {Journal of Artificial Intelligence Research},
	Pages = {153--195},
	Title = {{Global continuous optimization with error bound and fast convergence}},
	Volume = {56},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1613/jair.4742}}

@inproceedings{narayananrandom,
	Author = {Narayanan, Hariharan and Rakhlin, Alexander},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {1777--1785},
	Title = {{Random Walk Approach to Regret Minimization}}}

@inproceedings{bartlett2009regal,
	Address = {Arlington, Virginia, United States},
	Author = {Bartlett, Peter L and Tewari, Ambuj},
	Booktitle = {Proceedings of the 25th conference on Uncertainty in Artificial Intelligence},
	Isbn = {978-0-9749039-5-8},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {35--42},
	Publisher = {AUAI Press},
	Series = {UAI '09},
	Title = {{REGAL: a regularization based algorithm for reinforcement learning in weakly communicating MDPs}},
	Year = {2009}}

@article{Agrawal2012,
	Abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of {\$}\backslashtilde{\{}O{\}}(d{\^{}}{\{}3/2{\}}\backslashsqrt{\{}T{\}}){\$} (or {\$}\backslashtilde{\{}O{\}}(d\backslashsqrt{\{}T \backslashlog(N){\}}){\$}), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of {\$}\backslashsqrt{\{}d{\}}{\$} (or {\$}\backslashsqrt{\{}\backslashlog(N){\}}{\$}) of the information-theoretic lower bound for this problem.},
	Author = {Agrawal, Shipra and Goyal, Navin},
	Month = {sep},
	Title = {{Thompson Sampling for Contextual Bandits with Linear Payoffs}},
	Url = {http://arxiv.org/abs/1209.3352},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1209.3352}}

@inproceedings{bonald2013two-target,
	Author = {Bonald, Thomas and Prouti{\`{e}}re, Alexandre},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Bonald, Proutiere - 2013 - Two-Target Algorithms for Infinite-Armed Bandits with Bernoulli Rewards.pdf:pdf},
	Title = {{Two-target algorithms for infinite-armed bandits with Bernoulli rewards}},
	Year = {2013}}

@inproceedings{bshouty2007,
	Address = {San Diego, CA, USA},
	Booktitle = {Proceedings of the 20th annual Conference On Learning Theory},
	Editor = {Bshouty, Nader H and Gentile, Claudio},
	Isbn = {978-3-540-72925-9},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jun},
	Publisher = {Springer},
	Series = {COLT '07, Lecture Notes in Computer Science},
	Title = {{No Title}},
	Volume = {4539},
	Year = {2007}}

@inproceedings{brodley2004,
	Address = {Banff, Alberta, Canada},
	Booktitle = {Proceedings of the 21st International Conference on Machine Learning},
	Editor = {Brodley, Carla E},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jul},
	Publisher = {ACM},
	Series = {ICML '04, ACM International Conference Proceeding Series},
	Title = {{No Title}},
	Volume = {69},
	Year = {2004}}

@inproceedings{ghosh2015ising,
	Author = {Ghosh, Shaona and Pr{\"{u}}gel-Bennett, Adam},
	Booktitle = {European Conference on Machine Learning},
	Title = {{Ising bandits with side Information}},
	Year = {2015}}

@article{russo2017tutorial,
	Abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
	Archiveprefix = {arXiv},
	Arxivid = {1707.02038},
	Author = {Russo, Daniel and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
	Eprint = {1707.02038},
	File = {::},
	Journal = {Foundations and Trends in Machine Learning},
	Month = {jul},
	Title = {{A tutorial on Thompson sampling}},
	Url = {http://arxiv.org/abs/1707.02038},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1707.02038}}

@inproceedings{lagree2017algorithms,
	Abstract = {Influence maximization is the problem of finding influential users, or nodes, in a graph so as to maximize the spread of information. It has many applications in advertising and marketing on social networks. In this paper, we study a highly generic version of influence maximization, one of optimizing influence campaigns by sequentially selecting "spread seeds" from a set of influencers, a small subset of the node population, under the hypothesis that, in a given campaign, previously activated nodes remain "persistently" active throughout and thus do not yield further rewards. This problem is in particular relevant for an important form of online marketing, known as influencer marketing, in which the marketers target a sub-population of influential people, instead of the entire base of potential buyers. Importantly, we make no assumptions on the underlying diffusion model and we work in a setting where neither a diffusion network nor historical activation data are available. We call this problem online influencer marketing with persistence (in short, OIMP). We first discuss motivating scenarios and present our general approach. We introduce an estimator on the influencers' remaining potential -- the expected number of nodes that can still be reached from a given influencer -- and justify its strength to rapidly estimate the desired value, relying on real data gathered from Twitter. We then describe a novel algorithm, GT-UCB, relying on upper confidence bounds on the remaining potential. We show that our approach leads to high-quality spreads on both simulated and real datasets, even though it makes almost no assumptions on the diffusion medium. Importantly, it is orders of magnitude faster than state-of-the-art influence maximization methods, making it possible to deal with large-scale online scenarios.},
	Author = {Lagr{\'{e}}e, Paul and Capp{\'{e}}, Olivier and Cautis, Bogdan and Maniu, Silviu},
	Booktitle = {International Conference on Data Mining},
	File = {::},
	Title = {{Algorithms for Online Influencer Marketing}},
	Year = {2017}}

@inproceedings{hren2008optimistic,
	Abstract = {If one possesses a model of a controlled deterministic system, then from any state, one may consider the set of all possible reachable states starting from that state and using any sequence of actions. This forms a tree whose size is exponential in the planning time horizon. Here we ask the question: given finite computational resources (e.g. CPU time), which may not be known ahead of time, what is the best way to explore this tree, such that once all resources have been used, the algorithm would be able to propose an action (or a sequence of actions) whose performance is as close as possible to optimality? The performance with respect to optimality is assessed in terms of the regret (with respect to the sum of discounted future rewards) resulting from choosing the action returned by the algorithm instead of an optimal action. In this paper we investigate an optimistic exploration of the tree, where the most promising states are explored first, and compare this approach to a naive uniform exploration. Bounds on the regret are derived both for uniform and optimistic exploration strategies. Numerical simulations illustrate the benefit of optimistic planning.},
	Author = {Hren, Jean-Francois and Munos, R{\'{e}}mi},
	Booktitle = {European Workshop on Reinforcement Learning},
	Title = {{Optimistic Planning of Deterministic Systems}},
	Year = {2008}}

@inproceedings{cohen2006,
	Address = {Pittsburgh, Pennsylvania, USA},
	Booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	Editor = {Cohen, William W and Moore, Andrew},
	Isbn = {1-59593-383-2},
	Month = {jun},
	Publisher = {ACM},
	Series = {ICML '06, ACM International Conference Proceeding Series},
	Title = {{No Title}},
	Volume = {148},
	Year = {2006}}

@article{gai2012combinatorial,
	Abstract = {In the classic multi-armed bandits problem, the goal is to have a policy for dynamically operating arms that each yield stochastic rewards with unknown means. The key metric of interest is regret, defined as the gap between the expected total reward accumulated by an omniscient player that knows the reward means for each arm, and the expected total reward accumulated by the given policy. The policies presented in prior work have storage, computation and regret all growing linearly with the number of arms, which is not scalable when the number of arms is large. We consider in this work a broad class of multi-armed bandits with dependent arms that yield rewards as a linear combination of a set of unknown parameters. For this general framework, we present efficient policies that are shown to achieve regret that grows logarithmically with time, and polynomially in the number of unknown parameters (even though the number of dependent arms may grow exponentially). Furthermore, these policies only require storage that grows linearly in the number of unknown parameters. We show that this generalization is broadly applicable and useful for many interesting tasks in networks that can be formulated as tractable combinatorial optimization problems with linear objective functions, such as maximum weight matching, shortest path, and minimum spanning tree computations.},
	Author = {Gai, Yi and Krishnamachari, Bhaskar and Jain, Rahul},
	Journal = {Transactions on Networking},
	Keywords = {Combinatorial network optimization,multi-armed bandits (MABs),online learning},
	Number = {5},
	Pages = {1466--1478},
	Title = {{Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations}},
	Url = {https://ieeexplore.ieee.org/document/6166915/},
	Volume = {20},
	Year = {2012},
	Bdsk-Url-1 = {https://ieeexplore.ieee.org/document/6166915/}}

@inproceedings{auer1995gambling,
	Author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E},
	Booktitle = {Proceedings of the 36th Annual Symposium on Foundations of Computer Science},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {322--331},
	Title = {{Gambling in a Rigged Casino: The Adversarial Multi-Armed Bandit problem}},
	Year = {1995}}

@article{auer2002finite,
	Address = {Hingham, MA, USA},
	Author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Fischer, Paul},
	Journal = {Machine Learning},
	Keywords = {adaptive allocation rules,bandit problems,bandits,finite horizon regret},
	Mendeley-Tags = {bandits},
	Number = {2-3},
	Pages = {235--256},
	Publisher = {Kluwer Academic Publishers},
	Title = {{Finite-time analysis of the multiarmed bandit problem}},
	Url = {https://homes.di.unimi.it/{~}cesabian/Pubblicazioni/ml-02.pdf},
	Volume = {47},
	Year = {2002},
	Bdsk-Url-1 = {https://homes.di.unimi.it/%7B~%7Dcesabian/Pubblicazioni/ml-02.pdf}}

@inproceedings{munos2011optimistic,
	Abstract = {We consider a global optimization problem of a deterministic function f in a semi-metric space, given a finite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A first contribution is an algorithm, DOO, that requires the knowledge of . We report a finite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then define a second algorithm, SOO, which does not require the knowledge of the semi-metric under which f is smooth, and whose performance is almost as good as DOO optimally-fitted.},
	Author = {Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Optimistic optimization of deterministic functions without the knowledge of its smoothness}},
	Url = {https://papers.nips.cc/paper/4304-optimistic-optimization-of-a-deterministic-function-without-the-knowledge-of-its-smoothness.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/4304-optimistic-optimization-of-a-deterministic-function-without-the-knowledge-of-its-smoothness.pdf}}

@misc{Guillou2015a,
	Abstract = {Recommender Systems (RS) aim at suggesting to users one or several items in which they might have interest. Following the feedback they receive from the user, these systems have to adapt their model in order to improve future recommendations. The repetition of these steps defines the RS as a sequential process. This sequential aspect raises an exploration-exploitation dilemma, which is surprisingly rarely taken into account for RS without contextual information. In this paper we present an explore-exploit collaborative filtering RS, based on Matrix Factor-ization and Bandits algorithms. Using experiments on artificial and real datasets, we show the importance and practicability of using sequential approaches to perform recommendation. We also study the impact of the model update on both the quality and the computation time of the recommendation procedure.},
	Author = {Guillou, Fr{\'{e}}d{\'{e}}ric and Gaudel, Romaric and Preux, Philippe},
	Booktitle = {NIPS'15 Workshop: Machine Learning for eCommerce},
	Keywords = {Collaborative Filtering,Matrix Factorization,Multi-Armed Bandtis,Recommender Systems,sequential Recommender Systems},
	Language = {en},
	Month = {dec},
	Title = {{Collaborative Filtering as a Multi-Armed Bandit}},
	Url = {https://hal.inria.fr/hal-01256254},
	Year = {2015},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-01256254}}

@inproceedings{gentile2014online,
	Author = {Gentile, Claudio and Li, Shuai and Zappella, Giovanni},
	Booktitle = {International Conference on Machine Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Gentile, Li, Zappella - 2014 - Online Clustering of Bandits.pdf:pdf},
	Title = {{Online clustering of bandits}},
	Url = {http://proceedings.mlr.press/v32/gentile14.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v32/gentile14.pdf}}

@inproceedings{wu2015online,
	Author = {Wu, Yifan and Gy{\"{o}}rgy, Andr{\'{a}}s and Szepesv{\'{a}}ri, Csaba},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Online learning with Gaussian payoffs and side observations}},
	Year = {2015}}

@inproceedings{grunewalder2010regret,
	Author = {Gr{\"{u}}new{\"{a}}lder, Steffen and Audibert, Jean-Yves and Opper, Manfred and Shawe-Taylor, John},
	Booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Regret Bounds for Gaussian Process Bandit Problems}},
	Year = {2010}}

@article{kaufmann2012thompson,
	Abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.},
	Archiveprefix = {arXiv},
	Arxivid = {1205.4217},
	Author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'{e}}mi},
	Eprint = {1205.4217},
	Journal = {Algorithmic Learning Theory},
	Title = {{Thompson sampling: An asymptotically optimal finite-time analysis}},
	Url = {https://arxiv.org/pdf/1205.4217.pdf},
	Year = {2012},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1205.4217.pdf}}

@article{audibert2010best,
	Abstract = {We consider the problem of finding the best arm in a stochastic multi-armed bandit game. The regret of a forecaster is here defined by the gap between the mean reward of the optimal arm and the mean reward of the ultimately chosen arm. We propose a highly exploring UCB policy and a new algorithm based on successive rejects. We show that these algorithms are essentially optimal since their regret decreases exponentially at a rate which is, up to a logarithmic factor, the best possible. However, while the UCB policy needs the tuning of a parameter depending on the unobservable hardness of the task, the successive rejects policy benefits from being parameter-free, and also independent of the scaling of the rewards.},
	Annote = {From Duplicate 3 ( Best arm identification in multi-armed bandits - Audibert, Jean-Yves; Bubeck, S{\'{e}}bastien; Munos, R{\'{e}}mi )
},
	Author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi},
	Journal = {Conference on Learning Theory},
	Keywords = {learning,statistics {\&} optimisation,theory {\&} algorithms},
	Title = {{Best arm identification in multi-armed bandits}},
	Year = {2010}}

@inproceedings{pandey2007bandits,
	Author = {Pandey, S and Agarwal, D and Chakrabarti, D and Josifovski, V},
	Booktitle = {Proceedings of the Seventh SIAM International Conference on Data Mining},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Bandits for Taxonomies: A Model-based Approach}},
	Year = {2007}}

@inproceedings{gabillon2013adaptive,
	Author = {Gabillon, Victor and Kveton, Branislav and Wen, Zheng and Eriksson, Brian and Muthukrishnan, S.},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Adaptive submodular maximization in bandit setting}},
	Year = {2013}}

@book{Mary2015a,
	Abstract = {This paper addresses the on-line recommendation problem facing new users and new items; we assume that no information is available neither about users, nor about the items. The only source of information is a set of ratings given by users to some items. By on-line, we mean that the set of users, and the set of items, and the set of ratings is evolving along time and that at any moment, the recommendation system has to select items to recommend based on the currently available information, that is basically the sequence of past events. We also mean that each user comes with her preferences which may evolve along short and longer scales of time; so we have to continuously update their preferences. When the set of ratings is the only available source of information , the traditional approach is matrix factorization. In a decision making under uncertainty setting, actions should be selected to balance exploration with exploitation; this is best modeled as a bandit problem. Matrix factors provide a latent representation of users and items. These representations may then be used as contextual information by the bandit algorithm to select items. This last point is exactly the originality of this paper: the combination of matrix factorization and bandit algorithms to solve the on-line recommendation problem. Our work is driven by considering the recommendation problem as a feedback controlled loop. This leads to interactions between the representation learning, and the recommendation policy.},
	Address = {Cham},
	Author = {Mary, J{\'{e}}r{\'{e}}mie and Gaudel, Romaric and Preux, Philippe},
	Booktitle = {First International Workshop on Machine Learning, Optimization, and Big Data (MOD'15)},
	Editor = {Pardalos, Panos and Pavone, Mario and Farinella, Giovanni Maria and Cutello, Vincenzo},
	Keywords = {Collaborative Filtering,Matrix Factorization,Multi-Armed Bandtis,Recommender Systems,contextual Bandits,sequential Recommender Systems},
	Language = {en},
	Month = {jul},
	Pages = {325--336},
	Publisher = {Springer International Publishing},
	Series = {Lecture Notes in Computer Science},
	Title = {{Machine Learning, Optimization, and Big Data}},
	Url = {https://hal.inria.fr/hal-01256033},
	Volume = {9432},
	Year = {2015},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-01256033}}

@inproceedings{chakrabartimortal,
	Author = {Chakrabarti, Deepayan and Kumar, Ravi and Radlinski, Filip and Upfal, Eli},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {273--280},
	Title = {{Mortal Multi-Armed Bandits.}}}

@article{guha2007approximation,
	Author = {Guha, Sudipto and Munagala, Kamesh and Shi, Peng},
	Journal = {CoRR},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Approximation Algorithms for Restless Bandit Problems}},
	Volume = {abs/0711.3},
	Year = {2007}}

@inproceedings{audibert2011minimax,
	Author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien and Lugosi, Gabor},
	Booktitle = {Proceedings of the 24th annual Conference On Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Series = {COLT '11},
	Title = {{Minimax Policies for Combinatorial Prediction Games}},
	Year = {2011}}

@inproceedings{yue2011linear,
	Author = {Yue, Yisong and Guestrin, Carlos},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Linear submodular bandits and their application to diversified retrieval}},
	Year = {2011}}

@inproceedings{pandey2007multi,
	Address = {New York, NY, USA},
	Author = {Pandey, S and Chakrabarti, D and Agarwal, D},
	Booktitle = {ICML '07: Proceedings of the 24th international conference on Machine learning},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {721--728},
	Publisher = {ACM},
	Title = {{Multi-Armed Bandit Problems with Dependent Arms}},
	Year = {2007}}

@book{neumaier2008interval,
	Author = {Neumaier, Arnold},
	Isbn = {9780521102148},
	Publisher = {Cambridge University Press},
	Series = {Encyclopedia of Mathematics and its Applications},
	Title = {{Interval Methods for Systems of Equations}},
	Url = {http://books.google.fr/books?id=ObInPwAACAAJ},
	Year = {2008},
	Bdsk-Url-1 = {http://books.google.fr/books?id=ObInPwAACAAJ}}

@article{Lei2015a,
	Abstract = {Social networks are commonly used for marketing purposes. For example, free samples of a product can be given to a few influential social network users (or "seed nodes"), with the hope that they will convince their friends to buy it. One way to formalize marketers' objective is through influence maximization (or IM), whose goal is to find the best seed nodes to activate under a fixed budget, so that the number of people who get influenced in the end is maximized. Recent solutions to IM rely on the influence probability that a user influences another one. However, this probability information may be unavailable or incomplete. In this paper, we study IM in the absence of complete information on influence probability. We call this problem Online Influence Maximization (OIM) since we learn influence probabilities at the same time we run influence campaigns. To solve OIM, we propose a multiple-trial approach, where (1) some seed nodes are selected based on existing influence information; (2) an influence campaign is started with these seed nodes; and (3) users' feedback is used to update influence information. We adopt the Explore-Exploit strategy, which can select seed nodes using either the current influence probability estimation (exploit), or the confidence bound on the estimation (explore). Any existing IM algorithm can be used in this framework. We also develop an incremental algorithm that can significantly reduce the overhead of handling users' feedback information. Our experiments show that our solution is more effective than traditional IM methods on the partial information.},
	Author = {Lei, Siyu and Maniu, Silviu and Mo, Luyi and Cheng, Reynold and Senellart, Pierre},
	Month = {jun},
	Pages = {13},
	Title = {{Online Influence Maximization (Extended Version)}},
	Url = {http://arxiv.org/abs/1506.01188},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1506.01188}}

@inproceedings{ma2015active,
	Author = {Ma, Yifei and Huang, Tzu-Kuo and Schneider, Jeff},
	Booktitle = {Uncertainty in Artificial Intelligence},
	Title = {{Active search and bandits on graphs using sigma-optimality}},
	Url = {https://pdfs.semanticscholar.org/f72b/71c747d2f487e8c0ade09f4d31e4ad2c0185.pdf},
	Year = {2015},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/f72b/71c747d2f487e8c0ade09f4d31e4ad2c0185.pdf}}

@inproceedings{cesa-bianchi2012combinatorial,
	Abstract = {We study sequential prediction problems in which, at each time instance, the forecaster chooses a vector from a given finite set S⊂ Rd. At the same time, the opponent chooses a loss vector in Rd and the forecaster suffers a loss that is the inner product of the two vectors. The goal of the forecaster is to achieve that, in the long run, the accumulated loss is not much larger than that of the best possible element in S. We consider the bandit setting in which the forecaster only has access to the losses of the chosen vectors (i.e., the entire loss vectors are not observed). We introduce a variant of a strategy by Dani, Hayes and Kakade achieving a regret bound that, for a variety of concrete choices of S, is of order √ndln|S| where n is the time horizon. This is not improvable in general and is better than previously known bounds. The examples we consider are all such that S⊂{\{} 0,1{\}}d, and we show how the combinatorial structure of these classes can be exploited to improve the regret bounds. We also point out computationally efficient implementations for various interesting choices of S. {\textcopyright} 2012 Elsevier Inc.},
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Lugosi, G{\'{a}}bor},
	Booktitle = {Journal of Computer and System Sciences},
	Keywords = {Adversarial bandit problems,Online linear optimization,Online prediction},
	Number = {5},
	Pages = {1404--1422},
	Title = {{Combinatorial bandits}},
	Url = {http://cesa-bianchi.di.unimi.it/Pubblicazioni/comband.pdf},
	Volume = {78},
	Year = {2012},
	Bdsk-Url-1 = {http://cesa-bianchi.di.unimi.it/Pubblicazioni/comband.pdf}}

@inproceedings{yu2011unimodal,
	Author = {Yu, Jia Yuan and Mannor, Shie},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Unimodal bandits}},
	Url = {http://www.icml-2011.org/papers/50{\_}icmlpaper.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://www.icml-2011.org/papers/50%7B%5C_%7Dicmlpaper.pdf}}

@phdthesis{shalev-shwartz2007online,
	Author = {Shalev-Shwartz, Shai},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jul},
	School = {The Hebrew University of Jerusalem},
	Title = {{Online Learning: Theory, Algorithms, and Applications}},
	Year = {2007}}

@inproceedings{levine2017rotting,
	Abstract = {The Multi-Armed Bandits (MAB) framework highlights the trade-off between acquiring new knowledge (Exploration) and leveraging available knowledge (Ex-ploitation). In the classical MAB problem, a decision maker must choose an arm at each time step, upon which she receives a reward. The decision maker's objective is to maximize her cumulative expected reward over the time horizon. The MAB problem has been studied extensively, specifically under the assumption of the arms' rewards distributions being stationary, or quasi-stationary, over time. We consider a variant of the MAB framework, which we termed Rotting Bandits, where each arm's expected reward decays as a function of the number of times it has been pulled. We are motivated by many real-world scenarios such as online advertis-ing, content recommendation, crowdsourcing, and more. We present algorithms, accompanied by simulations, and derive theoretical guarantees.},
	Author = {Levine, Nir and Crammer, Koby and Mannor, Shie},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Rotting bandits}},
	Url = {http://papers.nips.cc/paper/6900-rotting-bandits.pdf},
	Year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6900-rotting-bandits.pdf}}

@inproceedings{slivkins2011multi-armed,
	Author = {Slivkins, Aleksandrs},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Multi-armed bandits on implicit metric spaces}},
	Url = {https://papers.nips.cc/paper/4332-multi-armed-bandits-on-implicit-metric-spaces.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/4332-multi-armed-bandits-on-implicit-metric-spaces.pdf}}

@phdthesis{stoltz2011contributions,
	Author = {Stoltz, Gilles},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	School = {Universit{\{}{\'{e}}{\}} Paris-Sud},
	Title = {{Contributions to the sequential prediction of arbitrary sequences: applications to the theory of repeated games and empirical studies of the performance of the aggregation of experts}},
	Type = {Habilitation {\{}{\`{a}}{\}} Diriger des Recherches},
	Year = {2011}}

@inproceedings{cohen2008,
	Address = {Helsinki, Finland},
	Booktitle = {Proceedings of the 25th International Conference on Machine Learning},
	Editor = {Cohen, William W and McCallum, Andrew and Roweis, Sam T},
	Isbn = {978-1-60558-205-4},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jun},
	Publisher = {ACM},
	Series = {ICML '08, ACM International Conference Proceeding Series},
	Title = {{No Title}},
	Volume = {307},
	Year = {2008}}

@article{poland2008nonstochastic,
	Author = {Poland, Jan},
	Journal = {Theoretical Computuer Science},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jul},
	Number = {1-3},
	Pages = {77--93},
	Title = {{Nonstochastic bandits: Countable decision set, unbounded costs and reactive environments.}},
	Volume = {397},
	Year = {2008}}

@inproceedings{munos1999variable,
	Author = {Munos, R{\'{e}}mi and Moore, Andrew},
	Booktitle = {Proceedings of the 16th International Joint Conference on Artificial Intelligence},
	Pages = {1348--1355},
	Title = {{Variable Resolution Discretization for High-Accuracy Solutions of Optimal Control Problems}},
	Year = {1999}}

@inproceedings{cohen2016online,
	Author = {Cohen, Alon and Hazan, Tamir and Koren, Tomer},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Online learning with feedback graphs without the graphs}},
	Year = {2016}}

@inproceedings{abernethyoptimal,
	Author = {Abernethy, Jacob D and Bartlett, Peter L and Rakhlin, Alexander and Tewari, Ambuj},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Optimal strategies and minimax lower bounds for online convex games}}}

@inproceedings{katariya2016stochastic,
	Abstract = {We propose stochastic rank-{\$}1{\$} bandits, a class of online learning problems where at each step a learning agent chooses a pair of row and column arms, and receives the product of their values as a reward. The main challenge of the problem is that the individual values of the row and column are unobserved. We assume that these values are stochastic and drawn independently. We propose a computationally-efficient algorithm for solving our problem, which we call Rank1Elim. We derive a {\$}O((K + L) (1 / \backslashDelta) \backslashlog n){\$} upper bound on its {\$}n{\$}-step regret, where {\$}K{\$} is the number of rows, {\$}L{\$} is the number of columns, and {\$}\backslashDelta{\$} is the minimum of the row and column gaps; under the assumption that the mean row and column rewards are bounded away from zero. To the best of our knowledge, we present the first bandit algorithm that finds the maximum entry of a rank-{\$}1{\$} matrix whose regret is linear in {\$}K + L{\$}, {\$}1 / \backslashDelta{\$}, and {\$}\backslashlog n{\$}. We also derive a nearly matching lower bound. Finally, we evaluate Rank1Elim empirically on multiple problems. We observe that it leverages the structure of our problems and can learn near-optimal solutions even if our modeling assumptions are mildly violated.},
	Author = {Katariya, Sumeet and Kveton, Branislav and Szepesv{\'{a}}ri, Csaba and Vernade, Claire and Wen, Zheng},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Katariya et al. - 2016 - Stochastic Rank-1 Bandits.pdf:pdf},
	Title = {{Stochastic rank-1 bandits}},
	Year = {2017}}

@inproceedings{gabillon2014largescale,
	Author = {Gabillon, Victor and Kveton, Branislav and Wen, Zheng and Eriksson, Brian and Muthukrishnan, S.},
	Booktitle = {AAAI Conference on Artificial Intelligence},
	Title = {{Large-scale optimistic adaptive submodularity}},
	Year = {2014}}

@inproceedings{garivier2011kl,
	Author = {Garivier, Aur{\'{e}}lien and Capp{\'{e}}, Olivier},
	Booktitle = {Conference on Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{The KL-UCB algorithm for bounded stochastic bandits and beyond}},
	Url = {https://arxiv.org/pdf/1102.2490.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1102.2490.pdf}}

@inproceedings{kakadeefficient,
	Author = {Kakade, Sham M and Shalev-Shwartz, Shai and Tewari, Ambuj},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {440--447},
	Title = {{Efficient bandit algorithms for online multiclass prediction}}}

@inproceedings{auer2016algorithm,
	Abstract = {We present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits. Against adversarial bandits the pseudo-regret is {\$}O(K\backslashsqrt{\{}n \backslashlog n{\}}){\$} and against stochastic bandits the pseudo-regret is {\$}O(\backslashsum{\_}i (\backslashlog n)/\backslashDelta{\_}i){\$}. We also show that no algorithm with {\$}O(\backslashlog n){\$} pseudo-regret against stochastic bandits can achieve {\$}\backslashtilde{\{}O{\}}(\backslashsqrt{\{}n{\}}){\$} expected regret against adaptive adversarial bandits. This complements previous results of Bubeck and Slivkins (2012) that show {\$}\backslashtilde{\{}O{\}}(\backslashsqrt{\{}n{\}}){\$} expected adversarial regret with {\$}O((\backslashlog n){\^{}}2){\$} stochastic pseudo-regret.},
	Archiveprefix = {arXiv},
	Arxivid = {1605.08722},
	Author = {Auer, Peter and Chiang, Chao-Kai},
	Booktitle = {Conference on Learning Theory},
	Eprint = {1605.08722},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Auer, Chiang - 2016 - An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits.pdf:pdf},
	Month = {may},
	Title = {{An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits}},
	Url = {http://arxiv.org/abs/1605.08722},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1605.08722}}

@inproceedings{kaufmann2012bayesian,
	Abstract = {Stochastic bandit problems have been ana-lyzed from two different perspectives: a fre-quentist view, where the parameter is a deter-ministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution. We show in this paper that methods derived from this second per-spective prove optimal when evaluated using the frequentist cumulated regret as a mea-sure of performance. We give a general for-mulation for a class of Bayesian index policies that rely on quantiles of the posterior distri-bution. For binary bandits, we prove that the corresponding algorithm, termed Bayes-UCB, satisfies finite-time regret bounds that imply its asymptotic optimality. More gen-erally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit prob-lems (parametric multi-armed bandits, Gaus-sian bandits with unknown mean and vari-ance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In par-ticular, we show how to handle linear ban-dits with sparsity constraints by resorting to Gibbs sampling.},
	Author = {Kaufmann, Emilie and Capp{\'{e}}, Olivier and Garivier, Aur{\'{e}}lien},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{On Bayesian upper confidence bounds for bandit problems}},
	Url = {http://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf}}

@inproceedings{auer2005,
	Address = {Bertinoro, Italy},
	Booktitle = {Proceedings of the 18th annual Conference On Learning Theory},
	Editor = {Auer, Peter and Meir, Ron},
	Isbn = {3-540-26556-2},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jun},
	Publisher = {Springer},
	Series = {COLT '05, Lecture Notes in Computer Science},
	Title = {{No Title}},
	Volume = {3559},
	Year = {2005}}

@inproceedings{hazan2011beyond,
	Author = {Hazan, Elad and Kale, Satyen},
	Booktitle = {Conference on Learning Theory},
	Title = {{Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization.}},
	Year = {2011}}

@inproceedings{agrawal2013thomson,
	Author = {Agrawal, Shipra and Goyal, Navin},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Thompson sampling for contextual bandits with linear payoffs}},
	Url = {http://proceedings.mlr.press/v28/agrawal13.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v28/agrawal13.pdf}}

@inproceedings{raedt2005,
	Address = {Bonn, Germany},
	Booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
	Editor = {Raedt, Luc De and Wrobel, Stefan},
	Isbn = {1-59593-180-5},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {aug},
	Publisher = {ACM},
	Series = {ICML '05, ACM International Conference Proceeding Series},
	Title = {{No Title}},
	Volume = {119},
	Year = {2005}}

@article{auer2002nonstochastic,
	Author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E.},
	Journal = {Journal on Computing},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Number = {1},
	Pages = {48--77},
	Title = {{The nonstochastic multi-armed bandit problem}},
	Url = {https://epubs.siam.org/doi/pdf/10.1137/S0097539701398375},
	Volume = {32},
	Year = {2002},
	Bdsk-Url-1 = {https://epubs.siam.org/doi/pdf/10.1137/S0097539701398375}}

@article{rakhlin2010online,
	Author = {Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
	Journal = {ArXiv e-prints},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {nov},
	Title = {{Online Learning: Beyond Regret}},
	Year = {2010}}

@inproceedings{kocak2014efficient,
	Abstract = {We consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback. In the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions. The revealed losses depend on the learner's action and a directed observation system chosen by the environment. For this setting, we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines, we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback. As the predictions of our first algorithm cannot be always computed efficiently in this setting, we propose another algorithm with similar properties and with the benefit of always being computationally efficient, at the price of a slightly more complicated tuning mechanism. Both algorithms rely on a novel exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.},
	Author = {Koc{\'{a}}k, Tom{\'{a}}{\v{s}} and Neu, Gergely and Valko, Michal and Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Efficient learning by implicit exploration in bandit problems with side observations}},
	Url = {https://papers.nips.cc/paper/5462-efficient-learning-by-implicit-exploration-in-bandit-problems-with-side-observations.pdf},
	Year = {2014},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/5462-efficient-learning-by-implicit-exploration-in-bandit-problems-with-side-observations.pdf}}

@inproceedings{streeter2006asymptotically,
	Abstract = {We present an asymptotically optimal algorithm for the max variant of the k-armed bandit problem. Given a set of k slot machines, each yielding payoff from a fixed (but unknown) distribution, we wish to allocate trials to the machines so as to maximize the expected maximum payoff � received over a series of n trials. Subject to certain distributional assumptions, we show that O ln ( 1 ln(n)2) $\delta$ ɛ2 � trials are sufficient to identify, with probability at least 1 − $\delta$, a machine whose expected maximum payoff is within ɛ of optimal. This result leads to a strategy for solving the problem that is asymptotically optimal in the following sense: the gap between the expected maximum payoff obtained by using our strategy for n trials and that obtained by pulling the single best arm for all n trials approaches zero as n → ∞.},
	Author = {Streeter, Matthew J. and Smith, Stephen F.},
	Booktitle = {AAAI Conference on Artificial Intelligence Intelligence},
	Doi = {10.1.1.91.4735},
	Keywords = {constraint satisfaction,satisfiability},
	Pages = {135--142},
	Title = {{An Asymptotically Optimal Algorithm for the Max k-Armed Bandit Problem}},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1.1.91.4735}}

@inproceedings{antos2008active,
	Abstract = {In this paper we consider the problem of actively learning the mean values of distributions associated with a finite number of options (arms). The algorithms can select which option to generate the next sample from in order to produce estimates with equally good precision for all the distributions. When an algorithm uses sample means to estimate the unknown values then the optimal solution, assuming full knowledge of the distributions, is to sample each option proportional to its variance. In this paper we propose an incremental algorithm that asymptotically achieves the same loss as an optimal rule. We prove that the excess loss suffered by this algorithm, apart from logarithmic factors, scales as n − 3/2 , which we conjecture to be the optimal rate. The performance of the algorithm is illustrated in a simple problem.},
	Author = {Antos, Andr{\'{a}}s and Grover, Varun and Szepesv{\'{a}}ri, Csaba and Freund, Yoav and Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and Tur{\'{a}}n, Gy{\"{o}}rgy and Zeugmann, Thomas},
	Booktitle = {Algorithmic Learning Theory},
	Title = {{Active Learning in Multi-armed Bandits}},
	Year = {2008}}

@inproceedings{neu2014online,
	Abstract = {Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.},
	Author = {Neu, Gergely and Valko, Michal},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Neu, Valko - 2014 - Online combinatorial optimization with stochastic decision sets and adversarial losses.pdf:pdf},
	Title = {{Online combinatorial optimization with stochastic decision sets and adversarial losses}},
	Year = {2014}}

@inproceedings{hazanextracting,
	Author = {Hazan, Elad and Kale, Satyen},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {57--68},
	Title = {{Extracting Certainty from Uncertainty: Regret Bounded by Variation in Costs.}}}

@inproceedings{li2015online,
	Abstract = {Classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, we investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Our algorithm takes into account the collaborative effects that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. We provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. We also provide a regret analysis within a standard linear stochastic noise setting.},
	Author = {Li, Shuai and Karatzoglou, Alexandros and Gentile, Claudio},
	Booktitle = {Conference on Research and Development in Information Retrieval},
	Title = {{Collaborative filtering bandits}},
	Url = {https://arxiv.org/pdf/1502.03473.pdf},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1502.03473.pdf}}

@article{kleinberg2013,
	Author = {Kleinberg, Robert and Slivkins, Aleksandrs and Upfal, Eli},
	Journal = {Journal of ACM},
	Title = {{Bandits and experts in metric spaces}},
	Url = {https://arxiv.org/pdf/1312.1277.pdf},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1312.1277.pdf}}

@inproceedings{flaxman2005online,
	Author = {Flaxman, Abraham D and Kalai, Adam Tauman and {Brendan McMahan}, Hugh},
	Booktitle = {Proceedings of the 16th annual ACM-SIAM Symposium On Discrete Algorithms},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {385--394},
	Publisher = {SIAM},
	Series = {SODA '05},
	Title = {{Online convex optimization in the bandit setting: gradient descent without a gradient}},
	Year = {2005}}

@incollection{filippi2010parametric,
	Author = {Filippi, Sarah and Capp{\'{e}}, Olivier and Garivier, Aur{\'{e}}lien and Szepesv{\'{a}}ri, Csaba},
	Booktitle = {Advances in Neural Information Processing Systems 23},
	Editor = {Lafferty, J and Williams, C K I and Shawe-Taylor, J and Zemel, R S and Culotta, A},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {586--594},
	Title = {{Parametric Bandits: The Generalized Linear Case}},
	Year = {2010}}

@article{cortez2009,
	Author = {Cortez, P and Cerdeira, A and Almeida, F and Matos, T and Reis, J},
	Journal = {Decision Support Systems},
	Pages = {547--553},
	Publisher = {Elsevier},
	Title = {{Modeling wine preferences by data mining from physicochemical properties}},
	Volume = {47},
	Year = {2009}}

@inproceedings{cesa-bianchi2010online,
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Shalev-Shwartz, Shai and Shamir, Oha},
	Booktitle = {Conference on Learning Theory},
	Title = {{Online learning of noisy data with kernels}},
	Year = {2010}}

@inproceedings{korda2013thompson,
	Abstract = {Thompson Sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for 1-dimensional exponential family bandits. Our proof builds on previous work, but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (and thus Jeffreys prior) available in an exponential family. This allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families.},
	Archiveprefix = {arXiv},
	Arxivid = {1307.3400},
	Author = {Korda, Nathaniel and Kaufmann, Emilie and Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1307.3400},
	Title = {{Thompson Sampling for 1-Dimensional Exponential Family Bandits}},
	Year = {2013}}

@techreport{traca2015regulating,
	Abstract = {In retail, there are predictable yet dramatic time-dependent patterns in customer behavior, such as periodic changes in the number of visitors, or increases in visitors just before major holidays (e.g., Christmas). The current paradigm of multi-armed bandit analysis does not take these known patterns into account, which means that despite the firm theoretical foundation of these methods, they are fundamentally flawed when it comes to real applications. This work provides a remedy that takes the time-dependent patterns into account, and we show how this remedy is implemented in the UCB and {\{}$\backslash$epsilon{\}}-greedy methods. In the corrected methods, exploitation (greed) is regulated over time, so that more exploitation occurs during higher reward periods, and more exploration occurs in periods of low reward. In order to understand why regret is reduced with the corrected methods, we present a set of bounds that provide insight into why we would want to exploit during periods of high reward, and discuss the impact on regret. Our proposed methods have excellent performance in experiments, and were inspired by a high-scoring entry in the Exploration and Exploitation 3 contest using data from Yahoo! Front Page. That entry heavily used time-series methods to regulate greed over time, which was substantially more effective than other contextual bandit methods.},
	Archiveprefix = {arXiv},
	Arxivid = {1505.05629},
	Author = {Trac{\`{a}}, Stefano and Rudin, Cynthia},
	Eprint = {1505.05629},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Trac{\`{a}}, Rudin - 2015 - Regulating Greed Over Time.pdf:pdf},
	Title = {{Regulating Greed Over Time}},
	Url = {https://arxiv.org/abs/1505.05629},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1505.05629}}

@article{garivier2018explore,
	Abstract = {We revisit lower bounds on the regret in the case of multi-armed bandit problems. We obtain non-asymptotic, distribution-dependent bounds and provide straightforward proofs based only on well-known properties of Kullback-Leibler divergences. These bounds show in particular that in an initial phase the regret grows almost linearly, and that the well-known logarithmic growth of the regret only holds in a final phase. The proof techniques come to the essence of the information-theoretic arguments used and they are deprived of all unnecessary complications.},
	Archiveprefix = {arXiv},
	Arxivid = {1602.07182},
	Author = {Garivier, Aur{\'{e}}lien and M{\'{e}}nard, Pierre and Stoltz, Gilles},
	Eprint = {1602.07182},
	File = {::},
	Journal = {Mathematics of Operations Research},
	Title = {{Explore First, Exploit Next: The True Shape of Regret in Bandit Problems}},
	Year = {2018}}

@inproceedings{auerlogarithmic,
	Author = {Auer, Peter and Ortner, Ronald},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {49--56},
	Title = {{Logarithmic online regret bounds for undiscounted reinforcement learning}}}

@article{jaksch2010near,
	Address = {Cambridge, MA, USA},
	Author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	Issn = {1532-4435},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {aug},
	Pages = {1563--1600},
	Publisher = {MIT Press},
	Title = {{Near-optimal Regret Bounds for Reinforcement Learning}},
	Volume = {99},
	Year = {2010}}

@incollection{russo2013eluder,
	Author = {Russo, Daniel and {Van Roy}, Benjamin},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Eluder Dimension and the Sample Complexity of Optimistic Exploration}},
	Url = {http://papers.nips.cc/paper/4909-eluder-dimension-and-the-sample-complexity-of-optimistic-exploration.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/4909-eluder-dimension-and-the-sample-complexity-of-optimistic-exploration.pdf}}

@article{Chen2014a,
	Abstract = {We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which more based arms could be probabilistically triggered based on the outcomes of already triggered arms. The reward of the super arm depends on the outcomes of all played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an offline ($\backslash$alpha,$\backslash$beta)-approximation oracle that takes the means of the outcome distributions of arms and outputs a super arm that with probability {\{}$\backslash$beta{\}} generates an {\{}$\backslash$alpha{\}} fraction of the optimal expected reward. The objective of an online learning algorithm for CMAB is to minimize ($\backslash$alpha,$\backslash$beta)-approximation regret, which is the difference between the $\backslash$alpha{\{}$\backslash$beta{\}} fraction of the expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(log n) distribution-dependent regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound of UCB1 algorithm (up to a constant factor) for the classical MAB problem, and it significantly improves the regret bound in a earlier paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage and social influence maximization, both having nonlinear reward structures. In particular, application to social influence maximization requires our extension on probabilistically triggered arms.},
	Author = {Chen, Wei and Wang, Yajun and Yuan, Yang},
	Month = {jul},
	Title = {{Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms}},
	Url = {http://arxiv.org/abs/1407.8339},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1407.8339}}

@inproceedings{guillory2011online,
	Author = {Guillory, Andrew and Bilmes, Jeff},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Online submodular set cover, ranking, and repeated active learning}},
	Year = {2011}}

@phdthesis{filippi2010strategies,
	Author = {Filippi, Sarah},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	School = {T{\{}{\'{e}}{\}}l{\{}{\'{e}}{\}}com ParisTech},
	Title = {{Strat{\{}{\'{e}}{\}}gies optimistes en apprentissage par renforcement}},
	Year = {2010}}

@inproceedings{balcazar2006,
	Address = {Barcelona, Spain},
	Booktitle = {Proceedings of the 17th international conference on Algorithmic Learning Theory},
	Editor = {Balc{\'{a}}zar, Jos{\'{e}} L and Long, Philip M and Stephan, Frank},
	Isbn = {3-540-46649-5},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {oct},
	Publisher = {Springer},
	Series = {ALT '06, Lecture Notes in Computer Science},
	Title = {{No Title}},
	Volume = {4264},
	Year = {2006}}

@article{carpentier2014asimple,
	Author = {Carpentier, Alexandra and Valko, Michal},
	Journal = {ArXiv e-prints},
	Title = {{Simple regret for infinitely many armed bandits}},
	Year = {2015}}

@article{blum2007from,
	Author = {Blum, Avrim and Mansour, Yishay},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {dec},
	Pages = {1307--1324},
	Publisher = {JMLR.org},
	Title = {{From External to Internal Regret}},
	Volume = {8},
	Year = {2007}}

@inproceedings{bengio2009,
	Address = {Vancouver, British Columbia, Canada},
	Booktitle = {Proceedings of the 23rd conference on advances in Neural Information Processing Systems},
	Editor = {Bengio, Yoshua and Schuurmans, Dale and Lafferty, John D and Williams, Chris K I and Culotta, Aron},
	Month = {dec},
	Series = {NIPS '09},
	Title = {{No Title}},
	Year = {2009}}

@inproceedings{alon2013from,
	Abstract = {We consider the partial observability model for multi-armed bandits, introduced by Mannor and Shamir. Our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph. We also show that in the undirected case, the learner can achieve optimal regret without even accessing the observability graph before selecting an action. Both results are shown using variants of the Exp3 algorithm operating on the observability graph in a time-efficient manner.},
	Author = {Alon, Noga and Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio and Mansour, Yishay},
	Booktitle = {Neural Information Processing Systems},
	Title = {{From bandits to experts: A tale of domination and independence}},
	Url = {https://papers.nips.cc/paper/4908-from-bandits-to-experts-a-tale-of-domination-and-independence.pdf},
	Year = {2013},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/4908-from-bandits-to-experts-a-tale-of-domination-and-independence.pdf}}

@inproceedings{lugosi2006,
	Address = {Pittsburgh, PA, USA},
	Booktitle = {Proceedings of the 19th annual Conference On Learning Theory},
	Editor = {Lugosi, G{\'{a}}bor and Simon, Hans-Ulrich},
	Isbn = {3-540-35294-5},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jun},
	Publisher = {Springer},
	Series = {COLT '06, Lecture Notes in Computer Science},
	Title = {{No Title}},
	Volume = {4005},
	Year = {2006}}

@inproceedings{caron2012leveraging,
	Author = {Caron, St{\'{e}}phane and Kveton, Branislav and Lelarge, Marc and Bhagat, Smriti},
	Booktitle = {Uncertainty in Artificial Intelligence},
	Title = {{Leveraging side observations in stochastic bandits.}},
	Url = {https://arxiv.org/pdf/1210.4839.pdf},
	Year = {2012},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1210.4839.pdf}}

@article{bull2015adaptive,
	Author = {Bull, Adam D.},
	Journal = {Bernoulli},
	Keywords = {bandits on taxonomies,continuum-armed bandits,noisy global optimisation,tree-armed bandits,zooming dimension},
	Number = {4},
	Pages = {2289--2307},
	Title = {{Adaptive-treed bandits}},
	Url = {https://arxiv.org/pdf/1302.2489.pdf},
	Volume = {21},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1302.2489.pdf}}

@book{Shawe-Taylor2004,
	Author = {Shawe-Taylor, John and Cristianini, Nelo},
	Publisher = {Cambridge University Press},
	Title = {{Kernel Methods for Pattern Analysis}},
	Year = {2004}}

@article{Cesa-Bianchi2016a,
	Abstract = {We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than {\$}d{\$} hops to arrive, where {\$}d{\$} is a delay parameter. We introduce $\backslash$textsc{\{}Exp3-Coop{\}}, a cooperative version of the {\{}$\backslash$sc Exp3{\}} algorithm and prove that with {\$}K{\$} actions and {\$}N{\$} agents the average per-agent regret after {\$}T{\$} rounds is at most of order {\$}\backslashsqrt{\{}\backslashbigl(d+1 + \backslashtfrac{\{}K{\}}{\{}N{\}}\backslashalpha{\_}{\{}\backslashle d{\}}\backslashbigr)(T\backslashln K){\}}{\$}, where {\$}\backslashalpha{\_}{\{}\backslashle d{\}}{\$} is the independence number of the {\$}d{\$}-th power of the connected communication graph {\$}G{\$}. We then show that for any connected graph, for {\$}d=\backslashsqrt{\{}K{\}}{\$} the regret bound is {\$}K{\^{}}{\{}1/4{\}}\backslashsqrt{\{}T{\}}{\$}, strictly better than the minimax regret {\$}\backslashsqrt{\{}KT{\}}{\$} for noncooperating agents. More informed choices of {\$}d{\$} lead to bounds which are arbitrarily close to the full information minimax regret {\$}\backslashsqrt{\{}T\backslashln K{\}}{\$} when {\$}G{\$} is dense. When {\$}G{\$} has sparse components, we show that a variant of $\backslash$textsc{\{}Exp3-Coop{\}}, allowing agents to choose their parameters according to their centrality in {\$}G{\$}, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.},
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio and Mansour, Yishay and Minora, Alberto},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Cesa-Bianchi et al. - 2016 - Delay and cooperation in nonstochastic bandits.pdf:pdf},
	Month = {feb},
	Pages = {27},
	Title = {{Delay and Cooperation in Nonstochastic Bandits}},
	Url = {http://arxiv.org/abs/1602.04741},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1602.04741}}

@article{srinivas2009gaussian,
	Abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
	Author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Srinivas et al. - 2010 - Gaussian Process Optimization in the Bandit Setting No Regret and Experimental Design.3995:3995},
	Journal = {International Conference on Machine Learning},
	Title = {{Gaussian process optimization in the bandit setting: No regret and experimental design}},
	Url = {https://arxiv.org/pdf/0912.3995.pdf},
	Year = {2010},
	Bdsk-Url-1 = {https://arxiv.org/pdf/0912.3995.pdf}}

@article{berry1997bandit,
	Abstract = {We consider a bandit problem consisting of a sequence of {\$}n{\$} choices from an infinite number of Bernoulli arms, with {\$}n /rightarrow /infty{\$}. The objective is to minimize the long-run failure rate. The Bernoulli parameters are independent observations from a distribution {\$}F{\$}. We first assume {\$}F{\$} to be the uniform distribution on (0, 1) and consider various extensions. In the uniform case we show that the best lower bound for the expected failure proportion is between {\$}/sqrt2//sqrtn{\$} and {\$}2//sqrtn{\$} and we exhibit classes of strategies that achieve the latter.},
	Author = {Berry, Donald A. and Chen, Robert W. and Zame, Alan and Heath, David C. and Shepp, Larry A.},
	Journal = {Annals of Statistics},
	Keywords = {Bandit problems,Dynamic allocation of bernoulli processes,Sequential experimentation,Staying with a winner,Switching with a loser},
	Pages = {2103--2116},
	Title = {{Bandit problems with infinitely many arms}},
	Volume = {25},
	Year = {1997}}

@inproceedings{hutter2007,
	Address = {Sendai, Japan},
	Booktitle = {Proceedings of the 18th international conference on Algorithmic Learning Theory},
	Editor = {Hutter, Marcus and Servedio, Rocco A and Takimoto, Eiji},
	Isbn = {978-3-540-75224-0},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {oct},
	Publisher = {Springer},
	Series = {ALT '07, Lecture Notes in Computer Science},
	Title = {{No Title}},
	Volume = {4754},
	Year = {2007}}

@inproceedings{garivier2016optimal,
	Author = {Garivier, Aur{\'{e}}lien and Kaufmann, Emilie},
	File = {::},
	Title = {{Optimal Best Arm Identification with Fixed Confidence}},
	Year = {2016}}

@book{gittins1989multi,
	Author = {Gittins, John C and Weber, Richard and Glazebrook, Kevin},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Publisher = {Wiley},
	Title = {{Multi-armed Bandit Allocation Indices}},
	Year = {1989}}

@article{whittle1980multi,
	Author = {Whittle, Peter},
	Issn = {00359246},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Number = {2},
	Pages = {143--149},
	Publisher = {Blackwell Publishing for the Royal Statistical Society},
	Title = {{Multi-Armed Bandits and the Gittins Index}},
	Volume = {42},
	Year = {1980}}

@inproceedings{abbasi2011improved,
	Author = {Abbasi-Yadkori, Yasin and P{\'{a}}l, D{\'{a}}vid and Szepesv{\'{a}}ri, Csaba},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Downloads/linear-bandits-NIPS2011-camera-ready.pdf:pdf},
	Title = {{Improved algorithms for linear stochastic bandits}},
	Url = {https://papers.nips.cc/paper/4417-improved-algorithms-for-linear-stochastic-bandits.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/4417-improved-algorithms-for-linear-stochastic-bandits.pdf}}

@article{Bao2016a,
	Abstract = {Social networks have been popular platforms for information propagation. An important use case is viral marketing: given a promotion budget, an advertiser can choose some influential users as the seed set and provide them free or discounted sample products; in this way, the advertiser hopes to increase the popularity of the product in the users' friend circles by the world-of-mouth effect, and thus maximizes the number of users that information of the production can reach. There has been a body of literature studying the influence maximization problem. Nevertheless, the existing studies mostly investigate the problem on a one-off basis, assuming fixed known influence probabilities among users, or the knowledge of the exact social network topology. In practice, the social network topology and the influence probabilities are typically unknown to the advertiser, which can be varying over time, i.e., in cases of newly established, strengthened or weakened social ties. In this paper, we focus on a dynamic non-stationary social network and design a randomized algorithm, RSB, based on multi-armed bandit optimization, to maximize influence propagation over time. The algorithm produces a sequence of online decisions and calibrates its explore-exploit strategy utilizing outcomes of previous decisions. It is rigorously proven to achieve an upper-bounded regret in reward and applicable to large-scale social networks. Practical effectiveness of the algorithm is evaluated using both synthetic and real-world datasets, which demonstrates that our algorithm outperforms previous stationary methods under non-stationary conditions.},
	Author = {Bao, Yixin and Wang, Xiaoke and Wang, Zhi and Wu, Chuan and Lau, Francis C. M.},
	Month = {apr},
	Pages = {10},
	Title = {{Online Influence Maximization in Non-Stationary Social Networks}},
	Url = {http://arxiv.org/abs/1604.07638},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1604.07638}}

@article{Asadi2016a,
	Abstract = {A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one's weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study an alternative softmax operator that, among other properties, is both a non-expansion (ensuring convergent behavior in learning and planning) and differentiable (making it possible to improve decisions via gradient descent methods). We provide proofs of these properties and present empirical comparisons between various softmax operators.},
	Author = {Asadi, Kavosh and Littman, Michael L.},
	Month = {dec},
	Title = {{A New Softmax Operator for Reinforcement Learning}},
	Url = {http://arxiv.org/abs/1612.05628},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1612.05628}}

@inproceedings{jamieson2016non-stochastic,
	Author = {Jamieson, Kevin and Talwalkar, Ameet},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{Non-stochastic best arm identification and hyperparameter optimization}},
	Url = {http://proceedings.mlr.press/v51/jamieson16.pdf},
	Year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v51/jamieson16.pdf}}

@article{li2016efficient,
	Abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where allocation of additional resources to an arm corresponds to training a configuration on larger subsets of the data. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. We compare Hyperband with state-of-the-art Bayesian optimization methods and a random search baseline on a comprehensive benchmark including 117 datasets. Our results on this benchmark demonstrate that while Bayesian optimization methods do not outperform random search trained for twice as long, Hyperband in favorable settings offers valuable speedups.},
	Archiveprefix = {arXiv},
	Arxivid = {1603.06560},
	Author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	Eprint = {1603.06560},
	Journal = {arXiv:1603.06560},
	Title = {{Efficient hyperparameter optimization and infinitely many armed bandits}},
	Url = {https://arxiv.org/pdf/1603.06560.pdf},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1603.06560.pdf}}

@inproceedings{kalenon,
	Author = {Kale, Satyen and Reyzin, Lev and Schapire, Robert E},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {1054--1062},
	Title = {{Non-Stochastic Bandit Slate Problems}}}

@article{bubeck2011x,
	Abstract = {We consider a generalization of stochastic bandits where the set of arms, cX, is allowed to be a generic measurable space and the mean-payoff function is "locally Lipschitz" with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if cX is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by sqrtn, i.e., the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.},
	Author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles and Szepesv{\'{a}}ri, Csaba},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Bubeck et al. - 2011 - X-armed bandits.pdf:pdf},
	Journal = {Journal of Machine Learning Research},
	Pages = {1587--1627},
	Title = {{X-armed bandits}},
	Url = {http://www.jmlr.org/papers/volume12/bubeck11a/bubeck11a.pdf},
	Volume = {12},
	Year = {2011},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume12/bubeck11a/bubeck11a.pdf}}

@inproceedings{Neu2015a,
	Author = {Neu, Gergely},
	Booktitle = {Advances in Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Neu - 2015 - Explore no more Improved high-probability regret bounds for non-stochastic bandits.pdf:pdf},
	Pages = {3150--3158},
	Title = {{Explore no more: Improved high-probability regret bounds for non-stochastic bandits}},
	Url = {http://machinelearning.wustl.edu/mlpapers/papers/NIPS2015{\_}5732},
	Year = {2015},
	Bdsk-Url-1 = {http://machinelearning.wustl.edu/mlpapers/papers/NIPS2015%7B%5C_%7D5732}}

@article{Contal2016a,
	Abstract = {The paper considers the problem of global optimization in the setup of stochastic process bandits. We introduce an UCB algorithm which builds a cascade of discretization trees based on generic chaining in order to render possible his operability over a continuous domain. The theoretical framework applies to functions under weak probabilistic smoothness assumptions and also extends significantly the spectrum of application of UCB strategies. Moreover generic regret bounds are derived which are then specialized to Gaussian processes indexed on infinite-dimensional spaces as well as to quadratic forms of Gaussian processes. Lower bounds are also proved in the case of Gaussian processes to assess the optimality of the proposed algorithm.},
	Author = {Contal, Emile and Vayatis, Nicolas},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Contal, Vayatis - 2016 - Stochastic process bandits Upper confidence bounds algorithms via generic chaining.pdf:pdf},
	Month = {feb},
	Title = {{Stochastic Process Bandits: Upper Confidence Bounds Algorithms via Generic Chaining}},
	Url = {http://arxiv.org/abs/1602.04976},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1602.04976}}

@phdthesis{stoltz2005incomplete,
	Address = {Orsay, France},
	Author = {Stoltz, Gilles},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {may},
	School = {Universit{\{}{\'{e}}{\}} Paris-Sud},
	Title = {{Incomplete Information and Internal Regret in Prediction of Individual Sequences}},
	Type = {PhD thesis},
	Year = {2005}}

@article{munos2014from,
	Abstract = {This work covers several aspects of the optimism in the face of uncertainty principle applied to large scale optimization problems under finite numerical budget. The initial motivation for the research reported here originated from the empirical success of the so-called Monte-Carlo Tree Search method popularized in computer-go and further extended to many other games as well as optimization and planning problems. Our objective is to contribute to the development of theoretical foundations of the field by characterizing the complexity of the underlying optimization problems and designing efficient algorithms with performance guarantees. The main idea presented here is that it is possible to decompose a complex decision making problem (such as an optimization problem in a large search space) into a sequence of elementary decisions, where each decision of the sequence is solved using a (stochastic) multi-armed bandit (simple mathematical model for decision making in stochastic environments). This so-called hierarchical bandit approach (where the reward observed by a bandit in the hierarchy is itself the return of another bandit at a deeper level) possesses the nice feature of starting the exploration by a quasi-uniform sampling of the space and then focusing progressively on the most promising area, at different scales, according to the evaluations observed so far, and eventually performing a local search around the global optima of the function. The performance of the method is assessed in terms of the optimality of the returned solution as a function of the number of function evaluations. Our main contribution to the field of function optimization is a class of hierarchical optimistic algorithms designed for general search spaces (such as metric spaces, trees, graphs, Euclidean spaces, ...) with different algorithmic instantiations depending on whether the evaluations are noisy or noiseless and whether some measure of the ''smoothness'' of the function is known or unknown. The performance of the algorithms depend on the local behavior of the function around its global optima expressed in terms of the quantity of near-optimal states measured with some metric. If this local smoothness of the function is known then one can design very efficient optimization algorithms (with convergence rate independent of the space dimension), and when it is not known, we can build adaptive techniques that can, in some cases, perform almost as well as when it is known.},
	Author = {Munos, R{\'{e}}mi},
	Journal = {Foundations and Trends in Machine Learning},
	Pages = {1--130},
	Title = {{From bandits to Monte-Carlo tree search: The optimistic principle applied to optimization and planning}},
	Url = {https://hal.archives-ouvertes.fr/hal-00747575v5/document},
	Volume = {7(1)},
	Year = {2014},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-00747575v5/document}}

@incollection{langford2008epoch,
	Address = {Cambridge, MA},
	Author = {Langford, John and Zhang, Tong},
	Booktitle = {Advances in Neural Information Processing Systems 20},
	Editor = {Platt, J C and Koller, D and Singer, Y and Roweis, S},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {817--824},
	Publisher = {MIT Press},
	Title = {{The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information}},
	Year = {2008}}

@inproceedings{maillard2011adaptive,
	Author = {Maillard, Odalric-Ambrym and Munos, R{\'{e}}mi},
	Booktitle = {To appear in Proceedings of the 14th international conference on Artificial Intelligence and Statistics},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Series = {JMLR W{\&}CP},
	Title = {{Adaptive bandits: Towards the best history-dependent strategy}},
	Volume = {15},
	Year = {2011}}

@inproceedings{wang2017improving,
	Abstract = {We study combinatorial multi-armed bandit with probabilistically triggered arms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior CMAB-T studies where the regret bounds contain a possibly exponentially large factor of {\$}1/p{\^{}}*{\$}, where {\$}p{\^{}}*{\$} is the minimum positive probability that an arm is triggered by any action. We address this issue by introducing triggering probability moderated (TPM) bounded smoothness conditions into the general CMAB-T framework, and show that many applications such as influence maximization bandit and combinatorial cascading bandit satisfy such TPM conditions. As a result, we completely remove the factor of {\$}1/p{\^{}}*{\$} from the regret bounds, achieving significantly better regret bounds for influence maximization and cascading bandits than before. Finally, we provide lower bound results showing that the factor {\$}1/p{\^{}}*{\$} is unavoidable for general CMAB-T problems, suggesting that TPM conditions are crucial in removing this factor.},
	Archiveprefix = {arXiv},
	Arxivid = {1703.01610},
	Author = {Wang, Qinshi and Chen, Wei},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1703.01610},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Chen - 2017 - Improving regret bounds for combinatorial semi-bandits with probabilistically triggered arms and its applications(2).pdf:pdf},
	Month = {mar},
	Title = {{Improving regret bounds for combinatorial semi-bandits with probabilistically triggered arms and its applications}},
	Url = {http://arxiv.org/abs/1703.01610},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.01610}}

@book{lattimore2019bandit,
	Author = {Lattimore, Tor and Szepesv{\'{a}}ri, Csaba},
	Title = {{Bandit algorithms}},
	Url = {http://downloads.tor-lattimore.com/book.pdf},
	Year = {2019},
	Bdsk-Url-1 = {http://downloads.tor-lattimore.com/book.pdf}}

@inproceedings{wang2008algorithms,
	Author = {Wang, Yizao and Audibert, Jean-Yves and Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Algorithms for infinitely many-armed bandits}},
	Year = {2008}}

@inproceedings{hondaasymptotically,
	Author = {Honda, Junya and Takemura, Akimichi},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {67--79},
	Title = {{An Asymptotically Optimal Bandit Algorithm for Bounded Support Models}}}

@inproceedings{slivkins2009contextual,
	Abstract = {In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now well-understood, a lot of recent work has focused on MAB problems with exponentially or infinitely large strategy sets, where one needs to assume extra structure in order to make the problem tractable. In particular, recent literature considered information on similarity between arms. We consider similarity information in the setting of "contextual bandits", a natural extension of the basic MAB problem where before each round an algorithm is given the "context" - a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on webpages, one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a "similarity distance" between the context-arm pairs which gives an upper bound on the difference between the respective expected payoffs. Prior work on contextual bandits with similarity uses "uniform" partitions of the similarity space, which is potentially wasteful. We design more efficient algorithms that are based on adaptive partitions adjusted to "popular" context and "high-payoff" arms.},
	Annote = {From Duplicate 2 ( Contextual Bandits with Similarity Information - Slivkins, Aleksandrs )
And Duplicate 4 ( Contextual Bandits with Similarity Information - Slivkins, Aleksandrs )
And Duplicate 5 ( Contextual Bandits with Similarity Information - Slivkins, Aleksandrs )

From Duplicate 1 ( Contextual Bandits with Similarity Information - Slivkins, Aleksandrs )
},
	Author = {Slivkins, Aleksandrs},
	Booktitle = {Conference on Learning Theory},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Slivkins - 2009 - Contextual Bandits with Similarity Information.pdf:pdf},
	Keywords = {6,7 have been obtained,a postdoc brown,a preliminary version,a write up has,bandits,been circulated 2007,been posted arxiv,colt 2011,contextual bandits,full,has,metric spaces,multi armed bandits,online learning,org july 2009,regret minimization,results section,university,version a paper,which does not include,while author},
	Mendeley-Tags = {bandits},
	Title = {{Contextual bandits with similarity information}},
	Url = {http://proceedings.mlr.press/v19/slivkins11a/slivkins11a.pdf},
	Year = {2009},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v19/slivkins11a/slivkins11a.pdf}}

@article{audibert2010regret,
	Author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {dec},
	Pages = {2785--2836},
	Publisher = {JMLR.org},
	Title = {{Regret Bounds and Minimax Policies under Partial Monitoring}},
	Volume = {11},
	Year = {2010}}

@article{cope2009,
	Author = {Cope, Eric W},
	Journal = {IEEE Transactions on Automatic Control},
	Number = {6},
	Pages = {1243--1253},
	Title = {{Regret and convergence bounds for immediate-reward reinforcement learning with continuous action spaces}},
	Url = {https://pdfs.semanticscholar.org/f7ec/f6102c14e939ce115a36a1efd02fe5034173.pdf},
	Volume = {54},
	Year = {2009},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/f7ec/f6102c14e939ce115a36a1efd02fe5034173.pdf}}

@inproceedings{audibert2007tuning,
	Author = {Audibert, Jean-Yves and Munos, R{\'{e}}mi and Szepesv{\'{a}}ri, Csaba},
	Booktitle = {Algorithmic Learning Theory},
	Title = {{Tuning Bandit Algorithms in Stochastic Environments}},
	Year = {2007}}

@misc{pandora2013,
	Author = {Pandora},
	Howpublished = {http://www.pandora.com},
	Title = {{Internet Radio}},
	Url = {http://www.pandora.com},
	Year = {2013},
	Bdsk-Url-1 = {http://www.pandora.com}}

@techreport{combes2015unimodal,
	Abstract = {We consider stochastic bandit problems with a continuous set of arms and where the expected reward is a continuous and unimodal function of the arm. No further assumption is made regarding the smoothness and the structure of the expected reward function. For these problems, we propose the Stochastic Pentachotomy (SP) algorithm, and derive finite-time upper bounds on its regret and optimization error. In particular, we show that, for any expected reward function {\$}\backslashmu{\$} that behaves as {\$}\backslashmu(x)=\backslashmu(x{\^{}}\backslashstar)-C|x-x{\^{}}\backslashstar|{\^{}}\backslashxi{\$} locally around its maximizer {\$}x{\^{}}\backslashstar{\$} for some {\$}\backslashxi, C{\textgreater}0{\$}, the SP algorithm is order-optimal. Namely its regret and optimization error scale as {\$}O(\backslashsqrt{\{}T\backslashlog(T){\}}){\$} and {\$}O(\backslashsqrt{\{}\backslashlog(T)/T{\}}){\$}, respectively, when the time horizon {\$}T{\$} grows large. These scalings are achieved without the knowledge of {\$}\backslashxi{\$} and {\$}C{\$}. Our algorithm is based on asymptotically optimal sequential statistical tests used to successively trim an interval that contains the best arm with high probability. To our knowledge, the SP algorithm constitutes the first sequential arm selection rule that achieves a regret and optimization error scaling as {\$}O(\backslashsqrt{\{}T{\}}){\$} and {\$}O(1/\backslashsqrt{\{}T{\}}){\$}, respectively, up to a logarithmic factor for non-smooth expected reward functions, as well as for smooth functions with unknown smoothness.},
	Archiveprefix = {arXiv},
	Arxivid = {1406.7447},
	Author = {Combes, Richard and Prouti{\`{e}}re, Alexandre},
	Eprint = {1406.7447},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Combes, Prouti{\`{e}}re - 2015 - Unimodal Bandits without Smoothness.pdf:pdf},
	Title = {{Unimodal Bandits without Smoothness}},
	Year = {2015}}

@inproceedings{maillard2011finite,
	Author = {Maillard, Odalric-Ambrym and Munos, R{\'{e}}mi and Stoltz, Gilles},
	Booktitle = {To appear in Proceedings of the 24th annual Conference On Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Series = {COLT '11},
	Title = {{Finite-Time Analysis of Multi-armed Bandits Problems with Kullback-Leibler Divergences}},
	Year = {2011}}

@inproceedings{wen2013sequential,
	Author = {Wen, Zheng and Kveton, Branislav and Eriksson, Brian and Bhamidipati, Sandilya},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Sequential Bayesian search}},
	Year = {2013}}

@incollection{seldin2011pac,
	Author = {Seldin, Yevgeny and Auer, Peter and Laviolette, Francois and Shawe-Taylor, John S and Ortner, Ronald},
	Booktitle = {Neural Information Processing Systems (NIPS)},
	Pages = {1683--1691},
	Title = {{PAC-Bayesian Analysis of Contextual Bandits}},
	Year = {2011}}

@inproceedings{bubeck2008online,
	Author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles and Szepesv{\'{a}}ri, Csaba},
	Booktitle = {Advances in Neural Information Processing Systems},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {201--208},
	Title = {{Online Optimization of X-armed Bandits}},
	Year = {2008}}

@inproceedings{littlestone1989weighted,
	Address = {Washington, DC, USA},
	Author = {Littlestone, Nick and Warmuth, Manfred K},
	Booktitle = {Proceedings of the 30th annual Symposium on Foundations of Computer Science},
	Isbn = {0-8186-1982-1},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {256--261},
	Publisher = {IEEE Computer Society},
	Title = {{The weighted majority algorithm}},
	Year = {1989}}

@article{Kolla2016,
	Abstract = {We consider a collaborative online learning paradigm, wherein a group of agents connected through a social network are engaged in playing a stochastic multi-armed bandit game. Each time an agent takes an action, the corresponding reward is instantaneously observed by the agent, as well as its neighbours in the social network. We perform a regret analysis of various policies in this collaborative learning setting. A key finding of this paper is that natural extensions of widely-studied single agent learning policies to the network setting need not perform well in terms of regret. In particular, we identify a class of non-altruistic and individually consistent policies, and argue by deriving regret lower bounds that they are liable to suffer a large regret in the networked setting. We also show that the learning performance can be substantially improved if the agents exploit the structure of the network, and develop a simple learning algorithm based on dominating sets of the network. Specifically, we first consider a star network, which is a common motif in hierarchical social networks, and show analytically that the hub agent can be used as an information sink to expedite learning and improve the overall regret. We also derive networkwide regret bounds for the algorithm applied to general networks. We conduct numerical experiments on a variety of networks to corroborate our analytical results.},
	Author = {Kolla, Ravi Kumar and Jagannathan, Krishna and Gopalan, Aditya},
	Month = {feb},
	Pages = {14},
	Title = {{Collaborative Learning of Stochastic Bandits over a Social Network}},
	Url = {http://arxiv.org/abs/1602.08886},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1602.08886}}

@inproceedings{mellor2013nonstationary,
	Abstract = {Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS). We show empirical results of the algorithm in 4 artificial environments, and 2 derived from real world data; news click-through[Yahoo!, 2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other bandit algorithms. In real world data CTS is the most effective.},
	Archiveprefix = {arXiv},
	Arxivid = {1302.3721},
	Author = {Mellor, Joseph and Shapiro, Jonathan},
	Booktitle = {Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AIStats)},
	Eprint = {1302.3721},
	Issn = {15337928},
	Pages = {442--450},
	Title = {{Thompson sampling in switching environments with Bayesian online change point detection}},
	Url = {http://arxiv.org/pdf/1302.3721.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/pdf/1302.3721.pdf}}

@inproceedings{maillard2010online,
	Address = {Berlin, Heidelberg},
	Author = {Maillard, Odalric-Ambrym and Munos, R{\'{e}}mi},
	Booktitle = {Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part II},
	Isbn = {3-642-15882-X, 978-3-642-15882-7},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {305--320},
	Publisher = {Springer-Verlag},
	Series = {ECML PKDD'10},
	Title = {{Online learning in adversarial Lipschitz environments}},
	Year = {2010}}

@inproceedings{ghahramani2007,
	Address = {Corvalis, Oregon, USA},
	Booktitle = {Proceedings of the 24th International Conference on Machine Learning},
	Editor = {Ghahramani, Zoubin},
	Isbn = {978-1-59593-793-3},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jun},
	Publisher = {ACM},
	Series = {ICML '07, ACM International Conference Proceeding Series},
	Title = {{No Title}},
	Volume = {227},
	Year = {2007}}

@inproceedings{alon2015online,
	Abstract = {We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multi-armed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced {\$}T{\$}-round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with {\$}\backslashwidetilde\backslashTheta(\backslashalpha{\^{}}{\{}1/2{\}} T{\^{}}{\{}1/2{\}}){\$} minimax regret, where {\$}\backslashalpha{\$} is the independence number of the underlying graph; the second class induces problems with {\$}\backslashwidetilde\backslashTheta(\backslashdelta{\^{}}{\{}1/3{\}}T{\^{}}{\{}2/3{\}}){\$} minimax regret, where {\$}\backslashdelta{\$} is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time.},
	Author = {Alon, Noga and Cesa-Bianchi, Nicol{\`{o}} and Dekel, Ofer and Koren, Tomer},
	Booktitle = {Conference on Learning Theory},
	Title = {{Online learning with feedback graphs: Beyond bandits}},
	Url = {http://proceedings.mlr.press/v40/Alon15.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v40/Alon15.pdf}}

@inproceedings{coquelin2007bandit,
	Author = {Coquelin, Pierre-Arnaud and Munos, R{\'{e}}mi},
	Booktitle = {Uncertainty in Artificial Intelligence},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Bandit algorithms for tree search}},
	Url = {https://arxiv.org/pdf/1408.2028.pdf},
	Year = {2007},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1408.2028.pdf}}

@inproceedings{bubeck2013bounded,
	Abstract = {We study the stochastic multi-armed bandit problem when one knows the value {\$}\backslashmu{\^{}}{\{}(\backslashstar){\}}{\$} of an optimal arm, as a well as a positive lower bound on the smallest positive gap {\$}\backslashDelta{\$}. We propose a new randomized policy that attains a regret {\{}$\backslash$em uniformly bounded over time{\}} in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows {\$}\backslashDelta{\$}, and bounded regret of order {\$}1/\backslashDelta{\$} is not possible if one only knows {\$}\backslashmu{\^{}}{\{}(\backslashstar){\}}{\$}},
	Author = {Bubeck, S{\'{e}}bastien and Perchet, Vianney and Rigollet, Philippe},
	Booktitle = {Conference on Learning Theory},
	File = {::},
	Title = {{Bounded regret in stochastic multi-armed bandits}},
	Year = {2013}}

@article{bowling2015heads,
	Abstract = {Poker is a family of games that exhibit imperfect information, where players do not have full knowledge of past events. Whereas many perfect-information games have been solved (e.g., Connect Four and checkers), no nontrivial imperfect-information game played competitively by humans has previously been solved. Here, we announce that heads-up limit Texas hold'em is now essentially weakly solved. Furthermore, this computation formally proves the common wisdom that the dealer in the game holds a substantial advantage. This result was enabled by a new algorithm, CFR+, which is capable of solving extensive-form games orders of magnitude larger than previously possible. I'll see your program and raise you mine One of the fundamental differences between playing chess and two-handed poker is that the chessboard and the pieces on it are visible throughout the entire game, but an opponent's cards in poker are private. This informational deficit increases the complexity and the uncertainty in calculating the best course of action---to raise, to fold, or to call. Bowling et al. now report that they have developed a computer program that can do just that for the heads-up variant of poker known as Limit Texas Hold 'em (see the Perspective by Sandholm). Science, this issue p. 145; see also p. 122},
	Author = {Bowling, Michael and Burch, Neil and Johanson, Michael and Tammelin, Oskari},
	Journal = {Science},
	Number = {6218},
	Pages = {145--149},
	Title = {{Heads-up limit hold'em poker is solved}},
	Volume = {347},
	Year = {2015}}

@inproceedings{scholkopf2006,
	Address = {Vancouver, British Columbia, Canada},
	Booktitle = {Proceedings of the 20th conference on advances in Neural Information Processing Systems},
	Editor = {Sch{\"{o}}lkopf, Bernhard and Platt, John C and Hoffman, Thomas},
	Isbn = {0-262-19568-2},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {dec},
	Publisher = {MIT Press},
	Series = {NIPS '06},
	Title = {{No Title}},
	Year = {2006}}

@article{bartok2014partial,
	Abstract = {In a partial monitoring game, the learner repeatedly chooses an action, the environment responds with an outcome, and then the learner suffers a loss and receives a feedback signal, both of which are fixed functions of the action and the outcome. The goal of the learner is to minimize his regret, which is the difference between his total cumulative loss and the total loss of the best fixed action in hindsight. In this paper we characterize the minimax regret of any partial monitoring game with finitely many actions and outcomes. It turns out that the minimax regret of any such game is either zero, $\Theta$(√ T), $\Theta$(T 2/3), or $\Theta$(T). We provide computationally efficient learning algorithms that achieve the minimax regret within logarithmic factor for any game. In addition to the bounds on the minimax regret, if we assume that the outcomes are generated in an i.i.d. fashion, we prove individual upper bounds on the expected regret.},
	Author = {Bart{\'{o}}k, G{\'{a}}bor and Foster, Dean P. and P{\'{a}}l, D{\'{a}}vid and Rakhlin, Alexander and Szepesv{\'{a}}ri, Csaba},
	Journal = {Mathematics of Operations Research},
	Number = {4},
	Pages = {967--997},
	Title = {{Partial monitoring-classification, regret bounds, and algorithms}},
	Volume = {39},
	Year = {2014}}

@inproceedings{neu2013efficient,
	Author = {Neu, Gergely and Bart{\'{o}}k, G{\'{a}}bor},
	Booktitle = {Algorithmic Learning Theory},
	Title = {{An efficient algorithm for learning with semi-bandit feedback}},
	Year = {2013}}

@inproceedings{sen2017contextual,
	Abstract = {Motivated by online recommendation and advertising systems, we consider a causal model for stochastic contextual bandits with a latent low-dimensional confounder. In our model, there are {\$}L{\$} observed contexts and {\$}K{\$} arms of the bandit. The observed context influences the reward obtained through a latent confounder variable with cardinality {\$}m{\$} ({\$}m \backslashll L,K{\$}). The arm choice and the latent confounder causally determines the reward while the observed context is correlated with the confounder. Under this model, the {\$}L \backslashtimes K{\$} mean reward matrix {\$}\backslashmathbf{\{}U{\}}{\$} (for each context in {\$}[L]{\$} and each arm in {\$}[K]{\$}) factorizes into non-negative factors {\$}\backslashmathbf{\{}A{\}}{\$} ({\$}L \backslashtimes m{\$}) and {\$}\backslashmathbf{\{}W{\}}{\$} ({\$}m \backslashtimes K{\$}). This insight enables us to propose an {\$}\backslashepsilon{\$}-greedy NMF-Bandit algorithm that designs a sequence of interventions (selecting specific arms), that achieves a balance between learning this low-dimensional structure and selecting the best arm to minimize regret. Our algorithm achieves a regret of {\$}\backslashmathcal{\{}O{\}}\backslashleft(L\backslashmathrm{\{}poly{\}}(m, \backslashlog K) \backslashlog T \backslashright){\$} at time {\$}T{\$}, as compared to {\$}\backslashmathcal{\{}O{\}}(LK\backslashlog T){\$} for conventional contextual bandits, assuming a constant gap between the best arm and the rest for each context. These guarantees are obtained under mild sufficiency conditions on the factors that are weaker versions of the well-known Statistical RIP condition. We further propose a class of generative models that satisfy our sufficient conditions, and derive a lower bound of {\$}\backslashmathcal{\{}O{\}}\backslashleft(Km\backslashlog T\backslashright){\$}. These are the first regret guarantees for online matrix completion with bandit feedback, when the rank is greater than one. We further compare the performance of our algorithm with the state of the art, on synthetic and real world data-sets.},
	Archiveprefix = {arXiv},
	Arxivid = {1606.00119},
	Author = {Sen, Rajat and Shanmugam, Karthikeyan and Kocaoglu, Murat and Dimakis, Alexandros G. and Shakkottai, Sanjay},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Eprint = {1606.00119},
	File = {::},
	Title = {{Contextual bandits with Latent confounders: An NMF approach}},
	Year = {2017}}

@inproceedings{vaswani2015influence,
	Abstract = {Most work on influence maximization assumes network influence probabilities are given. The few papers that propose algorithms for learning these probabilities assume the availability of a batch of diffusion cascades and learn the probabilities offline. We tackle the real but difficult problems of (i)learning in influence probabilities and (ii) maximizing influence spread, when no cascades are available as input, by adopting a combinatorial multi-armed bandit (CMAB) paradigm. We formulate the above problems respectively as network exploration, i.e., minimizing the error in learned influence probabilities, and minimization of loss in spread from choosing suboptimal seed sets over the rounds of a CMAB game. We propose algorithms for both problems and establish bounds on their performance. Finally, we demonstrate the effectiveness and usefulness of the proposed algorithms via a comprehensive set of experiments over three real datasets.},
	Author = {Vaswani, Sharan and Lakshmanan, Laks. V. S. and {Mark Schmidt}},
	Booktitle = {NIPS workshop on Networks in the Social and Information Sciences 2015},
	Title = {{Influence maximization with bandits}},
	Year = {2015}}

@inproceedings{becker2002,
	Address = {Vancouver, British Columbia, Canada},
	Booktitle = {Proceedings of the 16th conference on advances in Neural Information Processing Systems},
	Editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
	Isbn = {0-262-02550-7},
	Month = {dec},
	Publisher = {MIT Press},
	Series = {NIPS '02},
	Title = {{No Title}},
	Year = {2002}}

@article{wang2005bandit,
	Author = {Wang, Chih-chun and Kulkarni, Sanjeev R and Poor, H Vincent},
	Journal = {IEEE Transactions on Automatic Control},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {338--355},
	Title = {{Bandit problems with side observations}},
	Volume = {50},
	Year = {2005}}

@inproceedings{krause11contextual,
	Author = {Krause, Andreas and Ong, Cheng Soon},
	Booktitle = {Proceedings of Neural Information Processing Systems (NIPS)},
	Title = {{Contextual Gaussian Process Bandit Optimization}},
	Year = {2011}}

@inproceedings{hanawal2015cheap,
	Abstract = {We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a Omega($\backslash$sqrt(dT)) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d.},
	Author = {Hanawal, Manjesh and Saligrama, Venkatesh and Valko, Michal and Munos, R{\'{e}}mi},
	Booktitle = {International Conference on Machine Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Hanawal et al. - 2015 - Cheap bandits.pdf:pdf},
	Title = {{Cheap bandits}},
	Url = {http://proceedings.mlr.press/v37/hanawal15.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v37/hanawal15.pdf}}

@inproceedings{fang2014networked,
	Author = {Fang, Meng and Tao, Dacheng},
	Booktitle = {International Conference on Knowledge Discovery and Data Mining},
	Keywords = {exploration/exploitation dilemma,networked bandits,social network},
	Title = {{Networked bandits with disjoint linear payoffs}},
	Url = {http://delivery.acm.org/10.1145/2630000/2623672/p1106-fang.pdf?ip=193.49.212.233{\&}id=2623672{\&}acc=ACTIVE SERVICE{\&}key=7EBF6E77E86B478F.5C2A4B72BE2A7DDF.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1528847270{\_}23b4902004322713593e1389d42ae48c},
	Year = {2014},
	Bdsk-Url-1 = {http://delivery.acm.org/10.1145/2630000/2623672/p1106-fang.pdf?ip=193.49.212.233%7B%5C&%7Did=2623672%7B%5C&%7Dacc=ACTIVE%20SERVICE%7B%5C&%7Dkey=7EBF6E77E86B478F.5C2A4B72BE2A7DDF.4D4702B0C3E38B35.4D4702B0C3E38B35%7B%5C&%7D%7B%5C_%7D%7B%5C_%7Dacm%7B%5C_%7D%7B%5C_%7D=1528847270%7B%5C_%7D23b4902004322713593e1389d42ae48c}}

@inproceedings{slivkins2008adapting,
	Author = {Slivkins, Aleksandrs and Upfal, Eli},
	Booktitle = {COLT},
	Editor = {Servedio, Rocco A and Zhang, Tong},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {343--354},
	Publisher = {Omnipress},
	Title = {{Adapting to a Changing Environment: the Brownian Restless Bandits}},
	Year = {2008}}

@inproceedings{kleinbergregret,
	Author = {Kleinberg, Robert D and Niculescu-Mizil, Alexandru and Sharma, Yogeshwer},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {425--436},
	Title = {{Regret Bounds for Sleeping Experts and Bandits}}}

@inproceedings{kleinberg2008multi,
	Author = {Kleinberg, Robert and Slivkins, Aleksandrs and Upfal, Eli},
	Booktitle = {Symposium on Theory Of Computing},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Title = {{Multi-armed bandit problems in metric spaces}},
	Url = {https://arxiv.org/pdf/0809.4882.pdf},
	Year = {2008},
	Bdsk-Url-1 = {https://arxiv.org/pdf/0809.4882.pdf}}

@article{cicirello2005max,
	Abstract = {The multiarmed bandit is often used as an analogy for the tradeoff between exploration and exploitation in search problems. The classic problem involves allocating trials to the arms of a multiarmed slot machine to maximize the expected sum of rewards. We pose a new variation of the multiarmed bandit---the Max K-Armed Bandit---in which trials must be allocated among the arms to maximize the expected best single sample reward of the series of trials. Motivation for the Max K-Armed Bandit is the allocation of restarts among a set of multistart stochastic search algorithms. We present an analysis of this Max K-Armed Bandit showing under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms. This motivates an exploration strategy that follows a Boltzmann distribution with an exponentially decaying temperature parameter. We compare this exploration policy to policies that allocate trials to the observed best arm at rates faster (and slower) than double exponentially. The results confirm, for two scheduling domains, that the double exponential increase in the rate of allocations to the observed best heuristic outperforms the other approaches.},
	Author = {Cicirello, Vincent A. and Smith, Stephen F.},
	Journal = {AAAI Conference on Artificial Intelligence},
	Title = {{The max k-armed bandit: A new model of exploration applied to search heuristic selection}},
	Url = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-215.pdf},
	Year = {2005},
	Bdsk-Url-1 = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-215.pdf}}

@inproceedings{furnkranz2010,
	Address = {Haifa, Israel},
	Booktitle = {Proceedings of the 27th International Conference on Machine Learning},
	Editor = {F{\"{u}}rnkranz, Johannes and Joachims, Thorsten},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jun},
	Publisher = {Omnipress},
	Series = {ICML '10},
	Title = {{No Title}},
	Year = {2010}}

@article{Prisadnikov2014a,
	Author = {Prisadnikov, Nedyalko},
	Publisher = {ETH-Z{\"{u}}rich, Department of Computer Science},
	Title = {{Exploration-exploitation trade-offs via probabilistic matrix factorization}},
	Url = {http://e-collection.library.ethz.ch/view/eth:14399},
	Year = {2014},
	Bdsk-Url-1 = {http://e-collection.library.ethz.ch/view/eth:14399}}

@article{hazan2007logarithmic,
	Author = {Hazan, Elad and Agarwal, Amit and Kale, Satyen},
	Journal = {Machine Learning},
	Number = {2-3},
	Pages = {169--192},
	Title = {{Logarithmic Regret Algorithms for Online Convex Optimization}},
	Volume = {69},
	Year = {2007}}

@article{Sho67,
	Annote = {(In Russian)},
	Author = {Shor, N},
	Journal = {Kibernetika},
	Pages = {53--55},
	Title = {{Generalized gradient descent with application to block programming}},
	Volume = {3},
	Year = {1967}}

@inproceedings{ferns2005metrics,
	Author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
	Booktitle = {Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence},
	Title = {{Metrics for {\{}Markov{\}} Decision Processes with Infinite State Spaces}},
	Year = {2005}}

@article{auer2002adaptive,
	Author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio},
	Journal = {Journal of Computer and System Sciences},
	Pages = {48--75},
	Title = {{Adaptive and self-confident on-line learning algorithms}},
	Volume = {64},
	Year = {2002}}

@article{bousquet2002stability,
	Author = {Bousquet, Olivier and Elisseeff, Andre},
	Journal = {Journal of Machine Learning Research},
	Pages = {499--526},
	Title = {{Stability and Generalization}},
	Volume = {2},
	Year = {2002}}

@article{Seldin2011,
	Abstract = {We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The scaling of our regret bound with the number of states (contexts) N goes as sqrtN Irhot(S;A), where Irhot(S;A) is the mutual information between states and actions (the side information) used by the algorithm at round t. If the algorithm uses all the side information, the regret bound scales as sqrtN ln K, where K is the number of actions (arms). However, if the side information Irhot(S;A) is not fully used, the regret bound is significantly tighter. In the extreme case, when Irhot(S;A) = 0, the dependence on the number of states reduces from linear to logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with O(K) computational complexity per game round.},
	Author = {Seldin, Yevgeny and Auer, Peter and Laviolette, Fran{\c{c}}ois and Shawe-Taylor, John and Ortner, Ronald},
	Editor = {Shawe-Taylor, J and Zemel, R S and Bartlett, P and Pereira, F C N and Weinberger, K Q},
	Journal = {Computer},
	Keywords = {complacs,computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
	Pages = {1--9},
	Title = {{PAC-Bayesian Analysis of Contextual Bandits}},
	Url = {http://eprints.pascal-network.org/archive/00008826/},
	Year = {2011},
	Bdsk-Url-1 = {http://eprints.pascal-network.org/archive/00008826/}}

@techreport{nene1996columbia,
	Author = {Nene, S A and Nayar, S K and Murase, H},
	Booktitle = {Technical Report, Department of Computer Science, Columbia University CUCS-005-96},
	Institution = {Columbia University},
	Month = {feb},
	Title = {{{\{}C{\}}olumbia {\{}O{\}}bject {\{}I{\}}mage {\{}L{\}}ibrary ({\{}C{\}}{\{}O{\}}{\{}I{\}}{\{}L{\}}-100)}},
	Year = {1996}}

@inproceedings{krause2005near,
	Author = {Krause, A and Guestrin, C},
	Booktitle = {Proc. UAI},
	Title = {{Near-optimal nonmyopic value of information in graphical models}},
	Year = {2005}}

@article{Beck2009,
	Author = {Beck, A and Teboulle, M},
	Journal = {SIAM Journal on Imaging Sciences},
	Number = {1},
	Pages = {183--202},
	Title = {{A fast iterative shrinkage-thresholding algorithm for linear inverse problems}},
	Volume = {2},
	Year = {2009}}

@inproceedings{yu09ArbitraryRewardsTransitions,
	Address = {Piscataway, NJ, USA},
	Author = {Yu, J Y and Mannor, S},
	Booktitle = {GameNets'09: Proceedings of the First ICST International Conference on Game Theory for Networks},
	Isbn = {978-1-4244-4176-1},
	Pages = {314--322},
	Publisher = {IEEE Press},
	Title = {{Online learning in {\{}M{\}}arkov decision processes with arbitrarily changing rewards and transitions}},
	Year = {2009}}

@inproceedings{gautier2017zonotope,
	Abstract = {Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers. We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costly-to-evaluate functions, such as summary extraction in our experiments.},
	Author = {Gautier, Guillaume and Bardenet, R{\'{e}}mi and Valko, Michal},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Zonotope hit-and-run for efficient sampling from projection DPPs}},
	Year = {2017}}

@inproceedings{shelton2001policy,
	Author = {Shelton, Christian},
	Booktitle = {Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence},
	Pages = {496--503},
	Title = {{Policy Improvement for {\{}POMDPs{\}} Using Normalized Importance Sampling}},
	Year = {2001}}

@article{mobius,
	Author = {Foldes, S and Hammer, P L},
	Journal = {Mathematics of Operations Research},
	Number = {2},
	Pages = {453--461},
	Title = {{Submodularity, Supermodularity, and Higher-Order Monotonicities of Pseudo-{\{}B{\}}oolean Functions}},
	Volume = {30},
	Year = {2005}}

@inproceedings{Pomerleau_1989_2055,
	Author = {Pomerleau, Dean},
	Pages = {305--313},
	Title = {{{\{}ALVINN{\}}: An Autonomous Land Vehicle in a Neural Network}}}

@article{srivastava1996predictive,
	Author = {Srivastava, Mani and Chandrakasan, Anantha and Brodersen, Robert},
	Journal = {IEEE Transactions on Very Large Scale Integration Systems},
	Number = {1},
	Pages = {42--55},
	Title = {{Predictive System Shutdown and Other Architectural Techniques for Energy Effcient Programmable Computation}},
	Volume = {4},
	Year = {1996}}

@article{scholkopf1999estimating,
	Annote = {comps{\_}ano},
	Author = {Scholkopf, Bernhard and Platt, John C and Shawe-taylor, John and Smola, Alex J and Williamson, Robert C and Sch{\"{o}}lkopf, Bernhard},
	Journal = {Neural Computation},
	Pages = {2001},
	Title = {{Estimating the support of a high-dimensional distribution}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.9423},
	Volume = {13},
	Year = {1999},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.9423}}

@inproceedings{shapinglevelsets,
	Author = {Bach, F},
	Booktitle = {Adv. NIPS},
	Title = {{Shaping Level Sets with Submodular Functions}},
	Year = {2011}}

@inproceedings{sprechmann2010collaborative,
	Author = {Sprechmann, P and Ramirez, I and Sapiro, G and Eldar, Y},
	Booktitle = {Conf. Information Sciences and Systems (CISS)},
	Title = {{Collaborative hierarchical sparse modeling}},
	Year = {2010}}

@article{kendall1945treatment,
	Author = {Kendall, M G},
	Journal = {Biometrika},
	Pages = {239--251},
	Title = {{The treatment of ties in ranking problems}},
	Volume = {33},
	Year = {1945}}

@article{russo2014learning,
	Author = {Russo, Daniel and {Van Roy}, Benjamin},
	Journal = {Mathematics of Operations Research},
	Title = {{Learning to Optimize Via Posterior Sampling}},
	Year = {2014}}

@inproceedings{moonesignhe2006outlier,
	Address = {Washington, DC, USA},
	Author = {Moonesignhe, H D K and Tan, Pang-Ning},
	Booktitle = {ICTAI '06: Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence},
	Doi = {http://dx.doi.org/10.1109/ICTAI.2006.94},
	Isbn = {0-7695-2728-0},
	Pages = {532--539},
	Publisher = {IEEE Computer Society},
	Title = {{Outlier Detection Using Random Walks}},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICTAI.2006.94}}

@techreport{perrault2019finding,
	Abstract = {We consider the problem where an agent wants to find a hidden object that is randomly located in some vertex of a directed acyclic graph (DAG) according to a fixed but possibly unknown distribution. The agent can only examine vertices whose in-neighbors have already been examined. In scheduling theory, this problem is denoted by 1|prec|∑wjCj [Graham1979]. However, in this paper we address a learning setting where we allow the agent to stop before having found the object and restart searching on a new independent instance of the same problem. The goal is to maximize the total number of hidden objects found under a time constraint. The agent can thus skip an instance after realizing that it would spend too much time on it. Our contributions are both to the search theory and multi-armed bandits. If the distribution is known, we provide a quasi-optimal greedy strategy with the help of known computationally efficient algorithms for solving 1|prec|∑wjCj under some assumption on the DAG. If the distribution is unknown, we show how to sequentially learn it and, at the same time, act near-optimally in order to collect as many hidden objects as possible. We provide an algorithm, prove theoretical guarantees, and empirically show that it outperforms the naive baseline.},
	Author = {Perrault, Pierre and Perchet, Vianney and Valko, Michal},
	Title = {{Finding the bandit in a graph: Sequential search-and-stop}},
	Year = {2019}}

@article{baxter2001experiments,
	Author = {Baxter, Jonathan and Bartlett, Peter and Weaver, Lex},
	Journal = {Journal of Artificial Intelligence Research},
	Pages = {351--381},
	Title = {{Experiments with Infinite-Horizon, Policy-Gradient Estimation}},
	Volume = {15},
	Year = {2001}}

@incollection{suehiro12submodular,
	Author = {Suehiro, Daiki and Hatano, Kohei and Kijima, Shuji and Takimoto, Eiji and Nagano, Kiyohito},
	Booktitle = {Algorithmic Learning Theory},
	Isbn = {978-3-642-34105-2},
	Pages = {260--274},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {{Online Prediction under Submodular Constraints}},
	Volume = {7568},
	Year = {2012}}

@inproceedings{schultz2003learning,
	Annote = {comps{\_}distance},
	Author = {Schultz, Matthew and Joachims, Thorsten},
	Booktitle = {In NIPS},
	Publisher = {MIT Press},
	Title = {{Learning a distance metric from relative comparisons}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.1616},
	Year = {2003},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.1616}}

@article{Hofmann2011,
	Abstract = {In this paper we give an overview of and outlook on research at the intersection of information retrieval (IR) and contextual bandit problems. A critical problem in information retrieval is online learning to rank, where a search engine strives to improve the quality of the ranked result lists it presents to users on the basis of those users' interactions with those result lists. Recently, researchers have started to model interactions between users and search engines as contextual bandit problems, and initial methods for learning in this setting have been devised. Our research focuses on two aspects: balancing exploration and exploitation and inferring preferences from implicit user interactions. This paper summarizes our recent work on online learning to rank for information retrieval and points out challenges that are characteristic of this application area.},
	Author = {Hofmann, Katja},
	Journal = {NIPS 2011 Proceedings of the Conference on Neural Information Processing Systems Workshop on Bayesian Optimization Experimental Design and Bandits Theory and Applications},
	Pages = {1--5},
	Title = {{Contextual Bandits for Information Retrieval}},
	Url = {http://www.cs.ubc.ca/{~}hutter/nips2011workshop/papers{\_}and{\_}posters/nips-2012-rl4ir.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/%7B~%7Dhutter/nips2011workshop/papers%7B%5C_%7Dand%7B%5C_%7Dposters/nips-2012-rl4ir.pdf}}

@inproceedings{ratliff07subgradient,
	Author = {Ratliff, Nathan and Bagnell, James (Drew) and Zinkevich, Martin},
	Pages = {2:380--387},
	Title = {{({\{}O{\}}nline) {\{}S{\}}ubgradient Methods for Structured Prediction}}}

@inproceedings{atkeson,
	Author = {Atkeson, C G and Schaal, S},
	Pages = {12--20},
	Title = {{Robot learning from demonstration}}}

@article{hill1975simple,
	Author = {Hill, Bruce M.},
	Journal = {The Annals of Statistics},
	Keywords = {Bayesian inference,Tail of distribution,order statistics},
	Language = {EN},
	Number = {5},
	Pages = {1163--1174},
	Publisher = {Institute of Mathematical Statistics},
	Title = {{A Simple General Approach to Inference About the Tail of a Distribution}},
	Volume = {3},
	Year = {1975}}

@article{kempe2015maximizing,
	Author = {Kempe, David and Kleinberg, Jon and Tardos, {\'{E}}va},
	Journal = {Theory of Computing},
	Number = {4},
	Pages = {105--147},
	Title = {{Maximizing the spread of influence through a social network}},
	Volume = {11},
	Year = {2015}}

@incollection{ORVR13,
	Author = {Osband, Ian and Russo, Dan and {Van Roy}, Benjamin},
	Booktitle = {Advances in Neural Information Processing Systems 26},
	Editor = {Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and Weinberger, K Q},
	Pages = {3003--3011},
	Title = {{({\{}M{\}}ore) Efficient Reinforcement Learning via Posterior Sampling}},
	Year = {2013}}

@inproceedings{viola2001robust,
	Author = {Viola, Paul A and Jones, Michael J},
	Booktitle = {ICCV},
	Pages = {747},
	Title = {{Robust Real-Time Face Detection}},
	Year = {2001}}

@article{barto1983neuronlike,
	Author = {Barto, Andrew and Sutton, Richard and Anderson, Charles},
	Journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	Number = {5},
	Pages = {835--846},
	Title = {{Neuronlike Elements that Can Solve Difficult Learning Control Problems}},
	Volume = {13},
	Year = {1983}}

@book{munkres1984elements,
	Author = {Munkres, J R},
	Publisher = {Addison-Wesley Reading, MA},
	Title = {{Elements of algebraic topology}},
	Volume = {2},
	Year = {1984}}

@article{lane1999temporal,
	Address = {New York, NY, USA},
	Annote = {comps{\_}anX},
	Author = {Lane, Terran and Brodley, Carla E},
	Doi = {http://doi.acm.org/10.1145/322510.322526},
	Issn = {1094-9224},
	Journal = {ACM Trans. Inf. Syst. Secur.},
	Number = {3},
	Pages = {295--331},
	Publisher = {ACM},
	Title = {{Temporal sequence learning and data reduction for anomaly detection}},
	Volume = {2},
	Year = {1999},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/322510.322526}}

@inproceedings{bach2013sharp,
	Abstract = {We consider supervised learning problems within the positive-definite kernel framework, such as kernel ridge regression, kernel logistic regression or the support vector machine. With kernels leading to infinite-dimensional feature spaces, a common practical limiting difficulty is the necessity of computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic in the number of observations n, i.e., O(n{\^{}}2). Low-rank approximations of the kernel matrix are often considered as they allow the reduction of running time complexities to O(p{\^{}}2 n), where p is the rank of the approximation. The practicality of such methods thus depends on the required rank p. In this paper, we show that in the context of kernel ridge regression, for approximations based on a random subset of columns of the original kernel matrix, the rank p may be chosen to be linear in the degrees of freedom associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have sub-quadratic running time complexity, but provably exhibit the same predictive performance than existing algorithms, for any given problem instance, and not only for worst-case situations.},
	Author = {Bach, Francis},
	Booktitle = {Conference on Learning Theory},
	Title = {{Sharp analysis of low-rank kernel matrix approximations}},
	Year = {2013}}

@article{kalai2003efficient,
	Author = {Kalai, Adam and Vempala, Santosh},
	Journal = {Journal of Computer and System Sciences},
	Number = {3},
	Pages = {291--307},
	Title = {{Efficient algorithms for online decision problems}},
	Volume = {71},
	Year = {2005}}

@article{thompson1933likelihood,
	Author = {Thompson, William R.},
	Journal = {Biometrika},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {285--294},
	Title = {{On the likelihood that one unknown probability exceeds another in view of the evidence of two samples}},
	Url = {https://www.jstor.org/stable/pdf/2332286.pdf},
	Volume = {25},
	Year = {1933},
	Bdsk-Url-1 = {https://www.jstor.org/stable/pdf/2332286.pdf}}

@article{yedidia2005constructing,
	Annote = {comps{\_}models},
	Author = {Yedidia, J S and Freeman, W T and Weiss, Y},
	Doi = {10.1109/TIT.2005.850085},
	Issn = {0018-9448},
	Journal = {Information Theory, IEEE Transactions on},
	Keywords = {GBP algorithm,Kikuchi free energy,backpropagation,belief networks,cluster variation method,factor graphs,free energy approximation,generalized belief propagation,graph theory,inference mechanisms,inference problem,junction graph method,message passing,message passing Bethe approximation,region graph method,sum-product algorithm},
	Month = {jul},
	Number = {7},
	Pages = {2282--2312},
	Title = {{Constructing free-energy approximations and generalized belief propagation algorithms}},
	Volume = {51},
	Year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIT.2005.850085}}

@article{robbins1952some,
	Author = {Robbins, Herbert},
	Journal = {Bulletin of the American Mathematics Society},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {527--535},
	Title = {{Some aspects of the sequential design of experiments}},
	Volume = {58},
	Year = {1952}}

@article{girvan2002community,
	Abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known--a collaboration network and a food web--and find that it detects significant and informative community divisions in both cases.},
	Author = {Girvan, Michelle and Newman, Mark E J},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Girvan, Newman - 2002 - Community structure in social and biological networks.pdf:pdf},
	Journal = {National Academy of Sciences of the United States of America},
	Keywords = {Algorithms,Animals,Community Networks,Computer Simulation,Humans,Models,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Social Behavior,Theoretical},
	Number = {12},
	Pages = {7821--6},
	Title = {{Community structure in social and biological networks.}},
	Volume = {99},
	Year = {2002}}

@article{kelner_spectral_2013,
	Author = {Kelner, Jonathan A and Levin, Alex},
	Journal = {Theory of Computing Systems},
	Number = {2},
	Pages = {243--262},
	Title = {{Spectral sparsification in the semi-streaming setting}},
	Volume = {53},
	Year = {2013}}

@book{zhang2005schur,
	Author = {Zhang, Fuzhen},
	Publisher = {Springer},
	Title = {{The Schur complement and its applications}},
	Volume = {4},
	Year = {2005}}

@book{cormen89introduction,
	Author = {Cormen, T H and Leiserson, C E and Rivest, R L},
	Publisher = {MIT Press},
	Title = {{Introduction to Algorithms}},
	Year = {1989}}

@article{kolar2010estimating,
	Author = {Kolar, Mladen and Song, Le and Ahmed, Amr and Xing, Eric P},
	Doi = {10.1214/09-AOAS308},
	Journal = {Annals of Applied Statistics},
	Pages = {94--123},
	Title = {{Estimating time-varying networks}},
	Volume = {4},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1214/09-AOAS308}}

@article{Lin2015,
	Abstract = {In decision-theoretic troubleshooting, we are given a Bayesian network model of a malfunctioning device and our task is to find a repair strategy with minimal expected cost. The troubleshooting problem has received considerable attention over the past two decades. We show that several troubleshooting scenarios proposed in the literature are equivalent to well-studied machine scheduling problems. This immediately yields new complexity-theoretic and algorithmic results for troubleshooting. We also apply scheduling results to multi-agent troubleshooting. Further, we examine the so-called call service action which is often used in troubleshooting but has no natural counterpart in machine scheduling. We show that adding the call service action to basic troubleshooting models does not make the problem intractable.},
	Author = {L{\'{i}}n, V{\'{a}}clav},
	Doi = {10.1016/j.ijar.2014.08.004},
	Issn = {0888613X},
	Journal = {International Journal of Approximate Reasoning},
	Keywords = {Algorithms,Computational complexity,Decision-theoretic troubleshooting,Single machine scheduling with weighted flowtime},
	Number = {PA},
	Pages = {87--107},
	Title = {{Scheduling results applicable to decision-theoretic troubleshooting}},
	Volume = {56},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.ijar.2014.08.004}}

@article{beygelzimer2010contextual,
	Abstract = {We address the problem of learning in an online, bandit setting where the learner must repeatedly select among K actions, but only receives partial feedback based on its choices. We establish two new facts: First, using a new algorithm called Exp4.P, we show that it is possible to compete with the best in a set of N experts with probability {\$}1-delta while incurring regret at most O(sqrtKTln(N/delta)) over T time steps. The new algorithm is tested empirically in a large-scale, real-world dataset. Second, we give a new algorithm called VE that competes with a possibly infinite set of policies of VC-dimension d while incurring regret at most O(sqrtT(dln(T) + ln (1/delta))) with probability {\$}1-delta. These guarantees improve on those of all previous algorithms, whether in a stochastic or adversarial environment, and bring us closer to providing supervised learning type guarantees for the contextual bandit setting.},
	Author = {Beygelzimer, Alina and Langford, John and Li, Lihong and Reyzin, Lev and Schapire, Robert E},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Beygelzimer et al. - 2010 - Contextual Bandit Algorithms with Supervised Learning Guarantees.pdf:pdf},
	Journal = {Machine Learning},
	Pages = {14},
	Title = {{Contextual Bandit Algorithms with Supervised Learning Guarantees}},
	Url = {http://arxiv.org/abs/1002.4058},
	Volume = {15},
	Year = {2010},
	Bdsk-Url-1 = {http://arxiv.org/abs/1002.4058}}

@inproceedings{sanner2006practical,
	Author = {Sanner, Scott and Boutilier, Craig},
	Booktitle = {Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence},
	Title = {{Practical Linear Value-Approximation Techniques for First-Order {\{}MDPs{\}}}},
	Year = {2006}}

@inproceedings{dearden1999model,
	Author = {Dearden, Richard and Friedman, Nir and Andre, David},
	Booktitle = {Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence},
	Pages = {150--159},
	Title = {{Model Based {\{}Bayesian{\}} Exploration}},
	Year = {1999}}

@article{tesauro1995temporal,
	Author = {Tesauro, Gerald},
	Journal = {Communications of the ACM},
	Number = {3},
	Pages = {58--68},
	Title = {{Temporal Difference Learning and {\{}TD-Gammon{\}}}},
	Volume = {38},
	Year = {1995}}

@inproceedings{collins00discriminative,
	Author = {Collins, Michael},
	Pages = {175--182},
	Title = {{Discriminative Reranking for Natural Language Parsing}}}

@inproceedings{peters10reps,
	Author = {Peters, Jan and M{\"{u}}lling, Katharina and Altun, Yasemin},
	Pages = {1607--1612},
	Title = {{Relative Entropy Policy Search}}}

@article{ghavamzadeh2016bayesian,
	Abstract = {Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Many conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. The policy is improved by adjusting the parameters in the direction of the gradient estimate. Since Monte-Carlo methods tend to have high variance, a large number of samples is required to attain accurate estimates, resulting in slow convergence. In this paper, we first propose a Bayesian framework for policy gradient, based on modeling the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates, namely, the gradient covariance, are provided at little extra cost. Since the proposed Bayesian framework considers system trajectories as its basic observable unit, it does not require the dynamics within trajectories to be of any particular form, and thus, can be easily extended to partially observable problems. On the downside, it cannot take advantage of the Markov property when the system is Markovian. To address this issue, we proceed to supplement our Bayesian policy gradient framework with a new actor-critic learning model in which a Bayesian class of non- parametric critics, based on Gaussian process temporal difference learning, is used. Such critics model the action- value function as a Gaussian process, allowing Bayes' rule to be used in computing the posterior distribution over action-value functions, conditioned on the observed data. Appropriate choices of the policy parameterization and of the prior covariance (kernel) between action-values allow us to obtain closed-form expressions for the posterior distribution of the gradient of the expected return with respect to the policy parameters. We perform detailed experimental comparisons of the proposed Bayesian policy gradient and actor-critic algorithms with classic Monte-Carlo based policy gradient methods, as well as with each other, on a number of reinforcement learning problems.},
	Author = {Ghavamzadeh, Mohammad and Engel, Yaakov and Valko, Michal},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ghavamzadeh, Engel, Valko - 2016 - Bayesian policy gradient and actor-critic algorithms.pdf:pdf},
	Journal = {Journal of Machine Learning Research},
	Number = {66},
	Pages = {1--53},
	Title = {{Bayesian policy gradient and actor-critic algorithms}},
	Volume = {17},
	Year = {2016}}

@article{karypis1999fast,
	Author = {Karypis, G and Kumar, V},
	Journal = {SIAM Journal on Scientific Computing},
	Pages = {359--392},
	Title = {{A fast and high quality multilevel scheme for partitioning irregular graphs}},
	Volume = {20},
	Year = {1999}}

@article{even-dar09OnlineMDP,
	Address = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
	Author = {Even-Dar, Eyal and Kakade, Sham. M and Mansour, Yishay},
	Doi = {http://dx.doi.org/10.1287/moor.1090.0396},
	Issn = {0364-765X},
	Journal = {Mathematics of Operations Research},
	Number = {3},
	Pages = {726--736},
	Publisher = {INFORMS},
	Title = {{Online {\{}M{\}}arkov Decision Processes}},
	Volume = {34},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/moor.1090.0396}}

@inproceedings{syed2007mwal,
	Author = {Syed, Umar and Schapire, Robert},
	Pages = {1449--1456},
	Title = {{A Game-Theoretic Approach to Apprenticeship Learning}}}

@inproceedings{calandriello2017second,
	Abstract = {Kernel online convex optimization (KOCO) is a framework combining the expressiveness of non-parametric kernel models with the regret guarantees of online learning. First-order KOCO methods such as functional gradient descent require only O(t) time and space per iteration, and, when the only information on the losses is their convexity, achieve a minimax optimal O(sqrtT) regret. Nonetheless, many common losses in kernel problems, such as squared loss, logistic loss, and squared hinge loss posses stronger curvature that can be exploited. In this case, second-order KOCO methods achieve O(log(Det(K))) regret, which we show scales as O(deff log T), where deff is the effective dimension of the problem and is usually much smaller than O(sqrtT). The main drawback of second-order methods is their much higher O(t 2) space and time complexity. In this paper, we introduce kernel online Newton step (KONS), a new second-order KOCO method that also achieves O(defflog T) regret. To address the computational complexity of second-order methods, we introduce a new matrix sketching algorithm for the kernel matrix K, and show that for a chosen parameter gamma leq 1 our Sketched-KONS reduces the space and time complexity by a factor of gamma 2 to O(t 2gamma 2) space and time per iteration, while incurring only 1/gamma times more regret.},
	Author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Second-order kernel online convex optimization with adaptive sketching}},
	Year = {2017}}

@book{rockafellar81theory,
	Author = {Rockafellar, R Tyrell},
	Publisher = {Heldermann Verlag, Berlin},
	Title = {{The theory of subgradients and its applications to problems of optimization: Convex and nonconvex functions}},
	Year = {1981}}

@article{cap,
	Author = {Zhao, P and Rocha, G and Yu, B},
	Journal = {Annals of Statistics},
	Number = {6A},
	Pages = {3468--3497},
	Title = {{Grouped and hierarchical model selection through composite absolute penalties}},
	Volume = {37},
	Year = {2009}}

@book{filar1996competitive,
	Address = {New York, NY},
	Author = {Filar, Jerzy and Vrieze, Koos},
	Isbn = {1461284813 9781461284819},
	Publisher = {Springer New York},
	Title = {{Competitive Markov decision processes}},
	Year = {2012}}

@phdthesis{kveton2006planning,
	Author = {Kveton, Branislav},
	School = {University of Pittsburgh},
	Title = {{Planning in Hybrid Structured Stochastic Domains}},
	Year = {2006}}

@article{boularias2013apprenticeship,
	Author = {Boularias, Abdeslam and Chaib-draa, Brahim},
	Doi = {http://dx.doi.org/10.1016/j.neucom.2012.11.002},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {Bootstrapping,Imitation learning,Inverse reinforcement learning,Transfer learning},
	Number = {0},
	Pages = {83--96},
	Title = {{Apprenticeship learning with few examples}},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231212008363},
	Volume = {104},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231212008363},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.neucom.2012.11.002}}

@inproceedings{LaurentGuillaumeGroupLasso,
	Author = {Jacob, L and Obozinski, G and Vert, J.-P.},
	Booktitle = {Proc. ICML},
	Title = {{Group {\{}L{\}}asso with overlaps and graph {\{}L{\}}asso}},
	Year = {2009}}

@article{Bull2015b,
	Author = {Bull, Adam D.},
	Issn = {1350-7265},
	Journal = {Bernoulli},
	Keywords = {bandits on taxonomies,continuum-armed bandits,noisy global optimisation,tree-armed bandits,zooming dimension},
	Month = {nov},
	Number = {4},
	Pages = {2289--2307},
	Publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	Title = {{Adaptive-treed Bandits}},
	Url = {http://projecteuclid.org/euclid.bj/1438777594},
	Volume = {21},
	Year = {2015},
	Bdsk-Url-1 = {http://projecteuclid.org/euclid.bj/1438777594}}

@inproceedings{aggarwal2001outlier,
	Address = {New York, NY, USA},
	Annote = {comps{\_}ano},
	Author = {Aggarwal, Charu C and Yu, Philip S},
	Booktitle = {SIGMOD '01: Proceedings of the 2001 ACM SIGMOD international conference on Management of data},
	Doi = {http://doi.acm.org/10.1145/375663.375668},
	Isbn = {1-58113-332-4},
	Pages = {37--46},
	Publisher = {ACM},
	Title = {{Outlier detection for high dimensional data}},
	Year = {2001},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/375663.375668}}

@article{zhou2004learning,
	Author = {Zhou, D and Bousquet, O and Lal, T N and Weston, J and Scholkopf, B},
	Journal = {Advances in Neural Information Processing Systems},
	Keywords = {manifold{\_}learning},
	Pages = {321--328},
	Title = {{Learning with local and global consistency}},
	Volume = {16},
	Year = {2004}}

@inproceedings{boularias2011model,
	Abstract = {We consider the problem of imitation learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is optimally acting in a Markov Decision Process (MDP). Past work on IRL requires that an accurate model of the underlying MDP is known. However, this requirement can hardly be satisfied in practice, as learning a model of a dynamical system with a large, or continuous, state space is a challenging task. In this paper, we propose a model-free IRL algorithm, where the relative entropy between the empirical distribution of the trajectories under a uniform policy and their distribution under the learned policy is minimized by stochastic gradient descent. We compare this new approach to well-known IRL algorithms using approximate MDP models. Empirical results on simulated car racing, gridworld and ball-in-a-cup problems show that our approach is able to learn good policies from a small number of demonstrations.},
	Author = {Boularias, Abdeslam and Kober, Jens and Peters, Jan},
	Booktitle = {Proceedings of Fourteenth International Conference on Artificial Intelligence and Statistics},
	Keywords = {learning,statistics {\&} optimisation},
	Pages = {182--189},
	Title = {{Model-free inverse reinforcement learning}},
	Url = {http://eprints.pascal-network.org/archive/00008041/},
	Year = {2011},
	Bdsk-Url-1 = {http://eprints.pascal-network.org/archive/00008041/}}

@inproceedings{zhu2005harmonic,
	Address = {New York, NY, USA},
	Author = {Zhu, Xiaojin and Lafferty, John},
	Booktitle = {Proceedings of the 22nd international conference on Machine learning},
	Doi = {http://doi.acm.org/10.1145/1102351.1102484},
	Isbn = {1-59593-180-5},
	Pages = {1052--1059},
	Publisher = {ACM},
	Series = {ICML '05},
	Title = {{Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning}},
	Url = {http://doi.acm.org/10.1145/1102351.1102484},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1102351.1102484}}

@article{evans2013static,
	Author = {Evans, Thomas P Ol{\'{e}}ron and Bishop, Steven R},
	Journal = {European Journal of Operational Research},
	Number = {3},
	Pages = {667--689},
	Title = {{Static search games played over graphs and general metric spaces}},
	Url = {http://discovery.ucl.ac.uk/1414598/1/Evans{\_}European Journal of Operational Research.pdf},
	Volume = {231},
	Year = {2013},
	Bdsk-Url-1 = {http://discovery.ucl.ac.uk/1414598/1/Evans%7B%5C_%7DEuropean%20Journal%20of%20Operational%20Research.pdf}}

@article{hunter2004tutorial,
	Author = {Hunter, D R and Lange, K},
	Journal = {The American Statistician},
	Number = {1},
	Pages = {30--37},
	Publisher = {ASA},
	Title = {{A tutorial on {\{}MM{\}} algorithms}},
	Volume = {58},
	Year = {2004}}

@inproceedings{wagstaff2000clustering,
	Author = {Wagstaff, K and Cardie, C},
	Booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	Pages = {1103--1110},
	Title = {{Clustering with instance-level constraints}},
	Url = {citeseer.ist.psu.edu/wagstaff00clustering.html},
	Year = {2000},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/wagstaff00clustering.html}}

@techreport{BN99,
	Author = {Ben-Tal, A and Nemirovski, A},
	Institution = {MINERVA Optimization Center Report, Faculty of Industrial Engineering and Management, Technion---Israel Institute of Technology, Haifa},
	Title = {{The conjugate barrier mirror descent method for non-smooth convex optimization}},
	Year = {1999}}

@inproceedings{schuurmans2002direct,
	Author = {Schuurmans, Dale and Patrascu, Relu},
	Booktitle = {Advances in Neural Information Processing Systems 14},
	Pages = {1579--1586},
	Title = {{Direct Value-Approximation for Factored {\{}MDPs{\}}}},
	Year = {2002}}

@incollection{choi2012nonparametric,
	Author = {Choi, Jaedeug and Kim, Kee-Eung},
	Booktitle = {Advances in Neural Information Processing Systems 25},
	Editor = {Bartlett, P and Pereira, F C N and Burges, C J C and Bottou, L and Weinberger, K Q},
	Pages = {314--322},
	Title = {{Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions}},
	Url = {http://books.nips.cc/papers/files/nips25/NIPS2012{\_}0159.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips25/NIPS2012%7B%5C_%7D0159.pdf}}

@article{klopp2012noisy,
	Abstract = {In the present paper, we consider the problem of matrix completion with noise. Unlike previous works, we consider quite general sampling distribution and we do not need to know or to estimate the variance of the noise. Two new nuclear-norm penalized estimators are proposed, one of them of "square-root" type. We analyse their performance under high-dimensional scaling and provide non-asymptotic bounds on the Frobenius norm error. Up to a logarithmic factor, these performance guarantees are minimax optimal in a number of circumstances.},
	Author = {Klopp, Olga},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Klopp - 2012 - Noisy low-rank matrix completion with general sampling distribution.pdf:pdf},
	Journal = {Bernoulli},
	Title = {{Noisy low-rank matrix completion with general sampling distribution}},
	Year = {2014}}

@inproceedings{stobbe,
	Author = {Stobbe, P and Krause, A},
	Booktitle = {Adv. NIPS},
	Title = {{Efficient Minimization of Decomposable Submodular Functions}},
	Year = {2010}}

@article{johnson98pcfg,
	Author = {Johnson, Mark},
	Journal = {Computational Linguistics},
	Number = {4},
	Pages = {613--632},
	Title = {{{\{}PCFG{\}} Models of Linguistic Tree Representations}},
	Url = {citeseer.ist.psu.edu/johnson98pcfg.html},
	Volume = {24},
	Year = {1998},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/johnson98pcfg.html}}

@inproceedings{littlestone1991on-line,
	Author = {Littlestone, Nick and Long, Philip and Warmuth, Manfred},
	Booktitle = {Proceedings of the 23rd Annual ACM Symposium on Theory of Computing},
	Pages = {465--475},
	Title = {{On-Line Learning of Linear Functions}},
	Year = {1991}}

@inproceedings{herbster1995tracking,
	Author = {Herbster, Mark and Warmuth, Manfred},
	Booktitle = {Proceedings of the 12th International Conference on Machine Learning},
	Pages = {286--294},
	Title = {{Tracking the Best Expert}},
	Year = {1995}}

@inproceedings{BMSS09,
	Author = {Bubeck, S and Munos, R and Stoltz, G and Szepesvari, Cs.},
	Booktitle = {Advances in Neural Information Processing Systems (NIPS)},
	Pages = {201--208},
	Title = {{Online Optimization in $\backslash$mathcal{\{}X{\}}-Armed Bandits}},
	Year = {2009}}

@article{kelner2012spectral,
	Author = {Kelner, Jonathan A. and Levin, Alex},
	Journal = {Theory of Computing Systems},
	Number = {2},
	Pages = {243--262},
	Title = {{Spectral sparsification in the semi-streaming setting}},
	Volume = {53},
	Year = {2012}}

@article{gallo1989fast,
	Author = {Gallo, G and Grigoriadis, M D and Tarjan, R E},
	Journal = {SIAM Journal on Computing},
	Number = {1},
	Pages = {30--55},
	Title = {{A fast parametric maximum flow algorithm and applications}},
	Volume = {18},
	Year = {1989}}

@inproceedings{kanade2009sleeping,
	Author = {Kanade, Varun and McMahan, H Brendan and Bryan, Brent},
	Booktitle = {International conference on Artificial Intelligence and Statistics},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Number = {5},
	Pages = {272--279},
	Series = {AI{\&}Stats '09},
	Title = {{Sleeping Experts and Bandits with Stochastic Action Availability and Adversarial Rewards}},
	Year = {2009}}

@book{Edmonds1970,
	Author = {Edmonds, J},
	Booktitle = {Combinatorial Structures and Their Applications},
	Pages = {69--87},
	Publisher = {New York: Gordon and Breach},
	Title = {{Submodular functions, matroids, and certain polyhedra in Combinatorial Structures and Their Applications}},
	Year = {1970}}

@article{andrieu2003introduction,
	Abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
	Author = {Andrieu, Christophe and {De Freitas}, Nando and Doucet, Arnaud and Jordan, Michael I.},
	Journal = {Machine Learning},
	Keywords = {MCMC,Markov chain Monte Carlo,Sampling,Stochastic algorithms},
	Number = {1-2},
	Pages = {5--43},
	Title = {{An introduction to MCMC for machine learning}},
	Volume = {50},
	Year = {2003}}

@inproceedings{gittens2013revisiting,
	Abstract = {We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods based on leverage scores. We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projections methods. These bounds are qualitatively superior to existing bounds---e.g., improved additive-error bounds for the spectral and Frobenius norm errors and relative-error bounds for the trace norm error.},
	Archiveprefix = {arXiv},
	Arxivid = {1303.1849},
	Author = {Gittens, Alex and Mahoney, Michael W},
	Booktitle = {International Conference on Machine Learning},
	Eprint = {1303.1849},
	Keywords = {ized algorithms,kernel methods,low-rank approximation,numerical linear algebra,nystr,om approximation,random-},
	Title = {{Revisiting the Nystr{\"{o}}m method for improved large-scale machine learning}},
	Year = {2013}}

@inproceedings{Cevher2008,
	Author = {Cevher, V and Duarte, M F and Hegde, C and Baraniuk, R G},
	Booktitle = {Adv. NIPS},
	Title = {{Sparse signal recovery using {\{}M{\}}arkov random fields}},
	Year = {2008}}

@article{khachiyan1979polynomial,
	Author = {Khachiyan, Leonid},
	Journal = {Doklady Akademii Nauk SSSR},
	Pages = {1093--1096},
	Title = {{A Polynomial Algorithm in Linear Programming}},
	Volume = {244},
	Year = {1979}}

@article{tropp2015an-introduction,
	Author = {Tropp, Joel Aaron},
	Journal = {Foundations and Trends in Machine Learning},
	Number = {1-2},
	Pages = {1--230},
	Title = {{An introduction to matrix concentration inequalities}},
	Volume = {8},
	Year = {2015}}

@inproceedings{boutilier1995exploiting,
	Author = {Boutilier, Craig and Dearden, Richard and Goldszmidt, Mois{\'{e}}s},
	Booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence},
	Pages = {1104--1111},
	Title = {{Exploiting Structure in Policy Construction}},
	Year = {1995}}

@article{Audibert2009,
	Abstract = {Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations. {\textcopyright}2009 Elsevier B.V. All rights reserved.},
	Author = {Audibert, Jean Yves and Munos, R{\'{e}}mi and Szepesv{\'{a}}ri, Csaba},
	Doi = {10.1016/j.tcs.2009.01.016},
	Isbn = {0304-3975},
	Issn = {03043975},
	Journal = {Theoretical Computer Science},
	Keywords = {Bernstein's inequality,Exploration-exploitation tradeoff,High-probability bound,Multi-armed bandits,Risk analysis},
	Number = {19},
	Pages = {1876--1902},
	Title = {{Exploration-exploitation tradeoff using variance estimates in multi-armed bandits}},
	Volume = {410},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.tcs.2009.01.016}}

@inproceedings{seldin2014prediction,
	Author = {Seldin, Yevgeny and Bartlett, Peter and Crammer, Koby and Abbasi-Yadkori, Yasin},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Prediction with limited advice and multiarmed bandits with paid observations}},
	Year = {2014}}

@inproceedings{yuan2004annealed,
	Author = {Yuan, Changhe and Lu, Tsai-Ching and Druzdzel, Marek},
	Booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
	Pages = {628--635},
	Title = {{Annealed {\{}MAP{\}}}},
	Year = {2004}}

@inproceedings{crites1996improving,
	Author = {Crites, Robert and Barto, Andrew},
	Booktitle = {Advances in Neural Information Processing Systems 8},
	Pages = {1017--1023},
	Title = {{Improving Elevator Performance Using Reinforcement Learning}},
	Year = {1996}}

@article{blei2003latent,
	Address = {Cambridge, MA, USA},
	Annote = {comps{\_}models},
	Author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	Issn = {1533-7928},
	Journal = {J. Mach. Learn. Res.},
	Pages = {993--1022},
	Publisher = {MIT Press},
	Title = {{Latent dirichlet allocation}},
	Url = {http://delivery.acm.org/10.1145/950000/944937/3-993-blei.pdf?key1=944937{\&}key2=6322955221{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=8754681{\&}CFTOKEN=61191808},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {http://delivery.acm.org/10.1145/950000/944937/3-993-blei.pdf?key1=944937%7B%5C&%7Dkey2=6322955221%7B%5C&%7Dcoll=GUIDE%7B%5C&%7Ddl=GUIDE%7B%5C&%7DCFID=8754681%7B%5C&%7DCFTOKEN=61191808}}

@article{gyorfi12empirical,
	Author = {Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and Walk, Harro},
	Journal = {IEEE Transactions on Information Theory},
	Number = {10},
	Pages = {6320--6331},
	Title = {{Empirical Portfolio Selection Strategies With Proportional Transaction Costs}},
	Volume = {58},
	Year = {2012}}

@inproceedings{cevher,
	Author = {Krause, A and Cevher, V},
	Booktitle = {Proc. ICML},
	Title = {{Submodular dictionary selection for sparse representation}},
	Year = {2010}}

@inproceedings{McMaStre09,
	Author = {McMahan, H Brendan and Streeter, Matthew},
	Title = {{Tighter Bounds for Multi-Armed Bandits with Expert Advice}}}

@article{ghahramani1997factorial,
	Address = {Hingham, MA, USA},
	Annote = {comps{\_}models},
	Author = {Ghahramani, Zoubin and Jordan, Michael I},
	Issn = {0885-6125},
	Journal = {Mach. Learn.},
	Number = {2-3},
	Pages = {245--273},
	Publisher = {Kluwer Academic Publishers},
	Title = {{Factorial Hidden Markov Models}},
	Url = {http://www.springerlink.com/content/w3523227075k34t4/},
	Volume = {29},
	Year = {1997},
	Bdsk-Url-1 = {http://www.springerlink.com/content/w3523227075k34t4/}}

@article{hildebrand2014canonical,
	Abstract = {On the interior of a regular convex cone K in n-dimensional real space there exist two canonical Hessian metrics, the one generated by the logarithm of the characteristic function, and the Cheng-Yau metric. The former is associated with a self-concordant logarithmically homogeneous barrier on K, the universal barrier. It is invariant with respect to the unimodular automorphism subgroup of K and is compatible with the operation of taking product cones, but in general it does not behave well under duality. Here we introduce a barrier associated with the Cheng-Yau metric, the canonical barrier. It shares with the universal barrier the invariance, existence, and uniqueness properties and is compatible with the operation of taking product cones, but in addition is well behaved under duality. The canonical barrier can be characterized as the convex solution of the partial differential equation log det F? = 2F that tends to infinity as the argument tends to the boundary of K. Its barrier parameter does not exceed the dimension n of the cone. On homogeneous cones both barriers essentially coincide.},
	Author = {Hildebrand, Roland},
	Journal = {Mathematics of Operations Research},
	Number = {3},
	Pages = {841--850},
	Title = {{Canonical barriers on convex cones}},
	Volume = {39},
	Year = {2014}}

@book{Bar02,
	Author = {Barvinok, A},
	Publisher = {American Mathematical Society},
	Title = {{A Course in Convexity}},
	Year = {2002}}

@article{tsitsiklis1997analysis,
	Author = {Tsitsiklis, John and {Van Roy}, Benjamin},
	Journal = {IEEE Transactions on Automatic Control},
	Number = {5},
	Pages = {674--690},
	Title = {{An Analysis of Temporal-Difference Learning with Function Approximation}},
	Volume = {42},
	Year = {1997}}

@inproceedings{KP11,
	Author = {Kapralov, Michael and Panigrahy, Rina},
	Pages = {828--836},
	Title = {{Prediction strategies without loss}},
	Year = {2011}}

@article{madigan2002likelihood-based,
	Author = {Madigan, David and Raghavan, Ini and Dumouchel, William and Nason, Martha and Posse, Christian and Ridgeway, Greg},
	Journal = {Data Mining and Knowledge Discovery},
	Number = {2},
	Pages = {173--190},
	Title = {{Likelihood-based data squashing: a modeling approach to instance construction}},
	Volume = {6},
	Year = {2002}}

@inproceedings{lee2003video-based,
	Author = {Lee, Kuang-Chih and Ho, Jeffrey and Yang, Ming-Hsuan and Kriegman, David},
	Booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	Pages = {313--320},
	Title = {{Video-Based Face Recognition Using Probabilistic Appearance Manifolds}},
	Year = {2003}}

@inproceedings{GGLB11,
	Author = {Gabillon, V and Ghavamzadeh, M and Lazaric, A and Bubeck, S},
	Booktitle = {Neural Information Processing Systems (NIPS)},
	Title = {{Multi-Bandit Best Arm Identification}},
	Year = {2011}}

@article{KW52,
	Author = {Kiefer, J and Wolfowitz, J},
	Journal = {Annals of Mathematical Statistics},
	Pages = {462--466},
	Title = {{Stochastic estimation of the maximum of a regression function}},
	Volume = {23},
	Year = {1952}}

@inproceedings{nodelman2002continuous,
	Author = {Nodelman, Uri and Shelton, Christian and Koller, Daphne},
	Booktitle = {Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence},
	Pages = {378--387},
	Title = {{Continuous Time {\{}Bayesian{\}} Networks}},
	Year = {2002}}

@article{wolsey,
	Author = {Wolsey, Laurence A},
	Journal = {Mathematics of Operations Research},
	Number = {3},
	Pages = {pp. 410--425},
	Publisher = {INFORMS},
	Title = {{Maximising Real-Valued Submodular Functions: Primal and Dual Heuristics for Location Problems}},
	Volume = {7},
	Year = {1982}}

@inproceedings{lafferty2001conditional,
	Address = {San Francisco, CA, USA},
	Annote = {comps{\_}models},
	Author = {Lafferty, John D and McCallum, Andrew and Pereira, Fernando C N},
	Booktitle = {ICML '01: Proceedings of the Eighteenth International Conference on Machine Learning},
	Isbn = {1-55860-778-1},
	Pages = {282--289},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data}},
	Url = {http://www.cis.upenn.edu/{~}pereira/papers/crf.pdf},
	Year = {2001},
	Bdsk-Url-1 = {http://www.cis.upenn.edu/%7B~%7Dpereira/papers/crf.pdf}}

@inproceedings{das2011submodular,
	Author = {Das, A and Kempe, D},
	Booktitle = {Proc. ICML},
	Title = {{Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection}},
	Year = {2011}}

@article{sutton1988learning,
	Author = {Sutton, Richard},
	Journal = {Machine Learning},
	Pages = {9--44},
	Title = {{Learning to Predict by the Methods of Temporal Differences}},
	Volume = {3},
	Year = {1988}}

@article{mairal2010online,
	Author = {Mairal, J and Bach, F and Ponce, J and Sapiro, G},
	Issn = {1532-4435},
	Journal = {Journal of Machine Learning Research},
	Pages = {19--60},
	Publisher = {MIT Press},
	Title = {{Online learning for matrix factorization and sparse coding}},
	Volume = {11},
	Year = {2010}}

@inproceedings{koller2000policy,
	Author = {Koller, Daphne and Parr, Ronald},
	Booktitle = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
	Pages = {326--334},
	Title = {{Policy Iteration for Factored {\{}MDPs{\}}}},
	Year = {2000}}

@inproceedings{Taskar+al:EMNLP04,
	Author = {Taskar, B and Klein, D and Collins, M and Koller, D and Manning, C},
	Booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	Pages = {1--8},
	Title = {{Max-Margin Parsing}},
	Year = {2004}}

@book{boucheron13concentration,
	Author = {Boucheron, St{\'{e}}phane and Lugosi, G{\'{a}}bor and Massart, Pascal},
	Publisher = {Oxford University Press},
	Title = {{Concentration inequalities}},
	Year = {2013}}

@inproceedings{rudi2015less,
	Abstract = {We study Nystr$\backslash$"om type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystr$\backslash$"om Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets.},
	Author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Less is more: Nystr{\"{o}}m computational regularization}},
	Year = {2015}}

@phdthesis{gordon1999approximate,
	Author = {Gordon, Geoffrey},
	School = {Carnegie Mellon University},
	Title = {{Approximate Solutions to {\{}Markov{\}} Decision Processes}},
	Year = {1999}}

@inproceedings{ng2001spectral,
	Abstract = {Despite many empirical successes of spectral clustering methods| algorithms that cluster points using eigenvectors of matrices derived from the data|there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly dierent ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems. 1},
	Author = {Ng, Andrew Y and Jordan, Michael I and Weiss, Yair},
	Booktitle = {Neural Information Processing Systems},
	Keywords = {clustering community detection graph spectral theo},
	Title = {{On spectral clustering: Analysis and an algorithm}},
	Year = {2001}}

@article{rosset,
	Author = {Rosset, S and Zhu, J},
	Journal = {Ann. Statist.},
	Number = {3},
	Pages = {1012--1030},
	Title = {{Piecewise linear regularized solution paths}},
	Volume = {35},
	Year = {2007}}

@article{Ratliff2007,
	Abstract = {Decision making in robotics often involves computing an optimal action for a given state, where the space of actions under consideration can potentially be large and state dependent. Many of these decision making problems can be naturally formalized in the multiclass classification framework, where actions are regarded as labels for states. One powerful approach to multiclass classification relies on learning a function that scores each action; action selection is done by returning the action with maximum score. In this work, we focus on two imitation learning problems in particular that arise in robotics. The first problem is footstep prediction for quadruped locomotion, in which the system predicts next footstep locations greedily given the current four-foot configuration of the robot over a terrain height map. The second problem is grasp prediction, in which the system must predict good grasps of complex free-form objects given an approach direction for a robotic hand. We present experimental results of applying a recently developed functional gradient technique for optimizing a structured margin formulation of the corresponding large non-linear multiclass classification problems.},
	Author = {Ratliff, Nathan and Bagnell, J Andrew and Srinivasa, Siddhartha S},
	Doi = {10.1109/ICHR.2007.4813899},
	Institution = {Robotics Institute},
	Isbn = {9781424418619},
	Journal = {2007 7th IEEERAS International Conference on Humanoid Robots},
	Pages = {392--397},
	Publisher = {Ieee},
	Title = {{Imitation learning for locomotion and manipulation}},
	Url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4813899},
	Year = {2007},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4813899},
	Bdsk-Url-2 = {https://doi.org/10.1109/ICHR.2007.4813899}}

@book{Bertsekas,
	Author = {Bertsekas, D},
	Publisher = {Athena Scientific},
	Title = {{Nonlinear programming}},
	Year = {1995}}

@inproceedings{rousseau2013graph,
	Author = {Rousseau, Fran{\c{c}}ois and Vazirgiannis, Michalis},
	Booktitle = {Proceedings of the 22nd ACM international conference on Conference on information {\&} knowledge management},
	Organization = {ACM},
	Pages = {59--68},
	Title = {{Graph-of-word and TW-IDF: new approach to ad hoc IR}},
	Year = {2013}}

@article{gans2005medical,
	Abstract = {We surveyed a nationally representative sample of medical group practices
to assess their current use of information technology (IT). Our results
suggest that adoption of electronic health records (EHRs) is progressing
slowly, at least in smaller practices, although a number of group
practices plan to implement an EHR within the next two years. Moreover,
the process of choosing and implementing an EHR appears to be more
complex and varied than we expected. This suggests a need for greater
support for practices, particularly smaller ones, in this quest if
the benefits expected from EHRs are to be realized.},
	Author = {Gans, David and Kralewski, John and Hammons, Terry and Dowd, Bryan},
	Doi = {10.1377/hlthaff.24.5.1323},
	Institution = {Medical Group Management Association, Englewood, Colorado, USA. dng@mgma.com},
	Journal = {Health Aff (Millwood)},
	Keywords = {Computerized,Data Collection; Diffusion of Innovation; Informat,Medical; United States,utilization; Medical Records Systems,utilization; Practice Management},
	Number = {5},
	Pages = {1323--1333},
	Pmid = {16162580},
	Title = {{Medical groups' adoption of electronic health records and information systems.}},
	Url = {http://dx.doi.org/10.1377/hlthaff.24.5.1323},
	Volume = {24},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1377/hlthaff.24.5.1323}}

@inproceedings{catoni2012challenging,
	Author = {Catoni, Olivier},
	Booktitle = {Annales de l'Institut Henri Poincar{\'{e}}, Probabilit{\'{e}}s et Statistiques},
	Number = {4},
	Pages = {1148--1185},
	Title = {{Challenging the empirical mean and empirical variance: A deviation study}},
	Volume = {48},
	Year = {2012}}

@article{iwata2009simple,
	Abstract = {This paper presents a new simple algorithm for minimizing submodular functions. For integer valued submodular functions, the algorithm runs in O(n6EO log nM) time, where n is the cardinality of the ground set, M is the maximum absolute value of the function value, and EO is the time for function evaluation. The algorithm can be improved to run in O ((n4EO+n5)log nM) time. The strongly polynomial version of this faster algorithm runs in O((n5EO + n6) log n) time for real valued general submodular functions. These are comparable to the best known running time bounds for submodular function minimization. The algorithm can also be implemented in strongly polynomial time using only additions, subtractions, comparisons, and the oracle calls for function evaluation. This is the first fully combinatorial submodular function minimization algorithm that does not rely on the scaling method.},
	Author = {Iwata, Satoru and Orlin, James B},
	Doi = {10.1006/jctb.2001.2072},
	Issn = {00958956},
	Journal = {Journal of Combinatorial Theory Series B},
	Number = {2},
	Pages = {1230--1237},
	Publisher = {Society for Industrial and Applied Mathematics},
	Title = {{A simple combinatorial algorithm for submodular function minimization}},
	Url = {http://portal.acm.org/citation.cfm?id=1496770.1496903},
	Volume = {84},
	Year = {2009},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?id=1496770.1496903},
	Bdsk-Url-2 = {https://doi.org/10.1006/jctb.2001.2072}}

@inproceedings{Jenatton2010a,
	Author = {Jenatton, R and Mairal, J and Obozinski, G and Bach, F},
	Booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
	Title = {{Proximal Methods for Sparse Hierarchical Dictionary Learning}},
	Year = {2010}}

@book{good1994permutation,
	Author = {Good, P},
	Publisher = {Springer-Verlag},
	Title = {{Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypothesis}},
	Year = {1994}}

@inproceedings{guestrin2001max-norm,
	Author = {Guestrin, Carlos and Koller, Daphne and Parr, Ronald},
	Booktitle = {Proceedings of the 17th International Joint Conference on Artificial Intelligence},
	Pages = {673--682},
	Title = {{Max-Norm Projections for Factored {\{}MDPs{\}}}},
	Year = {2001}}

@inproceedings{gao2006novel,
	Annote = {comps{\_}anX},
	Author = {Gao, Jing and Cheng, Haibin and Tan, Pang-Ning},
	Booktitle = {SDM},
	Title = {{A Novel Framework for Incorporating Labeled Examples into Anomaly Detection.}},
	Url = {http://www.siam.org/meetings/sdm06/proceedings/068gaoj2.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://www.siam.org/meetings/sdm06/proceedings/068gaoj2.pdf}}

@inproceedings{cesa-bianchi2016delay,
	Abstract = {We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than {\$}d{\$} hops to arrive, where {\$}d{\$} is a delay parameter. We introduce $\backslash$textsc{\{}Exp3-Coop{\}}, a cooperative version of the {\{}$\backslash$sc Exp3{\}} algorithm and prove that with {\$}K{\$} actions and {\$}N{\$} agents the average per-agent regret after {\$}T{\$} rounds is at most of order {\$}\backslashsqrt{\{}\backslashbigl(d+1 + \backslashtfrac{\{}K{\}}{\{}N{\}}\backslashalpha{\_}{\{}\backslashle d{\}}\backslashbigr)(T\backslashln K){\}}{\$}, where {\$}\backslashalpha{\_}{\{}\backslashle d{\}}{\$} is the independence number of the {\$}d{\$}-th power of the connected communication graph {\$}G{\$}. We then show that for any connected graph, for {\$}d=\backslashsqrt{\{}K{\}}{\$} the regret bound is {\$}K{\^{}}{\{}1/4{\}}\backslashsqrt{\{}T{\}}{\$}, strictly better than the minimax regret {\$}\backslashsqrt{\{}KT{\}}{\$} for noncooperating agents. More informed choices of {\$}d{\$} lead to bounds which are arbitrarily close to the full information minimax regret {\$}\backslashsqrt{\{}T\backslashln K{\}}{\$} when {\$}G{\$} is dense. When {\$}G{\$} has sparse components, we show that a variant of $\backslash$textsc{\{}Exp3-Coop{\}}, allowing agents to choose their parameters according to their centrality in {\$}G{\$}, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.},
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio and Mansour, Yishay and Minora, Alberto},
	Booktitle = {Conference on Learning Theory},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Cesa-Bianchi et al. - 2016 - Delay and cooperation in nonstochastic bandits.pdf:pdf},
	Title = {{Delay and cooperation in nonstochastic bandits}},
	Year = {2016}}

@inproceedings{geulen10buffering,
	Author = {Geulen, S and Voecking, B and Winkler, M},
	Title = {{Regret Minimization for Online Buffering Problems Using the Weighted Majority Algorithm}}}

@article{carpentier2017adaptive,
	Abstract = {In the present paper we study the problem of existence of honest and adaptive confidence sets for matrix completion. We consider two statistical models: the trace regression model and the Bernoulli model. In the trace regression model, we show that honest confidence sets that adapt to the unknown rank of the matrix exist even when the error variance is unknown. Contrary to this, we prove that in the Bernoulli model, honest and adaptive confidence sets exist only when the error variance is known a priori. In the course of our proofs we obtain bounds for the minimax rates of certain composite hypothesis testing problems arising in low rank inference.},
	Archiveprefix = {arXiv},
	Arxivid = {1608.04861},
	Author = {Carpentier, Alexandra and Klopp, Olga and L{\"{o}}ffler, Matthias and Nickl, Richard},
	Eprint = {1608.04861},
	Journal = {Bernoulli},
	Title = {{Adaptive confidence sets for matrix completion}},
	Year = {2017}}

@inproceedings{MB04,
	Author = {McMahan, H Brendan and Blum, Avrim},
	Booktitle = {In Proceedings of the 17th Annual Conference on Learning Theory (COLT)},
	Pages = {109--123},
	Title = {{Online geometric optimization in the bandit setting against an adaptive adversary}},
	Year = {2004}}

@phdthesis{ortiz2002selecting,
	Author = {Ortiz, Luis},
	School = {Brown University},
	Title = {{Selecting Approximately-Optimal Actions in Complex Structured Domains}},
	Year = {2002}}

@article{bellman1963polynomial,
	Author = {Bellman, Richard and Kalaba, Robert and Kotkin, Bella},
	Journal = {Mathematics of Computation},
	Number = {82},
	Pages = {155--161},
	Title = {{Polynomial Approximation -- A New Computational Technique in Dynamic Programming: Allocation Processes}},
	Volume = {17},
	Year = {1963}}

@inproceedings{rubin2005auctioning,
	Address = {New York, NY, USA},
	Author = {Rubin, Shai and Christodorescu, Mihai and Ganapathy, Vinod and Giffin, Jonathon T and Kruger, Louis and Wang, Hao and Kidd, Nicholas},
	Booktitle = {Proceedings of the 12th ACM conference on Computer and communications security},
	Isbn = {1-59593-226-7},
	Keywords = {anomaly detection,auction,reputation system,shilling},
	Pages = {270--279},
	Publisher = {ACM},
	Series = {CCS '05},
	Title = {{An auctioning reputation system based on anomaly detection}},
	Year = {2005}}

@inproceedings{park2003solving,
	Author = {Park, James and Darwiche, Adnan},
	Booktitle = {Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence},
	Pages = {459--468},
	Title = {{Solving {\{}MAP{\}} Exactly Using Systematic Search}},
	Year = {2003}}

@incollection{Bal97,
	Author = {Ball, K},
	Booktitle = {Flavors of Geometry},
	Editor = {Levy, S},
	Pages = {1--58},
	Publisher = {Cambridge University Press},
	Title = {{An Elementary Introduction to Modern Convex Geometry}},
	Year = {1997}}

@article{bull2011convergence,
	Author = {Bull, Adam},
	Journal = {The Journal of Machine Learning Research},
	Pages = {2879--2904},
	Publisher = {JMLR. org},
	Title = {{Convergence rates of efficient global optimization algorithms}},
	Volume = {12},
	Year = {2011}}

@book{stone1976theory,
	Author = {Stone, Lawrence D},
	Publisher = {Elsevier},
	Title = {{Theory of optimal search}},
	Url = {https://books.google.fr/books?id=DFLpiYM9cg8C},
	Year = {1976},
	Bdsk-Url-1 = {https://books.google.fr/books?id=DFLpiYM9cg8C}}

@inproceedings{babenko2009visual,
	Author = {Babenko, Boris and Yang, Ming-Hsuan and Belongie, Serge},
	Booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	Title = {{Visual Tracking with Online Multiple Instance Learning}},
	Year = {2009}}

@article{gilks1992adaptive,
	Abstract = {We propose a method for rejection sampling from any univariate log-concave probability density function. The method is adaptive: as sampling proceeds, the rejection envelope and the squeezing function converge to the density function. The rejection envelope and squeezing function are piece-wise exponential functions, the rejection envelope touching the density at previously sampled points, and the squeezing function forming arcs between those points of contact. The technique is intended for situations where evaluation of the density is computationally expensive, in particular for applications of Gibbs sampling to Bayesian models with non-conjugacy. We apply the technique to a Gibbs sampling analysis of monoclonal antibody reactivity.},
	Author = {Gilks, W. R. and Wild, P.},
	Journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	Keywords = {adaptive rejection sampling,bayesian inference,density,gibbs sampling,log-concave,non-conjugate bayesian models,simulation},
	Number = {2},
	Pages = {337--348},
	Title = {{Adaptive rejection sampling for Gibbs sampling}},
	Volume = {41},
	Year = {1992}}

@article{Haasdonk2010,
	Author = {Haasdonk, Bernard and Pekalska, Elzbieta},
	Journal = {Advances in Data Analysis, Data Handling and Business Intelligence},
	Pages = {351--361},
	Publisher = {Springer},
	Title = {{Classification with kernel Mahalanobis distance classifiers}},
	Year = {2010}}

@inproceedings{hauskrecht2006approximate,
	Author = {Hauskrecht, Milos and Kveton, Branislav},
	Booktitle = {Proceedings of the 9th International Symposium on Artificial Intelligence and Mathematics},
	Pages = {114--120},
	Title = {{Approximate Linear Programming for Solving Hybrid Factored {\{}MDPs{\}}}},
	Year = {2006}}

@inproceedings{meek2003uai,
	Booktitle = {UAI},
	Editor = {Meek, Christopher and Kj{\ae}rulff, Uffe},
	Isbn = {0-127-05664-5},
	Publisher = {Morgan Kaufmann},
	Title = {{UAI '03, Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence, August 7-10 2003, Acapulco, Mexico}},
	Year = {2003}}

@book{wedel200market,
	Abstract = {Second edition. Modern marketing techniques in industrialized countries cannot be implemented without segmentation of the potential market. Goods are no longer produced and sold without a significant consideration of customer needs combined with a recognition that these needs are heterogeneous. Since first emerging in the late 1950s, the concept of segmentation has been one of the most researched topics in the marketing literature. Segmentation has become a central topic to both the theory and practice of marketing, particularly in the recent development of finite mixture models to better identify market segments. This second edition of Market Segmentation updates and extends the integrated examination of segmentation theory and methodology begun in the first edition. A chapter on mixture model analysis of paired comparison data has been added, together with a new chapter on the pros and cons of the mixture model. The book starts with a framework for considering the various bases and methods available for conducting segmentation studies. The second section contains a more detailed discussion of the methodology for market segmentation, from traditional clustering algorithms to more recent developments in finite mixtures and latent class models. Three types of finite mixture models are discussed in this second section: simple mixtures, mixtures of regressions and mixtures of unfolding models. The third main section is devoted to special topics in market segmentation such as joint segmentation, segmentation using tailored interviewing and segmentation with structural equation models. The fourth part covers four major approaches to applied market segmentation: geo-demographic, lifestyle, response-based, and conjoint analysis. The final concluding section discusses directions for further research. 1 -- Introduction -- 1 The Historical Development of the Market Segmentation Concept -- 2 Segmentation Bases -- 3 Segmentation Methods -- 4 Tools for Market Segmentation -- 2 -- Segmentation Methodology -- 5 Clustering Methods -- 6 Mixture Models -- 7 Mixture Regression Models -- 8 Mixture Unfolding Models -- 9 Profiling Segments -- 10 Dynamic Segmentation -- 3 -- Special Topics in Market Segmentation -- 11 Joint Segmentation -- 12 Market Segmentation with Tailored Interviewing -- 13 Model-Based Segmentation Using Structural Equation Models -- 14 Segmentation Based on Product Dissimilarity Judgements -- 4 -- Applied Market Segmentation -- 15 General Observable Bases: Geo-demographics -- 16 General Unobservable Bases: Values and Lifestyles -- 17 Product-specific observable Bases: Response-based Segmentation -- 18 Product-Specific Unobservable Bases: Conjoint Analysis -- 5 -- Conclusions and Directions for Future Research -- 19 Conclusions: Representations of Heterogeneity -- 20 Directions for Future Research -- References.},
	Author = {Wedel, Michel. and Kamakura, Wagner A.},
	Pages = {382},
	Publisher = {Springer US},
	Title = {{Market segmentation : Conceptual and methodological foundations}},
	Year = {2000}}

@inproceedings{hauskrecht2010conditional,
	Abstract = {We develop and evaluate a data-driven approach for detecting unusual (anomalous) patient-management actions using past patient cases stored in an electronic health record (EHR) system. Our hypothesis is that patient-management actions that are unusual with respect to past patients may be due to a potential error and that it is worthwhile to raise an alert if such a condition is encountered. We evaluate this hypothesis using data obtained from the electronic health records of 4,486 post-cardiac surgical patients. We base the evaluation on the opinions of a panel of experts. The results support that anomaly-based alerting can have reasonably low false alert rates and that stronger anomalies are correlated with higher alert rates.},
	Author = {Hauskrecht, Milos and Valko, Michal and Batal, Iyad and Clermont, Gilles and Visweswaran, Shyam and Cooper, Gregory F},
	Booktitle = {Annual American Medical Informatics Association Symposium},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Title = {{Conditional outlier detection for clinical alerting}},
	Year = {2010}}

@incollection{pelleg2005active,
	Address = {Cambridge, MA},
	Annote = {comps{\_}ano},
	Author = {Pelleg, Dan and Moore, Andrew W},
	Booktitle = {Advances in Neural Information Processing Systems 17},
	Editor = {Saul, Lawrence K and Weiss, Yair and Bottou, L{\'{e}}on},
	Pages = {1073--1080},
	Publisher = {MIT Press},
	Title = {{Active Learning for Anomaly and Rare-Category Detection}},
	Url = {http://books.nips.cc/papers/files/nips17/NIPS2004{\_}0438.pdf},
	Year = {2005},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips17/NIPS2004%7B%5C_%7D0438.pdf}}

@book{Sch03,
	Author = {Schrijver, A},
	Publisher = {Springer},
	Title = {{Combinatorial Optimization}},
	Year = {2003}}

@article{goldberg2009multi-manifold,
	Author = {Goldberg, Andrew B and Zhu, Xiaojin and Singh, Aarti and Xu, Zhiting and Nowak, Robert},
	Journal = {Journal of Machine Learning Research},
	Pages = {169--176},
	Title = {{Multi-Manifold Semi-Supervised Learning}},
	Volume = {5},
	Year = {2009}}

@inproceedings{silva:long,
	Author = {da Silva, Valdinei Freire and Costa, Anna Helena Reali and Lima, Pedro},
	Pages = {4246--4251},
	Title = {{Inverse Reinforcement Learning with Evaluation}}}

@inproceedings{ng2000,
	Author = {Ng, A Y and Russell, S},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ng, Russell - 2000 - Algorithms for inverse reinforcement learning.pdf:pdf},
	Pages = {663--670},
	Title = {{Algorithms for Inverse Reinforcement Learning}},
	Url = {citeseer.ist.psu.edu/ng00algorithms.html},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/ng00algorithms.html}}

@inproceedings{bagnell2010efficient,
	Abstract = {Imitation Learning, while applied successfully on many large real-world$\backslash$nproblems, is typically addressed as a standard supervised learning$\backslash$nproblem, where it is assumed the training and testing data are i.i.d..$\backslash$nThis is not true in imitation learning as the learned policy influences$\backslash$nthe future test inputs (states) upon which it will be tested. We$\backslash$nshow that this leads to compounding errors and a regret bound that$\backslash$ngrows quadratically in the time horizon of the task. We propose two$\backslash$nalternative algorithms for imitation learning where training occurs$\backslash$nover several episodes of interaction. These two approaches share$\backslash$nin common that the learner's policy is slowly modified from executing$\backslash$nthe expert's policy to the learned policy. We show that this leads$\backslash$nto stronger performance guarantees and demonstrate the improved performance$\backslash$non two challenging problems: training a learner to play 1) a 3D racing$\backslash$ngame (Super Tux Kart) and 2) Mario Bros.; given input images from$\backslash$nthe games and corresponding actions taken by a human expert and near-optimal$\backslash$nplanner respectively.},
	Author = {Bagnell, J Andrew and Ross, St{\'{e}}phane},
	Booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010},
	Pages = {661--668},
	Title = {{Efficient Reductions for Imitation Learning}},
	Volume = {9},
	Year = {2010}}

@inproceedings{piot2013learning,
	Abstract = {This paper provides a comparative study between Inverse Reinforcement Learning (IRL) and Apprenticeship Learning (AL). IRL and AL are two frameworks, using Markov Decision Processes (MDP), which are used for the imitation learning problem where an agent tries to learn from demonstrations of an expert. In the AL Framework, the agent tries to learn the expert policy whereas in the IRL Framework, the agent tries to learn a reward which can explain the behavior of the expert. This reward is then optimized to imitate the expert. One can wonder if it is worth estimating such a reward, or if estimating a Policy is sufficient. This quite natural question has not really been addressed in the literature right now. We provide partial answers, both from a theoretical and empirical point of view.},
	Address = {Prague (Czech Republic)},
	Author = {PIOT, Bilal and Geist, Matthieu and Pietquin, Olivier},
	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD 2013)},
	Doi = {10.1007/978-3-642-40988-2_2},
	Editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and Zelezny, Filip},
	Isbn = {978-3-642-40987-5},
	Month = {sep},
	Pages = {17--32},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {{Learning from demonstrations: Is it worth estimating a reward function?}},
	Url = {http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/384.pdf},
	Volume = {8188},
	Year = {2013},
	Bdsk-Url-1 = {http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/384.pdf},
	Bdsk-Url-2 = {https://doi.org/10.1007/978-3-642-40988-2_2}}

@inproceedings{abe2006outlier,
	Author = {Abe, Naoki and Zadrozny, Bianca and Langford, John},
	Booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	Doi = {10.1145/1150402.1150459},
	Isbn = {1-59593-339-5},
	Keywords = {active learning,ensemble method,outlier detection},
	Pages = {504--509},
	Title = {{Outlier Detection by Active Learning}},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1145/1150402.1150459}}

@inproceedings{park2001approximating,
	Author = {Park, James and Darwiche, Adnan},
	Booktitle = {Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence},
	Pages = {403--410},
	Title = {{Approximating {\{}MAP{\}} Using Local Search}},
	Year = {2001}}

@article{zhang2015divide,
	Abstract = {We establish optimal convergence rates for a decomposition-based scalable approach to kernel ridge regression. The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as m is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of N samples. As concrete examples, our theory guarantees that the number of processors m may grow nearly linearly for finite-rank kernels and Gaussian kernels and polynomially in N for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach.},
	Author = {Zhang, Yuchen and Duchi, John C. and Wainwright, Martin J.},
	Journal = {Journal Machine Learning Research},
	Pages = {3299--3340},
	Title = {{Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates}},
	Volume = {16},
	Year = {2015}}

@article{golovin2011adaptive,
	Author = {{Daniel Golovin} and {Andreas Krause}},
	Journal = {Journal of Artificial Intelligence Research (JAIR)},
	Pages = {427--486},
	Title = {{Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization}},
	Volume = {42},
	Year = {2011}}

@article{ghahramani2000variational,
	Address = {Cambridge, MA, USA},
	Annote = {c{\_}omps{\_}models},
	Author = {Ghahramani, Zoubin and Hinton, Geoffrey E},
	Doi = {http://dx.doi.org/10.1162/089976600300015619},
	Issn = {0899-7667},
	Journal = {Neural Comput.},
	Number = {4},
	Pages = {831--864},
	Publisher = {MIT Press},
	Title = {{Variational Learning for Switching State-Space Models}},
	Volume = {12},
	Year = {2000},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/089976600300015619}}

@techreport{Nesterov2007,
	Author = {Nesterov, Y},
	Institution = {Center for Operations Research and Econometrics (CORE), Catholic University of Louvain},
	Title = {{Gradient methods for minimizing composite objective function}},
	Year = {2007}}

@article{spielman_nearly_2014,
	Author = {Spielman, Daniel A and Teng, Shang-Hua},
	Journal = {SIAM Journal on Matrix Analysis and Applications},
	Number = {3},
	Pages = {835--885},
	Title = {{Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems}},
	Url = {http://epubs.siam.org/doi/abs/10.1137/090771430},
	Volume = {35},
	Year = {2014},
	Bdsk-Url-1 = {http://epubs.siam.org/doi/abs/10.1137/090771430}}

@inproceedings{jensen1994from,
	Author = {Jensen, Frank and Jensen, Finn and Dittmer, S{\o}ren},
	Booktitle = {Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence},
	Pages = {367--373},
	Title = {{From Influence Diagrams to Junction Trees}},
	Year = {1994}}

@phdthesis{bubeck2010bandits,
	Author = {Bubeck, S{\'{e}}bastien},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	School = {Universit{\{}{\'{e}}{\}} de Lille 1},
	Title = {{Bandits Games and Clustering Foundations}},
	Year = {2010}}

@book{horn1990matrix,
	Abstract = {Linear algebra and matrix theory have long been fundamental tools in mathematical disciplines as well as fertile fields for research. In this book the authors present classical and recent results of matrix analysis that have proved to be important to applied mathematics. Facts about matrices, beyond those found in an elementary linear algebra course, are needed to understand virtually any area of mathematical science, but the necessary material has appeared only sporadically in the literature and in university curricula. As interest in applied mathematics has grown, the need for a text and reference offering a broad selection of topics in matrix theory has become apparent, and this book meets that need. This volume reflects two concurrent views of matrix analysis. First, it encompasses topics in linear algebra that have arisen out of the needs of mathematical analysis. Second, it is an approach to real and complex linear algebraic problems that does not hesitate to use notions from analysis. Review and miscellanea -- Eigenvalues, eigenvectors, and similarity.},
	Author = {Horn, Roger A. and Johnson, Charles R.},
	Publisher = {Cambridge University Press},
	Title = {{Matrix analysis}},
	Year = {1990}}

@article{rust1997using,
	Author = {Rust, John},
	Journal = {Econometrica},
	Number = {3},
	Pages = {487--516},
	Title = {{Using Randomization to Break the Curse of Dimensionality}},
	Volume = {65},
	Year = {1997}}

@article{Kar2011,
	Abstract = {This paper studies the multi-agent bandit problem in a distributed networked setting. The setting considered assumes only one bandit (the major bandit) has accessible reward information from its samples, whereas the rest (the minor bandits) have unobservable rewards. Under the assumption that the minor bandits are aware of the sampling pattern of the major bandit (but with no direct access to its rewards), a lower bound on the expected average network regret is obtained. The lower bound resembles the logarithmic optimal regret attained in single (classical) bandit problems, but in addition is shown to scale down with the number of agents. A collaborative and adaptive distributed allocation rule DA is proposed and is shown to achieve the lower bound on the expected average regret for a connected inter-bandit communication network. In particular, it is shown that under the DA allocation rule, the minor bandits attain sub-logarithmic expected regrets as opposed to logarithmic in the single agent setting.},
	Author = {Kar, Soummya and Poor, H. Vincent and Cui, Shuguang},
	Doi = {10.1109/CDC.2011.6160719},
	Isbn = {9781612848006},
	Issn = {01912216},
	Journal = {Proceedings of the IEEE Conference on Decision and Control},
	Keywords = {Asymptotically Efficient,Distributed Allocation Rules,Networked Bandit Problems,Partially Observable Rewards},
	Pages = {1771--1778},
	Title = {{Bandit problems in networks: Asymptotically efficient distributed allocation rules}},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/CDC.2011.6160719}}

@inproceedings{NeuTDK2006,
	Author = {Neu, Gergely},
	Booktitle = {BME-VIK TDK'06},
	Title = {{Inverse Reinforcement Learning via the Method of Natural Gradients}},
	Year = {2006}}

@inproceedings{guestrin2002context,
	Author = {Guestrin, Carlos and Venkataraman, Shobha and Koller, Daphne},
	Booktitle = {Proceedings of the 18th National Conference on Artificial Intelligence},
	Pages = {253--259},
	Title = {{Context Specific Multiagent Coordination and Planning with Factored {\{}MDPs{\}}}},
	Year = {2002}}

@article{Komiyama2015,
	Abstract = {We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS for binary rewards has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance.},
	Archiveprefix = {arXiv},
	Arxivid = {1506.00779},
	Author = {Komiyama, Junpei and Honda, Junya and Nakagawa, Hiroshi},
	Eprint = {1506.00779},
	Journal = {International Conference on Machine Learning},
	Title = {{Optimal regret analysis of Thompson sampling in stochastic multi-armed bandit problem with multiple plays}},
	Url = {http://arxiv.org/abs/1506.00779},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1506.00779}}

@inproceedings{Jiang-2004-Mislabeled,
	Author = {Jiang, Y and Zhou, Z.-H.},
	Booktitle = {Lecture Notes in Computer Science 3173},
	Pages = {356--361},
	Title = {{Editing Training Data for kNN Classifiers with Neural Network Ensemble.}},
	Year = {2004}}

@misc{berkeley-parser,
	Annote = {http://nlp.cs.berkeley.edu/Main.html{\#}Parsing},
	Author = {Petrov, Slav},
	Publisher = {University of California},
	Title = {{Berkeley Parser}},
	Year = {2007}}

@unpublished{Rak09,
	Author = {Rakhlin, A},
	Title = {{Lecture Notes on Online Learning}},
	Year = {2009}}

@inproceedings{kveton2006learning,
	Author = {Kveton, Branislav and Hauskrecht, Milos},
	Booktitle = {Proceedings of the 21st National Conference on Artificial Intelligence},
	Pages = {1161--1166},
	Title = {{Learning Basis Functions in Hybrid Domains}},
	Year = {2006}}

@inproceedings{szepes06learning,
	Author = {Antos, A and Szepesv{\'{a}}ri, $\backslash$textCs and Munos, R},
	Doi = {10.1007/11776420_42},
	Pages = {574--588},
	Title = {{Learning near-optimal policies with {\{}B{\}}ellman-residual minimization based fitted policy iteration and a single sample path}},
	Url = {http://www.springerlink.com/content/x85884360133802l/?p=68d26700c320427caf7c027981b37b8f{\&}pi=41},
	Bdsk-Url-1 = {http://www.springerlink.com/content/x85884360133802l/?p=68d26700c320427caf7c027981b37b8f%7B%5C&%7Dpi=41},
	Bdsk-Url-2 = {https://doi.org/10.1007/11776420_42}}

@inproceedings{abbeel2004apprenticeship,
	Author = {Abbeel, Pieter and Ng, Andrew},
	Booktitle = {Proceedings of the 21st international conference on machine learning},
	Doi = {http://doi.acm.org/10.1145/1015330.1015430},
	Isbn = {1-58113-838-5},
	Keywords = {irl},
	Mendeley-Tags = {irl},
	Title = {{Apprenticeship learning via inverse reinforcement learning}},
	Url = {http://www.eecs.harvard.edu/{~}parkes/cs286r/spring06/papers/abeelng.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.eecs.harvard.edu/%7B~%7Dparkes/cs286r/spring06/papers/abeelng.pdf},
	Bdsk-Url-2 = {http://doi.acm.org/10.1145/1015330.1015430}}

@article{zhang1998characterization,
	Author = {Zhang, Z and Yeung, R W},
	Journal = {IEEE Transactions on Information Theory},
	Number = {4},
	Pages = {1440--1452},
	Publisher = {IEEE},
	Title = {{On characterization of entropy function via information inequalities}},
	Volume = {44},
	Year = {1998}}

@inproceedings{ng2000pegasus:,
	Author = {Ng, Andrew and Jordan, Michael},
	Booktitle = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
	Pages = {406--415},
	Title = {{{\{}PEGASUS{\}}: A Policy Search Method for Large {\{}MDPs{\}} and {\{}POMDPs{\}}}},
	Year = {2000}}

@article{Zhang2017,
	Abstract = {We study a mini-batch diversification scheme for stochastic gradient descent (SGD). While classical SGD relies on uniformly sampling data points to form a mini-batch, we propose a non-uniform sampling scheme based on the Determinantal Point Process (DPP). The DPP relies on a similarity measure between data points and gives low probabilities to mini-batches which contain redundant data, and higher probabilities to mini-batches with more diverse data. This simultaneously balances the data and leads to stochastic gradients with lower variance. We term this approach Diversified Mini-Batch SGD (DM-SGD). We show that regular SGD and a biased version of stratified sampling emerge as special cases. Furthermore, DM-SGD generalizes stratified sampling to cases where no discrete features exist to bin the data into groups. We show experimentally that our method results more interpretable and diverse features in unsupervised setups, and in better classification accuracies in supervised setups.},
	Archiveprefix = {arXiv},
	Arxivid = {1705.00607},
	Author = {Zhang, Cheng and Kjellstrom, Hedvig and Mandt, Stephan},
	Eprint = {1705.00607},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Kjellstrom, Mandt - 2017 - Determinantal Point Processes for Mini-Batch Diversification.pdf:pdf},
	Month = {may},
	Title = {{Determinantal Point Processes for Mini-Batch Diversification}},
	Url = {http://arxiv.org/abs/1705.00607},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.00607}}

@phdthesis{das2009detecting,
	Author = {Das, Kaustav},
	School = {Carnegie Mellon University},
	Title = {{Detecting Patterns of Anomalies}},
	Url = {http://www.cs.cmu.edu/{~}kaustav/thesis/kaustav{\_}thesis.pdf},
	Year = {2009},
	Bdsk-Url-1 = {http://www.cs.cmu.edu/%7B~%7Dkaustav/thesis/kaustav%7B%5C_%7Dthesis.pdf}}

@inproceedings{devroye2013random,
	Author = {Devroye, Luc and Lugosi, G{\'{a}}bor and Neu, Gergely},
	Booktitle = {Conference on Learning Theory},
	Title = {{Prediction by random-walk perturbation}},
	Year = {2013}}

@article{hochbaum2001efficient,
	Author = {Hochbaum, D S},
	Journal = {Journal of the ACM},
	Number = {4},
	Pages = {686--701},
	Publisher = {ACM},
	Title = {{An efficient algorithm for image segmentation, Markov random fields and related problems}},
	Volume = {48},
	Year = {2001}}

@inproceedings{flaxman2005online,
	Abstract = {We consider a the general online convex optimization framework introduced by Zinkevich. In this setting, there is a sequence of convex functions. Each period, we must choose a signle point (from some feasible set) and pay a cost equal to the value of the next function on our chosen point. Zinkevich shows that, if the each function is revealed after the choice is made, then one can achieve vanishingly small regret relative the best single decision chosen in hindsight. We extend this to the bandit setting where we do not find out the entire functions but rather just their value at our chosen point. We show how to get vanishingly small regret in this setting. Our approach uses a simple approximation of the gradient that is computed from evaluating a function at a single (random) point. We show that this estimate is sufficient to mimic Zinkevich's gradient descent online analysis, with access to the gradient (only being able to evaluate the function at a single point).},
	Author = {Flaxman, Abraham D and Kalai, Adam Tauman and McMahan, H Brendan and {Brendan McMahan}, Hugh},
	Booktitle = {Proceedings of the 16th annual ACM-SIAM Symposium On Discrete Algorithms},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Flaxman, Kalai, McMahan - 2004 - Online convex optimization in the bandit setting gradient descent without a gradient.pdf:pdf},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Number = {x},
	Organization = {Carnegie Mellon University},
	Pages = {385--394},
	Publisher = {SIAM},
	Series = {SODA '05},
	Title = {{Online convex optimization in the bandit setting: gradient descent without a gradient}},
	Url = {http://arxiv.org/abs/cs/0408007},
	Year = {2005},
	Bdsk-Url-1 = {http://arxiv.org/abs/cs/0408007}}

@techreport{prisadnikov2014exploration,
	Author = {Prisadnikov, Nedyalko},
	Doi = {10.3929/ethz-a-010211630},
	Institution = {Master Thesis, ETH-Z{\"{u}}rich, Department of Computer Science},
	Title = {{Exploration-exploitation trade-offs via probabilistic matrix factorization}},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.3929/ethz-a-010211630}}

@article{hart2011identification,
	Abstract = {The purpose of this study was to provide a univariate and multivariate analysis of genomic microbial data and salivary mass-spectrometry proteomic profiles for dental caries outcomes. In order to determine potential useful biomarkers for dental caries, a multivariate classification analysis was employed to build predictive models capable of classifying microbial and salivary sample profiles with generalization performance. We used high-throughput methodologies including multiplexed microbial arrays and SELDI-TOF-MS profiling to characterize the oral flora and salivary proteome in 204 children aged 1-8 years (n = 118 caries-free, n = 86 caries-active). The population received little dental care and was deemed at high risk for childhood caries. Findings of the study indicate that models incorporating both microbial and proteomic data are superior to models of only microbial or salivary data alone. Comparison of results for the combined and independent data suggests that the combination of proteomic and microbial sources is beneficial for the classification accuracy and that combined data lead to improved predictive models for caries-active and caries-free patients. The best predictive model had a 6{\%} test error, {\textgreater}92{\%} sensitivity, and {\textgreater}95{\%} specificity. These findings suggest that further characterization of the oral microflora and the salivary proteome associated with health and caries may provide clinically useful biomarkers to better predict future caries experience.},
	Author = {Hart, Thomas C and Corby, Patricia M and Hauskrecht, Milos and {Hee Ryu}, Ok and Pelikan, Richard and Valko, Michal and Oliveira, Maria B and Hoehn, Gerald T and Bretz, Walter A},
	Doi = {10.1155/2011/196721},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Hart et al. - 2011 - Identification of microbial and proteomic biomarkers in early childhood caries.pdf:pdf},
	Institution = {Department of Periodontics, College of Dentistry, University of Illinois at Chicago, 801 S. Paulina Street, Chicago, IL 60612, USA.},
	Journal = {Int J Dent},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Pages = {196721},
	Pmid = {22013442},
	Title = {{Identification of microbial and proteomic biomarkers in early childhood caries.}},
	Url = {http://dx.doi.org/10.1155/2011/196721},
	Volume = {2011},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1155/2011/196721}}

@inproceedings{rahimi2007random,
	Abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
	Author = {Rahimi, Ali and Recht, Ben},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Random features for large-scale kernel machines}},
	Year = {2007}}

@article{bickel_lasso_dantzig,
	Author = {Bickel, P and Ritov, Y and Tsybakov, A},
	Journal = {Annals of Statistics},
	Number = {4},
	Pages = {1705--1732},
	Title = {{Simultaneous analysis of {\{}L{\}}asso and {\{}D{\}}antzig selector}},
	Volume = {37},
	Year = {2009}}

@article{Moreau1962,
	Author = {Moreau, J J},
	Journal = {C. R. Acad. Sci. Paris S{\'{e}}r. A Math.},
	Pages = {2897--2899},
	Title = {{Fonctions convexes duales et points proximaux dans un espace {\{}H{\}}ilbertien}},
	Volume = {255},
	Year = {1962}}

@manual{nvidia-cusparse,
	Title = {{NVIDIA CUSPARSE and CUBLAS Libraries}},
	Url = {http://www.nvidia.com/object/cuda{\_}develop.html},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nvidia.com/object/cuda%7B%5C_%7Ddevelop.html}}

@article{mairal2011b,
	Author = {Mairal, J and Jenatton, R and Obozinski, G and Bach, F},
	Journal = {Journal of Machine Learning Research},
	Pages = {2681--2720},
	Title = {{Convex and Network Flow Optimization for Structured Sparsity}},
	Volume = {12},
	Year = {2011}}

@book{Nocedal:1999:NO1,
	Author = {Nocedal, J and Wright, S J},
	Edition = {2nd},
	Publisher = {Springer},
	Title = {{Numerical Optimization}},
	Year = {2006}}

@article{narasimhan2006q,
	Author = {Narasimhan, M and Jojic, N and Bilmes, J},
	Journal = {Adv. NIPS},
	Title = {{Q-clustering}},
	Volume = {18},
	Year = {2006}}

@article{Sidney1975,
	Abstract = {A one-machine deterministic job-shop sequencing problem is considered. Associated with each job is its processing time and linear deferral cost. In addition, the jobs are related by a general precedence relation. The objective is to order the jobs so as to minimize the sum of the deferral costs, subject to the constraint that the ordering must be consistent with the precedence relation. A decomposition algorithm is presented, and it is proved that a permutation is optimal if and only if it can be generated by this algorithm. Four special network structures are then considered, and specializations of the general algorithm are presented. [ABSTRACT FROM AUTHOR]},
	Author = {Sidney, J B},
	Doi = {10.1287/opre.23.2.283},
	Issn = {0030-364X},
	Journal = {Operations Research},
	Number = {2},
	Pages = {283--298},
	Title = {{Decomposition Algorithms for Single-Machine Sequencing with Precedence Relations and Deferral Costs}},
	Url = {http://or.journal.informs.org/cgi/doi/10.1287/opre.23.2.283},
	Volume = {23},
	Year = {1975},
	Bdsk-Url-1 = {http://or.journal.informs.org/cgi/doi/10.1287/opre.23.2.283},
	Bdsk-Url-2 = {https://doi.org/10.1287/opre.23.2.283}}

@article{aha1991instance-based,
	Address = {Hingham, MA, USA},
	Author = {Aha, David W and Kibler, Dennis and Albert, Marc K},
	Doi = {http://dx.doi.org/10.1023/A:1022689900470},
	Issn = {0885-6125},
	Journal = {Mach. Learn.},
	Number = {1},
	Pages = {37--66},
	Publisher = {Kluwer Academic Publishers},
	Title = {{Instance-Based Learning Algorithms}},
	Volume = {6},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1022689900470}}

@article{nemhauser1978analysis,
	Author = {Nemhauser, G L and Wolsey, L A and Fisher, M L},
	Journal = {Mathematical Programming},
	Number = {1},
	Pages = {265--294},
	Publisher = {Springer},
	Title = {{An analysis of approximations for maximizing submodular set functions--I}},
	Volume = {14},
	Year = {1978}}

@incollection{asadpour2008stochastic,
	Author = {Asadpour, Arash and Nazerzadeh, Hamid and Saberi, Amin},
	Booktitle = {Internet and Network Economics},
	Pages = {477--489},
	Publisher = {Springer},
	Title = {{Stochastic submodular maximization}},
	Year = {2008}}

@inproceedings{eaton2007bayesian,
	Annote = {comps{\_}models},
	Author = {Eaton, D and Murphy, K},
	Booktitle = {Proceedings of the 23nd Annual Conference on Uncertainty in Artificial Intelligence (UAI-07)},
	Title = {{{\{}B{\}}ayesian structure learning using dynamic programming and {\{}MCMC{\}}}},
	Url = {http://www.cs.ubc.ca/{~}murphyk/Papers/eaton-uai07.pdf},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/%7B~%7Dmurphyk/Papers/eaton-uai07.pdf}}

@article{lepski1997optimal,
	Author = {Lepski, O. V. and Spokoiny, V. G.},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Lepski, Spokoiny - 1997 - Optimal pointwise adaptive methods in nonparametric estimation.pdf:pdf},
	Journal = {The Annals of Statistics},
	Keywords = {Bandwidth selection,H{\"{o}}lder-type constraints,pointwise adaptive estimation},
	Number = {6},
	Pages = {2512--2546},
	Title = {{Optimal pointwise adaptive methods in nonparametric estimation}},
	Volume = {25},
	Year = {1997}}

@article{Gopalan2013,
	Abstract = {We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms' rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets.},
	Archiveprefix = {arXiv},
	Arxivid = {1311.0466},
	Author = {Gopalan, Aditya and Mannor, Shie and Mansour, Yishay},
	Eprint = {1311.0466},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Gopalan, Mannor, Mansour - 2013 - Thompson Sampling for Complex Bandit Problems(2).pdf:pdf},
	Month = {nov},
	Title = {{Thompson Sampling for Complex Bandit Problems}},
	Url = {http://arxiv.org/abs/1311.0466},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1311.0466}}

@article{Agr95,
	Author = {Agrawal, R},
	Journal = {Advances in Applied Mathematics},
	Pages = {1054--1078},
	Title = {{Sample mean based index policies with O(log n) regret for the multi-armed bandit problem}},
	Volume = {27},
	Year = {1995}}

@article{dekel13switching,
	Author = {Dekel, Ofer and Ding, Jian and Koren, Tomer and Peres, Yuval},
	Journal = {CoRR},
	Title = {{Bandits with Switching Costs: {\{}T{\^{}}{\{}2/3{\}}{\}} Regret}},
	Volume = {abs/1310.2},
	Year = {2013}}

@article{watts1998collective,
	Abstract = {Networks of coupled dynamical systems have been used to model biological oscillators1--4 , Josephson junction arrays5,6 , excitable media7 , neural networks8--10 , spatial games11 , genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. Butmany biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks `rewired' to intro- duce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them `small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15 ). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.},
	Author = {Watts, Duncan J. and Strogatz, Steven H.},
	Journal = {Nature},
	Pages = {440--442},
	Title = {{Collective dynamics of small-world networks}},
	Volume = {393},
	Year = {1998}}

@inproceedings{chau2011apolo,
	Author = {Chau, Duen Horng and Kittur, Aniket and Hong, Jason I. and Faloutsos, Christos},
	Booktitle = {Conference on Human Factors in Computing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Chau et al. - 2011 - Apolo making sense of large network data by combining rich user interaction and machine learning.pdf:pdf},
	Title = {{Apolo: Making sense of large network data by combining rich user interaction and machine learning}},
	Url = {https://www.cc.gatech.edu/{~}dchau/papers/11-chi-apolo.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://www.cc.gatech.edu/%7B~%7Ddchau/papers/11-chi-apolo.pdf}}

@phdthesis{zhu2005semi-supervised,
	Address = {Pittsburgh, PA, USA},
	Annote = {AAI3179046},
	Author = {Zhu, Xiaojin},
	Isbn = {0-542-19059-1},
	Publisher = {Carnegie Mellon University},
	School = {Carnegie Mellon University},
	Title = {{Semi-supervised learning with graphs}},
	Year = {2005}}

@inproceedings{bezdek2002some,
	Address = {London, UK},
	Author = {Bezdek, James C and Hathaway, Richard J},
	Booktitle = {Proceedings of the 2002 AFSS International Conference on Fuzzy Systems. Calcutta: Advances in Soft Computing},
	Isbn = {3-540-43150-0},
	Pages = {288--300},
	Publisher = {Springer-Verlag},
	Series = {AFSS '02},
	Title = {{Some Notes on Alternating Optimization}},
	Url = {http://portal.acm.org/citation.cfm?id=647300.721144},
	Year = {2002},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?id=647300.721144}}

@inproceedings{wen2017online,
	Abstract = {We study the online influence maximization problem in social networks under the independent cascade model. Specifically, we aim to learn the set of " best influencers " in a social network online while repeatedly interacting with it. We address the challenges of (i) combinatorial action space, since the number of feasible influencer sets grows exponentially with the maximum number of influencers, and (ii) limited feedback, since only the influenced portion of the network is observed. Under a stochastic semi-bandit feedback, we propose and analyze IMLinUCB, a computationally efficient UCB-based algorithm. Our bounds on the cumulative regret are polynomial in all quantities of interest, achieve near-optimal dependence on the number of interactions and reflect the topology of the network and the activation probabilities of its edges, thereby giving insights on the problem complexity. To the best of our knowledge, these are the first such results. Our experiments show that in several representative graph topologies, the regret of IMLinUCB scales as suggested by our upper bounds. IMLinUCB permits linear generalization and thus is both statistically and computationally suitable for large-scale problems. Our experiments also show that IMLinUCB with linear generalization can lead to low regret in real-world online influence maximization.},
	Author = {Wen, Zheng and Kveton, Branislav and Valko, Michal and Vaswani, Sharan},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Online influence maximization under independent cascade model with semi-bandit feedback}},
	Year = {2017}}

@article{lee2010spectral,
	Author = {Lee, Ann B and Wasserman, Larry},
	Doi = {10.1198/jasa.2010.tm09754},
	Journal = {Journal of the American Statistical Association},
	Number = {0},
	Pages = {1--15},
	Title = {{Spectral Connectivity Analysis}},
	Url = {http://pubs.amstat.org/doi/abs/10.1198/jasa.2010.tm09754},
	Volume = {0},
	Year = {2010},
	Bdsk-Url-1 = {http://pubs.amstat.org/doi/abs/10.1198/jasa.2010.tm09754},
	Bdsk-Url-2 = {https://doi.org/10.1198/jasa.2010.tm09754}}

@article{gomez2003immuno-fuzzy,
	Author = {Gomez, J and Gonzalez, F and Dasgupta, D},
	Doi = {10.1109/FUZZ.2003.1206605},
	Journal = {Fuzzy Systems, 2003. FUZZ '03. The 12th IEEE International Conference on},
	Keywords = {fuzzy logic,fuzzy rules,fuzzy set theory,immuno fuzzy approach,real data sets,security of data anomaly detection,synthetic sets},
	Month = {may},
	Pages = {1219--1224 vol.2},
	Title = {{An immuno-fuzzy approach to anomaly detection}},
	Volume = {2},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1109/FUZZ.2003.1206605}}

@book{puterman1994markov,
	Address = {New York, NY},
	Author = {Puterman, Martin L},
	Howpublished = {Hardcover},
	Isbn = {0471619779},
	Publisher = {John Wiley {\&} Sons},
	Title = {{Markov Decision Processes: Discrete Stochastic Dynamic Programming}},
	Url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0471619779},
	Year = {1994},
	Bdsk-Url-1 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20%7B%5C&%7Dpath=ASIN/0471619779}}

@inproceedings{ortner08deterministic,
	Author = {Ortner, Ronald},
	Booktitle = {Proceedings of the 19th International Conference on Algorithmic Learning Theory, ALT 2008},
	Title = {{Online Regret Bounds for {\{}M{\}}arkov Decision Processes with Deterministic Transitions}},
	Year = {2008}}

@inproceedings{bartlettadaptive,
	Author = {Bartlett, Peter L and Hazan, Elad and Rakhlin, Alexander},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {65--72},
	Title = {{Adaptive Online Gradient Descent.}}}

@article{carpentier2013adaptive,
	Abstract = {We consider the problem of estimating the tail index $\backslash$alpha of a distribution satisfying a ($\backslash$alpha, $\backslash$beta) second-order Pareto-type condition, where $\backslash$beta is the second-order coefficient. When $\backslash$beta is available, it was previously proved that $\backslash$alpha can be estimated with the oracle rate n{\^{}}{\{}-$\backslash$beta/(2$\backslash$beta+1){\}}. On the contrary, when $\backslash$beta is not available, estimating $\backslash$alpha with the oracle rate is challenging; so additional assumptions that imply the estimability of $\backslash$beta are usually made. In this paper, we propose an adaptive estimator of $\backslash$alpha, and show that this estimator attains the rate (n/$\backslash$log$\backslash$log n){\^{}}{\{}-$\backslash$beta/(2$\backslash$beta+1){\}} without a priori knowledge of $\backslash$beta and any additional assumptions. Moreover, we prove that this ($\backslash$log$\backslash$log n){\^{}}{\{}$\backslash$beta/(2$\backslash$beta+1){\}} factor is unavoidable by obtaining the companion lower bound.},
	Author = {Carpentier, Alexandra and Kim, Arlene K. H.},
	Journal = {Statistica Sinica},
	Title = {{Adaptive and minimax optimal estimation of the tail coefficient}},
	Year = {2014}}

@book{grunbaum2003convex,
	Author = {Gr{\"{u}}nbaum, B},
	Publisher = {Springer Verlag},
	Title = {{Convex polytopes}},
	Volume = {221},
	Year = {2003}}

@techreport{jaggi,
	Author = {Jaggi, M},
	Institution = {Arxiv},
	Number = {1108.1170},
	Title = {{Convex Optimization without Projection Steps}},
	Year = {2011}}

@inproceedings{goldberg2011oasis:,
	Author = {Goldberg, Andrew and Zhu, Xiaojin and Furger, Alex and Xu, Jun-Ming},
	Booktitle = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
	Title = {{OASIS: Online active semisupervised learning}},
	Year = {2011}}

@inproceedings{le2013fastfood,
	Abstract = {Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that computing the de- cision function is typically expensive, espe- cially at prediction time. In this paper, we overcome this difficulty by proposing Fast- food, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matri- ces when combined with diagonal Gaussian matrices exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks (Rahimi {\&} Recht, 2007) and thereby speeding up the computation for a large range of ker- nel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) com- putation and storage, without sacrificing ac- curacy. We prove that the approximation is unbiased and has low variance. Extensive ex- periments show that we achieve similar accu- racy to full kernel expansions and Random Kitchen Sinks while being 100x faster and us- ing 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applica- tions that have large training sets and/or re- quire real-time prediction.},
	Author = {Le, Quoc and Sarl{\'{o}}s, Tam{\'{a}}s and Smola, Alex J},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Fastfood --- Approximating kernel expansions in loglinear time}},
	Year = {2013}}

@book{Brent1973,
	Abstract = {Outstanding text for graduate students and research workers proposes improvements to existing algorithms, extends their related mathematical theories, and offers details on new algorithms for approximating local and global minima. Many numerical examples, along with complete analysis of rate of convergence for most of the algorithms and error bounds that allow for the effect of rounding errors.},
	Author = {Brent, R P},
	Booktitle = {Book},
	Chapter = {4},
	Editor = {Englewood, N and Cliffs, J},
	Isbn = {0486419983},
	Pages = {195},
	Publisher = {Prentice-Hall},
	Series = {Prentice-Hall series in automatic computation},
	Title = {{Algorithms for minimization without derivatives}},
	Url = {http://www.cs.ox.ac.uk/people/richard.brent/pd/rpb011a.pdf},
	Year = {1973},
	Bdsk-Url-1 = {http://www.cs.ox.ac.uk/people/richard.brent/pd/rpb011a.pdf}}

@article{koutis_solving_2011,
	Author = {Koutis, Ioannis and Miller, Gary L and Peng, Richard},
	Journal = {arXiv preprint arXiv:1102.4842},
	Title = {{Solving {\{}SDD{\}} linear systems in {\{}timeO{\}} (mlognlog (1/{\{}$\backslash$k o{\}}))}},
	Url = {http://www.researchgate.net/profile/Richard{\_}Peng/publication/221499482{\_}A{\_}Nearly-m{\_}log{\_}n{\_}Time{\_}Solver{\_}for{\_}SDD{\_}Linear{\_}Systems/links/004635362a1ac2587f000000.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://www.researchgate.net/profile/Richard%7B%5C_%7DPeng/publication/221499482%7B%5C_%7DA%7B%5C_%7DNearly-m%7B%5C_%7Dlog%7B%5C_%7Dn%7B%5C_%7DTime%7B%5C_%7DSolver%7B%5C_%7Dfor%7B%5C_%7DSDD%7B%5C_%7DLinear%7B%5C_%7DSystems/links/004635362a1ac2587f000000.pdf}}

@inproceedings{DBLP:conf/icml/2010,
	Booktitle = {ICML},
	Editor = {F{\"{u}}rnkranz, Johannes and Joachims, Thorsten},
	Publisher = {Omnipress},
	Title = {{Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel}},
	Year = {2010}}

@article{fill1998interruptible,
	Abstract = {For a large class of examples arising in statistical physics known as attractive spin systems (e.g., the Ising model), one seeks to sample from a probability distribution $\pi$ on an enormously large state space, but elementary sampling is ruled out by the infeasibility of calculating an appropriate normalizing constant. The same difficulty arises in computer science problems where one seeks to sample randomly from a large finite distributive lattice whose precise size cannot be ascertained in any reasonable amount of time. The Markov chain Monte Carlo (MCMC) approximate sampling approach to such a problem is to construct and run "for a long time" a Markov chain with long-run distribution $\pi$. But determining how long is long enough to get a good approximation can be both analytically and empirically difficult. Recently, Propp and Wilson have devised an ingenious and efficient algorithm to use the same Markov chains to produce perfect (i.e., exact) samples from $\pi$. However, the running time of their algorithm is an unbounded random variable whose order of magnitude is typically unknown a priori and which is not independent of the state sampled, so a naive user with limited patience who aborts a long run of the algorithm will introduce bias. We present a new algorithm which (1) again uses the same Markov chains to produce perfect samples from $\pi$, but is based on a different idea (namely, acceptance/rejection sampling); and (2) eliminates user-impatience bias. Like the Propp-Wilson algorithm, the new algorithm applies to a general class of suitably monotone chains, and also (with modification) to "anti-monotone" chains. When the chain is reversible, naive implementation of the algorithm uses fewer transitions but more space than Propp-Wilson. When fine-tuned and applied with the aid of a typical pseudorandom number generator to an attractive spin system on n sites using a random site updating Gibbs sampler whose mixing time $\tau$ is polynomial in n, the algorithm runs in time of the same order (bound) as Propp-Wilson [expectation O($\tau$ log n)] and uses only logarithmically more space [expectation O(n log n), vs. O(n) for Propp-Wilson].},
	Author = {Fill, James Allen},
	Journal = {Annals of Applied Probability},
	Keywords = {Attractive spin system,Duality,Gibbs sampler,Ising model,Markov chain Monte Carlo,Monotone chain,Partially ordered set,Perfect simulation,Rejection sampling,Separation,Strong stationary time},
	Number = {1},
	Pages = {131--162},
	Title = {{An interruptible algorithm for perfect sampling via Markov chains}},
	Volume = {8},
	Year = {1998}}

@article{BT03,
	Author = {Beck, A and Teboulle, M},
	Journal = {Operations Research Letters},
	Number = {3},
	Pages = {167--175},
	Title = {{Mirror descent and nonlinear projected subgradient methods for convex optimization}},
	Volume = {31},
	Year = {2003}}

@article{hager1989updating,
	Author = {Hager, W W},
	Journal = {SIAM review},
	Pages = {221--239},
	Publisher = {JSTOR},
	Title = {{Updating the inverse of a matrix}},
	Year = {1989}}

@inproceedings{WKH11,
	Author = {Warmuth, M and Koolen, W and Helmbold, D},
	Booktitle = {In Proceedings of the 22nd International Conference on Algorithmic Learning Theory (ALT)},
	Title = {{Combining initial segments of lists}},
	Year = {2011}}

@book{fedorov1972theory,
	Author = {Fedorov, V V},
	Publisher = {Academic press},
	Title = {{Theory of optimal experiments}},
	Year = {1972}}

@book{koller2009bayesian,
	Annote = {comps{\_}models},
	Author = {Koller, Daphne and Friedman, Nir},
	Keywords = {bibtex-import},
	Publisher = {MIT Press},
	Title = {{Probabilistic Graphical Models: Principles and Techniques}},
	Year = {2009}}

@book{berry1985bandit,
	Author = {Berry, Donald A and Fristedt, Bert},
	Booktitle = {Journal of the Operational Research Society},
	Number = {8},
	Pages = {viii, 275},
	Publisher = {Chapman and Hall},
	Series = {Monographs on statistics and applied probability},
	Title = {{Bandit Problems: Sequential Allocation of Experiments}},
	Volume = {38},
	Year = {1985}}

@techreport{zhu2008semi-supervised,
	Author = {Zhu, Xiaojin},
	Institution = {University of Wisconsin-Madison},
	Number = {1530},
	Title = {{Semi-supervised learning literature survey}},
	Url = {http://pages.cs.wisc.edu/{~}jerryzhu/pub/ssl{\_}survey.pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://pages.cs.wisc.edu/%7B~%7Djerryzhu/pub/ssl%7B%5C_%7Dsurvey.pdf}}

@article{carpentier2014asimple,
	Archiveprefix = {arXiv},
	Arxivid = {1505.04627},
	Author = {Carpentier, Alexandra and Valko, Michal},
	Eprint = {1505.04627},
	Journal = {arXiv:1505.04627, http://arxiv.org/abs/1505.04627, ArXiv e-prints,},
	Title = {{Simple regret for infinitely many armed bandits}},
	Year = {2015}}

@inproceedings{collins02discriminative,
	Address = {Morristown, NJ, USA},
	Author = {Collins, Michael},
	Booktitle = {EMNLP '02: Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing},
	Doi = {http://dx.doi.org/10.3115/1118693.1118694},
	Pages = {1--8},
	Publisher = {Association for Computational Linguistics},
	Title = {{Discriminative training methods for hidden {\{}M{\}}arkov models: theory and experiments with perceptron algorithms}},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.3115/1118693.1118694}}

@inproceedings{klein03a*parsing,
	Address = {Morristown, NJ, USA},
	Author = {Klein, Dan and Manning, Christopher D},
	Booktitle = {NAACL '03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology},
	Keywords = {algorithm,nlp,parsing,viterbi},
	Pages = {40--47},
	Publisher = {Association for Computational Linguistics},
	Title = {{{\{}A{\}}{\^{}}* parsing: fast exact Viterbi parse selection}},
	Url = {http://portal.acm.org/citation.cfm?id=1073461},
	Year = {2003},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?id=1073461}}

@article{gray1998quantization,
	Author = {Gray, Robert and Neuhoff, David},
	Journal = {IEEE Transactions on Information Theory},
	Number = {6},
	Pages = {2325--2383},
	Title = {{Quantization}},
	Volume = {44},
	Year = {1998}}

@inproceedings{guestrin2004solving,
	Author = {Guestrin, Carlos and Hauskrecht, Milos and Kveton, Branislav},
	Booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
	Pages = {235--242},
	Title = {{Solving Factored {\{}MDPs{\}} with Continuous and Discrete Variables}},
	Year = {2004}}

@article{bates2003ten,
	Abstract = {While evidence-based medicine has increasingly broad-based support
in health care, it remains difficult to get physicians to actually
practice it. Across most domains in medicine, practice has lagged
behind knowledge by at least several years. The authors believe that
the key tools for closing this gap will be information systems that
provide decision support to users at the time they make decisions,
which should result in improved quality of care. Furthermore, providers
make many errors, and clinical decision support can be useful for
finding and preventing such errors. Over the last eight years the
authors have implemented and studied the impact of decision support
across a broad array of domains and have found a number of common
elements important to success. The goal of this report is to discuss
these lessons learned in the interest of informing the efforts of
others working to make the practice of evidence-based medicine a
reality.},
	Author = {Bates, David W and Kuperman, Gilad J and Wang, Samuel and Gandhi, Tejal and Kittler, Anne and Volk, Lynn and Spurr, Cynthia and Khorasani, Ramin and Tanasijevic, Milenko and Middleton, Blackford},
	Doi = {10.1197/jamia.M1370},
	Institution = {Department of Medicine, Brigham and Women's Hospital, Boston, MA 02115, USA. dbates@partners.org},
	Journal = {J Am Med Inform Assoc},
	Keywords = {Clinical,Computer-Assisted,Decision Making,Decision Support Systems,Decision Support Techniques,Evidence-B},
	Number = {6},
	Pages = {523--530},
	Pmid = {12925543},
	Title = {{Ten commandments for effective clinical decision support: making the practice of evidence-based medicine a reality.}},
	Url = {http://dx.doi.org/10.1197/jamia.M1370},
	Volume = {10},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1197/jamia.M1370}}

@inproceedings{low2010graphlab:,
	Address = {Catalina Island, California},
	Author = {Low, Yucheng and Gonzalez, Joseph and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos and Hellerstein, Joseph M},
	Booktitle = {Conference on Uncertainty in Artificial Intelligence (UAI)},
	Month = {jul},
	Title = {{GraphLab: A New Parallel Framework for Machine Learning}},
	Year = {2010}}

@article{warkentin2004heparin-induced,
	Abstract = {This chapter about the recognition, treatment, and prevention of heparin-induced
thrombocytopenia (HIT) is part of the Seventh ACCP Conference on
Antithrombotic and Thrombolytic Therapy: Evidence Based Guidelines.
Grade 1 recommendations are strong and indicate that the benefits
do, or do not, outweigh risks, burden, and costs. Grade 2 suggests
that individual patients' values may lead to different choices (for
a full understanding of the grading, see Guyatt et al, CHEST 2004;
126:179S-187S). Among the key recommendations in this chapter are
the following: For patients in whom the risk of HIT is considered
to be {\textgreater} 0.1{\%}, we recommend platelet count monitoring (Grade 1C).
For patients who are receiving therapeutic-dose unfractionated heparin
(UFH), we suggest at least every-other-day platelet count monitoring
until day 14, or until UFH is stopped, whichever occurs first (Grade
2C). For patients who are receiving postoperative antithrombotic
prophylaxis with UFH (HIT risk {\textgreater} 1{\%}), we suggest at least every-other-day
platelet count monitoring between postoperative days 4 to 14 (or
until UFH is stopped, whichever occurs first) [Grade 2C]. For medical/obstetric
patients who are receiving prophylactic-dose UFH, postoperative patients
receiving prophylactic-dose low molecular weight heparin (LMWH),
postoperative patients receiving intravascular catheter UFH "flushes,"
or medical/obstetrical patients receiving LMWH after first receiving
UFH (risk, 0.1 to 1{\%}), we suggest platelet count monitoring every
2 days or 3 days from day 4 to day 14, or until heparin is stopped,
whichever occurs first (Grade 2C). For medical/obstetrical patients
who are only receiving LMWH, or medical patients who are receiving
only intravascular catheter UFH flushes (risk {\textless} 0.1{\%}), we suggest
clinicians do not use routine platelet count monitoring (Grade 2C).
For patients with strongly suspected (or confirmed) HIT, whether
or not complicated by thrombosis, we recommend use of an alternative
anticoagulant, such as lepirudin (Grade 1C+), argatroban (Grade 1C),
bivalirudin (Grade 2C), or danaparoid (Grade 1B). For patients with
strongly suspected (or confirmed) HIT, we recommend routine ultrasonography
of the lower-limb veins for investigation of deep venous thrombosis
(Grade 1C); against the use of vitamin K antagonist (VKA) [coumarin]
therapy until after the platelet count has substantially recovered;
that the VKA antagonist be administered only during overlapping alternative
anticoagulation (minimum 5-day overlap); and begun with low, maintenance
doses (all Grade 2C). For patients receiving VKAs at the time of
diagnosis of HIT, we recommend use of vitamin K (Grade 2C) [corrected]
For patients with a history of HIT who are HIT antibody negative
and require cardiac surgery, we recommend use of UFH (Grade 1C).},
	Author = {Warkentin, Theodore E and Greinacher, Andreas},
	Doi = {10.1378/chest.126.3_suppl.311S},
	Institution = {Hamilton Regional Laboratory Medicine Program, Hamilton Health Sciences, General Site, 237 Barton St E, Hamilton, Ontario L8L 2X2, Canada. twarken@mcmaster.ca},
	Journal = {Chest},
	Keywords = {Dose-Response Relationship,Drug; Drug Monitoring; Evidence-Based Medicine; F,Low-Molecular-Weight; Humans; Platelet Count; Pre},
	Month = {sep},
	Number = {3 Suppl},
	Pages = {311S--337S},
	Pmid = {15383477},
	Title = {{Heparin-induced thrombocytopenia: recognition, treatment, and prevention: the Seventh ACCP Conference on Antithrombotic and Thrombolytic Therapy.}},
	Url = {http://dx.doi.org/10.1378/chest.126.3{\_}suppl.311S},
	Volume = {126},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1378/chest.126.3%7B%5C_%7Dsuppl.311S},
	Bdsk-Url-2 = {http://dx.doi.org/10.1378/chest.126.3_suppl.311S}}

@inproceedings{GW98,
	Author = {Gentile, C and Warmuth, M},
	Booktitle = {Advances in Neural Information Processing Systems (NIPS)},
	Pages = {225--231},
	Title = {{Linear hinge loss and average margin}},
	Year = {1998}}

@article{Yun2014,
	Abstract = {In this paper, we consider networks consisting of a finite number of non-overlapping communities. To extract these communities, the interaction between pairs of nodes may be sampled from a large available data set, which allows a given node pair to be sampled several times. When a node pair is sampled, the observed outcome is a binary random variable, equal to 1 if nodes interact and to 0 otherwise. The outcome is more likely to be positive if nodes belong to the same communities. For a given budget of node pair samples or observations, we wish to jointly design a sampling strategy (the sequence of sampled node pairs) and a clustering algorithm that recover the hidden communities with the highest possible accuracy. We consider both non-adaptive and adaptive sampling strategies, and for both classes of strategies, we derive fundamental performance limits satisfied by any sampling and clustering algorithm. In particular, we provide necessary conditions for the existence of algorithms recovering the communities accurately as the network size grows large. We also devise simple algorithms that accurately reconstruct the communities when this is at all possible, hence proving that the proposed necessary conditions for accurate community detection are also sufficient. The classical problem of community detection in the stochastic block model can be seen as a particular instance of the problems consider here. But our framework covers more general scenarios where the sequence of sampled node pairs can be designed in an adaptive manner. The paper provides new results for the stochastic block model, and extends the analysis to the case of adaptive sampling.},
	Archiveprefix = {arXiv},
	Arxivid = {1402.3072},
	Author = {Yun, Se-Young and Proutiere, Alexandre},
	Eprint = {1402.3072},
	Month = {feb},
	Title = {{Community Detection via Random and Adaptive Sampling}},
	Url = {http://arxiv.org/abs/1402.3072},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1402.3072}}

@inproceedings{klein2013cascaded,
	Abstract = {This paper considers the Inverse Reinforcement Learning (IRL) problem, that is inferring a reward function for which a demonstrated expert policy is optimal. We propose to break the IRL problem down into two generic Supervised Learning steps: this is the Cascaded Supervised IRL (CSI) approach. A classification step that defines a score function is followed by a regression step providing a reward function. A theoretical analysis shows that the demonstrated expert policy is nearoptimal for the computed reward function. Not needing to repeatedly solve a Markov Decision Process (MDP) and the ability to leverage existing techniques for classification and regression are two important advantages of the CSI approach. It is furthermore empirically demonstrated to compare positively to state-of-the-art approaches when using only transitions sampled according to the expert policy, up to the use of some heuristics. This is exemplified on two classical benchmarks (the mountain car problem and a highway driving simulator).},
	Address = {Prague (Czech Republic)},
	Author = {Klein, Edouard and PIOT, Bilal and Geist, Matthieu and Pietquin, Olivier},
	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD 2013)},
	Doi = {10.1007/978-3-642-40988-2_1},
	Editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and Zelezny, Filip},
	Isbn = {978-3-642-40987-5},
	Month = {sep},
	Pages = {1--16},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {{A cascaded supervised learning approach to inverse reinforcement learning}},
	Url = {http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/327.pdf},
	Volume = {8188},
	Year = {2013},
	Bdsk-Url-1 = {http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/327.pdf},
	Bdsk-Url-2 = {https://doi.org/10.1007/978-3-642-40988-2_1}}

@inproceedings{black92,
	Author = {Black, Ezra},
	Booktitle = {{\{}LINGUIST{\}} {\{}L{\}}ist 3.587, $\backslash$texttt{\{}http://www.linguistlist.org/issues/3/3-587.html{\}}},
	Title = {{Meeting of interest group on evaluation of broad-coverage parsers of English}},
	Year = {1992}}

@article{ratliff2006maximum,
	Abstract = {Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task.},
	Author = {Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
	Doi = {10.1145/1143844.1143936},
	Isbn = {1595933832},
	Issn = {17458358},
	Journal = {Proceedings of the 23rd ICML},
	Number = {10},
	Pmid = {17914344},
	Publisher = {ACM Press},
	Title = {{Maximum margin planning}},
	Url = {http://webdocs.cs.ualberta.ca/{~}maz/publications/maximummarginplanning.pdf},
	Volume = {3},
	Year = {2006},
	Bdsk-Url-1 = {http://webdocs.cs.ualberta.ca/%7B~%7Dmaz/publications/maximummarginplanning.pdf},
	Bdsk-Url-2 = {https://doi.org/10.1145/1143844.1143936}}

@article{Spa97,
	Author = {Spall, J},
	Journal = {Automatica},
	Number = {1},
	Pages = {109--112},
	Title = {{A one-measurement form of simultaneous perturbation stochastic approximation}},
	Volume = {33},
	Year = {1997}}

@article{song2007conditional,
	Address = {Piscataway, NJ, USA},
	Annote = {Fellow-Sanjay Ranka
comps{\_}ano},
	Author = {Song, Xiuyao and Wu, Mingxi and Jermaine, Christopher},
	Doi = {http://dx.doi.org/10.1109/TKDE.2007.1009},
	Issn = {1041-4347},
	Journal = {IEEE Transactions on Knowledge and Data Engineering},
	Number = {5},
	Pages = {631--645},
	Publisher = {IEEE Educational Activities Department},
	Title = {{Conditional Anomaly Detection}},
	Volume = {19},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TKDE.2007.1009}}

@inproceedings{kawahara22submodularity,
	Author = {Kawahara, Y and Nagano, K and Tsuda, K and Bilmes, J A},
	Booktitle = {Adv. NIPS 22},
	Title = {{Submodularity Cuts and Applications}},
	Year = {2009}}

@inproceedings{dechter1996bucket,
	Author = {Dechter, Rina},
	Booktitle = {Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence},
	Pages = {211--219},
	Title = {{Bucket Elimination: A Unifying Framework for Probabilistic Inference}},
	Year = {1996}}

@article{Kveton2010,
	Abstract = {This paper proposes a novel algorithm for semi-supervised learning. This algorithm learns graph cuts that maximize the margin with respect to the labels induced by the harmonic function solution. We motivate the approach, compare it to existing work, and prove a bound on its generalization error. The quality of our solutions is evaluated on a synthetic problem and three UCI ML repository datasets. In most cases, we outperform manifold regularization of support vector machines, which is a state-of-the-art approach to semi-supervised max-margin learning.},
	Author = {Kveton, B. and Valko, M. and Rahimi, A. and Huang, L.},
	Issn = {15324435},
	Journal = {Journal of Machine Learning Research},
	Title = {{Semi-supervised learning with max-margin graph cuts}},
	Volume = {9},
	Year = {2010}}

@article{bach2008cgl,
	Author = {Bach, F},
	Journal = {Journal of Machine Learning Research},
	Pages = {1179--1225},
	Publisher = {MIT Press Cambridge, MA, USA},
	Title = {{Consistency of the group {\{}L{\}}asso and multiple kernel learning}},
	Volume = {9},
	Year = {2008}}

@inproceedings{Sanchez-2000-Mislabeled,
	Author = {Sanchez, J S and Barandela, R and Marques, A I and Alejo, R and J., Badenas.},
	Booktitle = {Advances in Pattern Recognition Lecture Notes in Computer Science 1876},
	Pages = {621--630},
	Title = {{Decontamination of Training Data for Supevised Pattern Recognition.}},
	Year = {2000}}

@article{Zhaoyu,
	Author = {Zhao, P and Yu, B},
	Journal = {Journal of Machine Learning Research},
	Pages = {2541--2563},
	Title = {{On Model Selection Consistency of {\{}L{\}}asso.}},
	Volume = {7},
	Year = {2006}}

@article{spielman_graph_2011,
	Author = {Spielman, Daniel A and Srivastava, Nikhil},
	Journal = {Journal on Computing},
	Number = {6},
	Pages = {1913--1926},
	Title = {{Graph sparsification by effective resistances}},
	Volume = {40},
	Year = {2011}}

@inproceedings{engel2002sparse,
	Abstract = {We present a novel algorithm for sparse online greedy kernel- based nonlinear regression. This algorithm improves current approaches to kernel-based regression in two aspects. First, it operates online - at each time step it observes a single new input sample, performs an update and discards it. Second, the solution maintained is extremely sparse. This is achieved by an explicit greedy sparsification process that admits into the kernel representation a new input sample only if its feature space image is linearly independent of the images of previously admitted samples. We show that the algorithm implements a form of gradient ascent and demonstrate its scaling and noise tolerance properties on three benchmark regression problems.},
	Author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
	Booktitle = {European Conference on Machine Learning},
	Title = {{Sparse online greedy support vector regression}},
	Year = {2002}}

@inproceedings{Vov90,
	Author = {Vovk, V},
	Booktitle = {Proceedings of the third annual workshop on Computational learning theory (COLT)},
	Pages = {371--386},
	Title = {{Aggregating strategies}},
	Year = {1990}}

@article{dayan1994td,
	Author = {Dayan, Peter and Sejnowski, Terry},
	Journal = {Machine Learning},
	Pages = {295--301},
	Title = {{{\{}TD{\}}($\lambda$) Converges with Probability 1}},
	Volume = {14},
	Year = {1994}}

@inproceedings{propp1998coupling,
	Abstract = {The Markov chain Monte Carlo method is a general technique for obtaining samples from a probability distribution. In earlier work, we showed that for many applications one can modify the Markov chain Monte Carlo method so as to remove all bias in the output resulting from the biased choice of an initial state for the chain; we have called this method coupling from the past (CFTP). Here we describe this method in a fashion that should make our ideas accessible to researchers from diverse areas. Our expository strategy is to avoid proofs and focus on sample applications.},
	Author = {Propp, James and Wilson, David},
	Booktitle = {Microsurveys in Discrete Probability},
	Title = {{Coupling from the past: A user's guide}},
	Year = {1998}}

@inproceedings{hazan2009online,
	Abstract = {We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efficient and are Hannan-consistent in both the full information and bandit settings.},
	Author = {Hazan, Elad and Kale, Satyen},
	Booktitle = {Advances in Neural Information Processing Systems 22},
	Editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C K I and Culotta, A},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Hazan, Kale - 2009 - Beyond Convexity Online Submodular Minimization.pdf:pdf},
	Pages = {700--708},
	Publisher = {Citeseer},
	Title = {{Beyond Convexity: Online Submodular Minimization}},
	Url = {http://www.satyenkale.com/papers/submodular.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.4365{\&}rep=rep1{\&}type=pdf},
	Year = {2009},
	Bdsk-Url-1 = {http://www.satyenkale.com/papers/submodular.pdf%20http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.4365%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{tewari08optimistic,
	Author = {Tewari, Ambuj and Bartlett, Peter L},
	Pages = {1505--1512},
	Title = {{Optimistic Linear Programming gives Logarithmic Regret for Irreducible {\{}MDPs{\}}}}}

@article{koivisto2004exact,
	Annote = {comps{\_}models},
	Author = {Koivisto, M and Sood, K},
	Journal = {Journal of Machine Learning Research},
	Pages = {549--573},
	Title = {{Exact {\{}B{\}}ayesian Structure Discovery in {\{}B{\}}ayesian Networks}},
	Url = {http://citeseer.ist.psu.edu/article/koivisto04exact.html},
	Volume = {5},
	Year = {2004},
	Bdsk-Url-1 = {http://citeseer.ist.psu.edu/article/koivisto04exact.html}}

@book{winograd72understanding,
	Address = {Orlando, FL, USA},
	Author = {Winograd, Terry},
	Isbn = {0127597506},
	Publisher = {Academic Press, Inc.},
	Title = {{Understanding Natural Language}},
	Year = {1972}}

@article{breunig2000lof:,
	Address = {New York, NY, USA},
	Author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"{o}}rg},
	Doi = {http://doi.acm.org/10.1145/335191.335388},
	Issn = {0163-5808},
	Journal = {SIGMOD Rec.},
	Number = {2},
	Pages = {93--104},
	Publisher = {ACM},
	Title = {{LOF: identifying density-based local outliers}},
	Volume = {29},
	Year = {2000},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/335191.335388}}

@article{Audibert-Bubeck-Munos-2010,
	Abstract = {We consider the problem of finding the best arm in a stochastic multi-armed bandit game. The regret of a forecaster is here defined by the gap between the mean reward of the optimal arm and the mean reward of the ultimately chosen arm. We propose a highly exploring UCB policy and a new algorithm based on successive rejects. We show that these algorithms are essentially optimal since their regret decreases exponentially at a rate which is, up to a logarithmic factor, the best possible. However, while the UCB policy needs the tuning of a parameter depending on the unobservable hardness of the task, the successive rejects policy benefits from being parameter-free, and also independent of the scaling of the rewards.},
	Author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi},
	Journal = {Order A Journal On The Theory Of Ordered Sets And Its Applications},
	Keywords = {learning,statistics {\&} optimisation,theory {\&} algorithms},
	Pages = {1--17},
	Title = {{Best arm identification in multi-armed bandits}},
	Url = {http://eprints.pascal-network.org/archive/00007409/},
	Year = {2010},
	Bdsk-Url-1 = {http://eprints.pascal-network.org/archive/00007409/}}

@article{Batson:2013:SSG:2492007.2492029,
	Author = {Batson, Joshua and Spielman, Daniel A and Srivastava, Nikhil and Teng, Shang-Hua},
	Journal = {Commun. ACM},
	Number = {8},
	Pages = {87--94},
	Title = {{Spectral Sparsification of Graphs: Theory and Algorithms}},
	Volume = {56},
	Year = {2013}}

@inproceedings{shang2018adaptive,
	Abstract = {Hierarchical bandits is an approach for global optimization of extremely irregular functions. This paper provides new elements regarding POO, an adaptive meta-algorithm that does not require the knowledge of local smoothness of the target function. We first highlight the fact that the sub-routine algorithm used in POO should have a small regret under the assumption of local smoothness with respect to the chosen partitioning, which is unknown if it is satisfied by the standard sub-routine HOO. In this work, we establish such regret guarantee for HCT which is another hierarchical optimistic optimization algorithm that needs to know the smoothness. This confirms the validity of POO. We show that POO can be used with HCT as a sub-routine with a regret upper bound that matches that of best-known algorithms using the knowledge of smoothness up to a sqrt(log(n)) factor.},
	Author = {Shang, Xuedong and Kaufmann, Emilie and Valko, Michal},
	Booktitle = {European Workshop on Reinforcement Learning},
	Title = {{Adaptive black-box optimization got easier: HCT needs only local smoothness}},
	Url = {http://researchers.lille.inria.fr/{~}valko/hp/publications/shang2018adaptive.pdf},
	Year = {2018},
	Bdsk-Url-1 = {http://researchers.lille.inria.fr/%7B~%7Dvalko/hp/publications/shang2018adaptive.pdf}}

@article{lauritzen1988local,
	Author = {Lauritzen, Steffen and Spiegelhalter, David},
	Journal = {Journal of Royal Statistical Society},
	Pages = {157--224},
	Title = {{Local computations with probabilities on graphical structures and their application to expert systems}},
	Volume = {50},
	Year = {1988}}

@techreport{Nes11,
	Author = {Nesterov, Y},
	Institution = {Universit{\{}{\'{e}}{\}} catholique de Louvain, Center for Operations Research and Econometrics (CORE)},
	Title = {{Random gradient-free minimization of convex functions}},
	Type = {CORE Discussion Papers},
	Year = {2011}}

@article{audibert2014regret,
	Author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien and Lugosi, G{\'{a}}bor},
	Journal = {Mathematics of Operations Research},
	Pages = {31--45},
	Title = {{Regret in online combinatorial optimization}},
	Volume = {39},
	Year = {2014}}

@article{fine1997prediction,
	Author = {Fine, M J and Auble, T E and Yealy, D M and Hanusa, B H and Weissfeld, L A and Singer, D E and Coley, C M and Marrie, T J and Kapoor, W N},
	Journal = {New England Journal of Medicine},
	Number = {4},
	Pages = {243--250},
	Title = {{A Prediction Rule to Identify Low-Risk Patients with Community-Acquired Pneumonia}},
	Volume = {336},
	Year = {1997}}

@article{REGK14,
	Author = {de Rooij, Steven and van Erven, Tim and Gr{\"{u}}nwald, Peter D and Koolen, Wouter M},
	Journal = {Accepted to the Journal of Machine Learning Research},
	Title = {{Follow the Leader If You Can, Hedge If You Must}},
	Year = {2014}}

@article{erdos1959on,
	Abstract = {P. ErdH os, A. R nyi. Publ. Math. Debrecen, Vol. 6 (1959), pp. 290-297. bibtex-import network.},
	Author = {Erd{\H o}s, Paul and R{\'{e}}nyi, Alfr{\'{e}}d},
	Journal = {Publicationes Mathematicae},
	Pages = {290--297},
	Title = {{On random graphs}},
	Volume = {6},
	Year = {1959}}

@article{castro2008minimax,
	Abstract = {This paper analyzes the potential advantages and theoretical challenges of "active learning" algorithms. Active learning involves sequential sampling procedures that use information gleaned from previous samples in order to focus the sampling and accelerate the learning process relative to "passive learning" algorithms, which are based on nonadaptive (usually random) samples. There are a number of empirical and theoretical results suggesting that in certain situations active learning can be significantly more effective than passive learning. However, the fact that active learning algorithms are feedback systems makes their theoretical analysis very challenging. This paper aims to shed light on achievable limits in active learning. Using minimax analysis techniques, we study the achievable rates of classification error convergence for broad classes of distributions characterized by decision boundary regularity and noise conditions. The results clearly indicate the conditions under which one can expect significant gains through active learning. Furthermore, we show that the learning rates derived are tight for "boundary fragment" classes in d-dimensional feature spaces when the feature marginal density is bounded from above and below.},
	Author = {Castro, Rui M. and Nowak, Robert D.},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Active learning,Adaptive sampling,Minimax lower bounds,Statistical learning theory},
	Number = {5},
	Pages = {2339--2353},
	Title = {{Minimax bounds for active learning}},
	Volume = {54},
	Year = {2008}}

@article{burges1998tutorial,
	Author = {Burges, Christopher J C},
	Journal = {Data Mining and Knowledge Discovery},
	Number = {2},
	Pages = {121--167},
	Title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
	Url = {citeseer.ist.psu.edu/burges98tutorial.html},
	Volume = {2},
	Year = {1998},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/burges98tutorial.html}}

@inproceedings{audibert2011minimax,
	Author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien and Lugosi, Gabor},
	Booktitle = {Proceedings of the 24th annual Conference On Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Series = {COLT '11},
	Title = {{Minimax Policies for Combinatorial Prediction Games}},
	Year = {2011}}

@inproceedings{kocak2016onlinea,
	Abstract = {We consider adversarial multi-armed bandit problems where the learner is allowed to observe losses of a number of arms beside the arm that it actually chose. We study the case where all non-chosen arms reveal their loss with an unknown probability rt, independently of each other and the action of the learner. Moreover, we allow rt to change in every round t, which rules out the possibility of estimating rt by a well-concentrated sample average. We propose an algorithm which operates under the assumption that rt is large enough to warrant at least one side observation with high probability. We show that after T rounds in a bandit problem with N arms, the expected regret of our algorithm is of order O(sqrt(sum(t=1)T (1/rt) log N )), given that rt less than log T / (2N-2) for all t. All our bounds are within logarithmic factors of the best achievable performance of any algorithm that is even allowed to know exact values of rt.},
	Author = {Koc{\'{a}}k, Tom{\'{a}}{\v{s}} and Neu, Gergely and Valko, Michal},
	Booktitle = {Uncertainty in Artificial Intelligence},
	Title = {{Online learning with Erd{\H o}s-R{\'{e}}nyi side-observation graphs}},
	Url = {https://hal.inria.fr/hal-01320588/document},
	Year = {2016},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-01320588/document}}

@inproceedings{Rao2011,
	Author = {Rao, N S and Nowak, R D and Wright, S J and Kingsbury, N G},
	Booktitle = {International Conference on Image Processing (ICIP)},
	Title = {{Convex approaches to model wavelet sparsity patterns}},
	Year = {2011}}

@article{GyLiLu11Corr,
	Annote = {From Duplicate 3 (Efficient Tracking of Large Classes of Experts - Gy{\"{o}}rgy, A; Linder, T; Lugosi, G)

Accepted with minor revisions},
	Author = {Gy{\"{o}}rgy, Andr{\'{a}}s and Linder, Tam{\'{a}}s and Lugosi, G{\'{a}}bor},
	Journal = {IEEE Transactions on Information Theory},
	Number = {11},
	Pages = {6709--6725},
	Title = {{Efficient Tracking of Large Classes of Experts}},
	Volume = {abs/1110.2},
	Year = {2012}}

@article{chollet1997some,
	Author = {Chollet, J},
	Journal = {American Mathematical Monthly},
	Number = {7},
	Pages = {609--617},
	Publisher = {Mathematical Association of America},
	Title = {{Some inequalities for principal submatrices}},
	Volume = {104},
	Year = {1997}}

@inproceedings{ashbrook2008quickdraw:,
	Author = {Ashbrook, Daniel L and Clawson, James R and Lyons, Kent and Starner, Thad E and Patel, Nirmal},
	Booktitle = {Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems},
	Pages = {219--222},
	Series = {CHI '08},
	Title = {{Quickdraw: the impact of mobility and on-body placement on device access time}},
	Year = {2008}}

@book{Roc70,
	Author = {Rockafellar, R},
	Publisher = {Princeton University Press},
	Title = {{Convex Analysis}},
	Year = {1970}}

@article{globerson2007visualizing,
	Annote = {comps{\_}distance},
	Author = {Globerson, A and Roweis, S},
	Journal = {The 11th International Workshop on Artificial Intelligence and Statistics Puerto-Rico},
	Title = {{Visualizing pairwise similarity via semidefinite programming}},
	Url = {http://people.csail.mit.edu/gamir/pubs/psde.pdf},
	Year = {2007},
	Bdsk-Url-1 = {http://people.csail.mit.edu/gamir/pubs/psde.pdf}}

@inproceedings{feige2006maximizing,
	Author = {Feige, U},
	Booktitle = {Proc. ACM symposium on Theory of computing},
	Pages = {41--50},
	Title = {{On maximizing welfare when utility functions are subadditive}},
	Year = {2006}}

@inproceedings{carpentier2016revealing,
	Abstract = {We study a graph bandit setting where the objective of the learner is to detect the most influential node of a graph by requesting as little information from the graph as possible. One of the relevant applications for this setting is marketing in social networks, where the marketer aims at finding and taking advantage of the most influential customers. The existing approaches for bandit problems on graphs require either partial or complete knowledge of the graph. In this paper, we do not assume any knowledge of the graph, but we consider a setting where it can be gradually discovered in a sequential and active way. At each round, the learner chooses a node of the graph and the only information it receives is a stochastic set of the nodes that the chosen node is currently influencing. To address this setting, we propose BARE, a bandit strategy for which we prove a regret guarantee that scales with the detectable dimension, a problem dependent quantity that is often much smaller than the number of nodes.},
	Author = {Carpentier, Alexandra and Valko, Michal},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Carpentier, Valko - 2016 - Revealing graph bandits for maximizing local influence.pdf:pdf},
	Title = {{Revealing graph bandits for maximizing local influence}},
	Year = {2016}}

@article{lowe1999object,
	Author = {Lowe, David},
	Journal = {IEEE International Conference on Computer Vision},
	Pages = {1150--1157},
	Title = {{Object Recognition from Local Scale-Invariant Features}},
	Volume = {2},
	Year = {1999}}

@inproceedings{black93history,
	Address = {Morristown, NJ, USA},
	Author = {Black, Ezra and Jelinek, Fred and Lafferty, John and Magerman, David M and Mercer, Robert and Roukos, Salim},
	Booktitle = {ACL '93: Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics},
	Pages = {31--37},
	Publisher = {Association for Computational Linguistics},
	Title = {{Towards history-based grammars: using richer models for probabilistic parsing}},
	Year = {1993}}

@inproceedings{dash2002exact,
	Address = {San Francisco, CA, USA},
	Annote = {comps{\_}models},
	Author = {Dash, Denver and Cooper, Gregory F},
	Booktitle = {ICML '02: Proceedings of the Nineteenth International Conference on Machine Learning},
	Isbn = {1-55860-873-7},
	Pages = {91--98},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Exact model averaging with naive {\{}B{\}}ayesian classifiers}},
	Url = {http://www.pittsburgh.intel-research.net/{~}dhdash//docs/icml{\_}02.pdf},
	Year = {2002},
	Bdsk-Url-1 = {http://www.pittsburgh.intel-research.net/%7B~%7Ddhdash//docs/icml%7B%5C_%7D02.pdf}}

@inproceedings{viswanath2009evolution,
	Abstract = {Online social networks have become extremely popular; numerous sites allow users to interact and share content using social links. Users of these networks often establish hundreds to even thousands of social links with other users. Recently, researchers have suggested examining the activity network--- a network that is based on the actual interaction between users, rather than mere friendship---to distinguish between strong and weak links. While initial studies have led to in- sights on how an activity network is structurally different from the social network itself, a natural and important aspect of the activity network has been disregarded: the fact that over time social links can grow stronger or weaker. In this paper, we study the evolution of activity between users in the Facebook social network to capture this notion. We find that links in the activity network tend to come and go rapidly over time, and the strength of ties exhibits a general decreasing trend of activity as the social network link ages. For example, only 30{\%} of Facebook user pairs interact consistently from one month to the next. Interestingly, we also find that even though the links of the activity network change rapidly over time, many graph-theoretic properties of the activity network remain unchanged.},
	Author = {Viswanath, Bimal and Mislove, Alan and Cha, Meeyoung and Gummadi, Krishna P.},
	Booktitle = {ACM Workshop on Online Social Networks},
	Title = {{On the evolution of user interaction in facebook}},
	Year = {2009}}

@inproceedings{EWK14,
	Author = {van Erven, Tim and Warmuth, Manfred and Owski, Wojciech Kot$\backslash$l},
	Pages = {949--974},
	Title = {{Follow the Leader with Dropout Perturbations}}}

@misc{TheMendeleySupportTeam2011b,
	Abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
	Address = {London},
	Author = {{The Mendeley Support Team}},
	Booktitle = {Mendeley Desktop},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
	Keywords = {Mendeley,how-to,user manual},
	Pages = {1--16},
	Publisher = {Mendeley Ltd.},
	Title = {{Getting Started with Mendeley}},
	Url = {http://www.mendeley.com},
	Year = {2011},
	Bdsk-Url-1 = {http://www.mendeley.com}}

@inproceedings{valko2011conditionala,
	Abstract = {In this paper, we consider the problem of conditional anomaly detection that aims to identify data instances with an unusual response or a class label. We develop a new non-parametric approach for conditional anomaly detection based on the soft harmonic solution, with which we estimate the confidence of the label to detect anomalous mislabeling. We further regularize the solution to avoid the detection of isolated examples and examples on the boundary of the distribution support. We demonstrate the efficacy of the proposed method on several synthetic and UCI ML datasets in detecting unusual labels when compared to several baseline approaches. We also evaluate the performance of our method on a real-world electronic health record dataset where we seek to identify unusual patient-management decisions.},
	Author = {Valko, Michal and Kveton, Branislav and Valizadegan, Hamed and Cooper, Gregory F and Hauskrecht, Milos},
	Booktitle = {Proceedings of the 2011 IEEE International Conference on Data Mining},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Month = {jun},
	Title = {{Conditional anomaly detection with soft harmonic functions}},
	Year = {2011}}

@article{CBGe08,
	Author = {Cesa-Bianchi, N and Gentile, C},
	Doi = {10.1109/TIT.2007.911292},
	Issn = {0018-9448},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {arbitrary learning algorithm;ensemble;incremental},
	Number = {1},
	Pages = {386--390},
	Title = {{Improved Risk Tail Bounds for On-Line Algorithms}},
	Volume = {54},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIT.2007.911292}}

@inproceedings{korda2016distributed,
	Abstract = {We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art.},
	Author = {Korda, Nathan and Sz{\"{o}}r{\'{e}}nyi, Bal{\'{a}}zs and Li, Shuai},
	Booktitle = {International Conference on Machine Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Korda, Szorenyi, Li - 2016 - Distributed Clustering of Linear Bandits in Peer to Peer Networks.pdf:pdf},
	Month = {apr},
	Title = {{Distributed clustering of linear bandits in peer to peer networks}},
	Url = {http://proceedings.mlr.press/v48/korda16.pdf},
	Year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v48/korda16.pdf}}

@article{boutilier1999decision-theoretic,
	Author = {Boutilier, Craig and Dean, Thomas and Hanks, Steve},
	Journal = {Journal of Artificial Intelligence Research},
	Pages = {1--94},
	Title = {{Decision-Theoretic Planning: Structural Assumptions and Computational Leverage}},
	Volume = {11},
	Year = {1999}}

@article{Prot2017,
	Abstract = {AThis survey aims at demonstrating that the structure of precedence constraints plays a tremendous role on the complexity of scheduling problems. Indeed many problems can be NP-hard when considering general precedence constraints, while they become polynomially solvable for particular precedence constraints. We also show that there still are many very exciting challenges in this research area.},
	Archiveprefix = {arXiv},
	Arxivid = {1510.04833},
	Author = {Prot, D and Bellenguez-Morineau, O},
	Doi = {10.1007/s10951-017-0519-z},
	Eprint = {1510.04833},
	Issn = {10946136},
	Journal = {Journal of Scheduling},
	Keywords = {Complexity,Precedence constraints,Scheduling},
	Number = {1},
	Pages = {3--16},
	Title = {{A survey on how the structure of precedence constraints may change the complexity class of scheduling problems}},
	Volume = {21},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10951-017-0519-z}}

@inproceedings{balcan2005person,
	Author = {Balcan, Maria-Florina and Blum, Avrim and Choi, Patrick Pakyan and Lafferty, John and Pantano, Brian and Rwebangira, Mugizi Robert and Zhu, Xiaojin},
	Booktitle = {ICML 2005 Workshop on Learning with Partially Classified Training Data},
	Title = {{Person Identification in Webcam Images: An Application of Semi-Supervised Learning}},
	Year = {2005}}

@article{GyLiLu08,
	Author = {Gy{\"{o}}rgy, A and Linder, T and Lugosi, G},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {1604--1625},
	Title = {{Tracking the Best Quantizer}},
	Volume = {54},
	Year = {2008}}

@inproceedings{cortes2008stability,
	Author = {Cortes, Corinna and Mohri, Mehryar and Pechyony, Dmitry and Rastogi, Ashish},
	Booktitle = {Proceedings of the 25th International Conference on Machine Learning},
	Pages = {176--183},
	Title = {{Stability of Transductive Regression Algorithms}},
	Year = {2008}}

@inproceedings{belkin2004regularization,
	Author = {Belkin, Mikhail and Matveeva, Irina and Niyogi, Partha},
	Booktitle = {Conference on Learning Theory},
	Title = {{Regularization and semi-supervised learning on large graphs}},
	Url = {http://people.cs.uchicago.edu/{~}niyogi/papersps/reg{\_}colt.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://people.cs.uchicago.edu/%7B~%7Dniyogi/papersps/reg%7B%5C_%7Dcolt.pdf}}

@article{steinwart2005classification,
	Address = {Cambridge, MA, USA},
	Annote = {comps{\_}anX},
	Author = {Steinwart, Ingo and Hush, Don and Scovel, Clint},
	Issn = {1533-7928},
	Journal = {Journal of Machine Learning Research},
	Pages = {211--232},
	Publisher = {MIT Press},
	Title = {{A Classification Framework for Anomaly Detection}},
	Url = {http://jmlr.csail.mit.edu/papers/volume6/steinwart05a/steinwart05a.pdf},
	Volume = {6},
	Year = {2005},
	Bdsk-Url-1 = {http://jmlr.csail.mit.edu/papers/volume6/steinwart05a/steinwart05a.pdf}}

@article{audibert2010regret,
	Author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {dec},
	Pages = {2785--2836},
	Publisher = {JMLR.org},
	Title = {{Regret bounds and minimax policies under partial monitoring}},
	Volume = {11},
	Year = {2010}}

@inproceedings{yurtsever2017sketchy,
	Abstract = {This paper concerns a fundamental class of convex matrix optimization problems. It presents the first algorithm that uses optimal storage and provably computes a low-rank approximation of a solution. In particular, when all solutions have low rank, the algorithm converges to a solution. This algorithm, SketchyCGM, modifies a standard convex optimization scheme, the conditional gradient method, to store only a small randomized sketch of the matrix variable. After the optimization terminates, the algorithm extracts a low-rank approximation of the solution from the sketch. In contrast to nonconvex heuristics, the guarantees for SketchyCGM do not rely on statistical models for the problem data. Numerical work demonstrates the benefits of SketchyCGM over heuristics.},
	Archiveprefix = {arXiv},
	Arxivid = {1702.06838},
	Author = {Yurtsever, Alp and Udell, Madeleine and Tropp, Joel A. and Cevher, Volkan},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Eprint = {1702.06838},
	Title = {{Sketchy decisions: Convex low-rank matrix optimization with optimal storage}},
	Year = {2017}}

@inproceedings{sammut92learning,
	Author = {Sammut, Claude and Hurst, Scott and Kedzier, Dana and Michie, Donald},
	Pages = {385--393},
	Title = {{Learning to Fly}}}

@article{schedlbauer2009what,
	Abstract = {Alerts and prompts represent promising types of decision support in
electronic prescribing to tackle inadequacies in prescribing. A systematic
review was conducted to evaluate the efficacy of computerized drug
alerts and prompts searching EMBASE, CINHAL, MEDLINE, and PsychINFO
up to May 2007. Studies assessing the impact of electronic alerts
and prompts on clinicians' prescribing behavior were selected and
categorized by decision support type. Most alerts and prompts (23
out of 27) demonstrated benefit in improving prescribing behavior
and/or reducing error rates. The impact appeared to vary based on
the type of decision support. Some of these alerts (n = 5) reported
a positive impact on clinical and health service management outcomes.
For many categories of reminders, the number of studies was very
small and few data were available from the outpatient setting. None
of the studies evaluated features that might make alerts and prompts
more effective. Details of an updated search run in Jan 2009 are
included in the supplement section of this review.},
	Author = {Schedlbauer, Angela and Prasad, Vibhore and Mulvaney, Caroline and Phansalkar, Shobha and Stanton, Wendy and Bates, David W and Avery, Anthony J},
	Doi = {10.1197/jamia.M2910},
	Institution = {Division of Primary Care, School of Community Health Sciences, Research and Learning Resources Division, Information Services, University of Nottingham, Nottingham, UK. angela.schedlbauer@nottingham.ac.uk},
	Journal = {J Am Med Inform Assoc},
	Keywords = {Clinical Competence; Decision Support Systems,Clinical; Drug Therapy,Computer-Assisted; Electronic Prescribing; Humans,prevention /{\&}/ control; Medication Systems; Remin},
	Number = {4},
	Pages = {531--538},
	Pmid = {19390110},
	Title = {{What evidence supports the use of computerized alerts and prompts to improve clinicians' prescribing behavior?}},
	Url = {http://dx.doi.org/10.1197/jamia.M2910},
	Volume = {16},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1197/jamia.M2910}}

@article{drineas2011fast,
	Abstract = {The statistical leverage scores of a matrix {\$}A{\$} are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr$\backslash$"{\{}o{\}}m-based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary {\$}n \backslashtimes d{\$} matrix {\$}A{\$}, with {\$}n \backslashgg d{\$}, and that returns as output relative-error approximations to all {\$}n{\$} of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of {\$}n{\$} and {\$}d{\$}) in {\$}O(n d \backslashlog n){\$} time, as opposed to the {\$}O(nd{\^{}}2){\$} time required by the na$\backslash$"{\{}i{\}}ve algorithm that involves computing an orthogonal basis for the range of {\$}A{\$}. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with {\$}n \backslashapprox d{\$}, and the extension to streaming environments.},
	Author = {Drineas, Petros and Magdon-Ismail, Malik and Mahoney, Michael W and Woodruff, David P.},
	Journal = {International Conference on Machine Learning},
	Keywords = {matrix coherence,randomized algorithm,statistical leverage},
	Title = {{Fast approximation of matrix coherence and statistical leverage}},
	Year = {2012}}

@inproceedings{allenberg2006hannan,
	Author = {Allenberg, Chamy and Auer, Peter and Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and Ottucs{\'{a}}k, Gy{\"{o}}rgy},
	Booktitle = {Algorithmic Learning Theory},
	Title = {{Hannan consistency in on-line learning in case of unbounded losses under partial monitoring}},
	Year = {2006}}

@book{scholkopf2001learning,
	Author = {Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J.},
	Publisher = {MIT Press},
	Title = {{Learning with kernels: Support vector machines, regularization, optimization, and beyond}},
	Year = {2001}}

@article{boros2002pseudo,
	Author = {Boros, E and Hammer, P L},
	Journal = {Discrete Applied Mathematics},
	Number = {1-3},
	Pages = {155--225},
	Publisher = {Elsevier},
	Title = {{Pseudo-{\{}B{\}}oolean optimization}},
	Volume = {123},
	Year = {2002}}

@article{narasimhan2006q,
	Author = {Narasimhan, M and Jojic, N and Bilmes, J},
	Journal = {Adv. NIPS},
	Title = {{Q-clustering}},
	Volume = {18},
	Year = {2006}}

@inproceedings{ghashami2016streaming,
	Abstract = {Kernel principal component analysis (KPCA) provides a concise set of basis vectors which capture non-linear structures within large data sets, and is a central tool in data analysis and learning. To allow for non-linear relations, typically a full {\$}n \backslashtimes n{\$} kernel matrix is constructed over {\$}n{\$} data points, but this requires too much space and time for large values of {\$}n{\$}. Techniques such as the Nystr$\backslash$"om method and random feature maps can help towards this goal, but they do not explicitly maintain the basis vectors in a stream and take more space than desired. We propose a new approach for streaming KPCA which maintains a small set of basis elements in a stream, requiring space only logarithmic in {\$}n{\$}, and also improves the dependence on the error parameter. Our technique combines together random feature maps with recent advances in matrix sketching, it has guaranteed spectral norm error bounds with respect to the original kernel matrix, and it compares favorably in practice to state-of-the-art approaches.},
	Author = {Ghashami, Mina and Perry, Daniel and Phillips, Jeff M.},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{Streaming kernel principal component analysis}},
	Year = {2016}}

@book{davey2002introduction,
	Author = {Davey, B A and Priestley, H A},
	Publisher = {Cambridge Univ. Press},
	Title = {{Introduction to Lattices and Order}},
	Year = {2002}}

@inproceedings{shalev07pegasos,
	Author = {Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
	Doi = {http://doi.acm.org/10.1145/1273496.1273598},
	Pages = {807--814},
	Title = {{{\{}P{\}}egasos: {\{}P{\}}rimal {\{}E{\}}stimated sub-{\{}G{\}}r{\{}A{\}}dient {\{}SO{\}}lver for {\{}SVM{\}}}},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1273496.1273598}}

@inproceedings{bartlett2009regal,
	Address = {Arlington, Virginia, United States},
	Author = {Bartlett, Peter L and Tewari, Ambuj},
	Booktitle = {Proceedings of the 25th conference on Uncertainty in Artificial Intelligence},
	Isbn = {978-0-9749039-5-8},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {35--42},
	Publisher = {AUAI Press},
	Series = {UAI '09},
	Title = {{REGAL: a regularization based algorithm for reinforcement learning in weakly communicating MDPs}},
	Year = {2009}}

@inproceedings{grill2015black-box,
	Abstract = {We study the problem of black-box optimization of a function f of any dimension, given function evaluations perturbed by noise. The function is assumed to be locally smooth around one of its global optima, but this smoothness is unknown. Our contribution is an adaptive optimization algorithm, POO or parallel optimistic optimization, that is able to deal with this setting. POO performs almost as well as the best known algorithms requiring the knowledge of the smoothness. Furthermore, POO works for a larger class of functions than what was previously considered, especially for functions that are difficult to optimize, in a very precise sense. We provide a finite-time analysis of POO's performance, which shows that its error after n evaluations is at most a factor of sqrt(ln n) away from the error of the best known optimization algorithms using the knowledge of the smoothness.},
	Author = {Grill, Jean-Bastien and Valko, Michal and Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Grill, Valko, Munos - 2015 - Black-box optimization of noisy functions with unknown smoothness.pdf:pdf},
	Title = {{Black-box optimization of noisy functions with unknown smoothness}},
	Url = {https://papers.nips.cc/paper/5721-black-box-optimization-of-noisy-functions-with-unknown-smoothness.pdf},
	Year = {2015},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/5721-black-box-optimization-of-noisy-functions-with-unknown-smoothness.pdf}}

@book{borwein2006caa,
	Author = {Borwein, J M and Lewis, A S},
	Publisher = {Springer},
	Title = {{Convex Analysis and Nonlinear Optimization: Theory and Examples}},
	Year = {2006}}

@incollection{visweswaran2005instance-specific,
	Address = {Cambridge, MA},
	Annote = {comps{\_}models},
	Author = {Visweswaran, Shyam and Cooper, Gregory F},
	Booktitle = {Advances in Neural Information Processing Systems 17},
	Editor = {Saul, Lawrence K and Weiss, Yair and Bottou, L{\'{e}}on},
	Pages = {1449--1456},
	Publisher = {MIT Press},
	Title = {{Instance-Specific {\{}B{\}}ayesian Model Averaging for Classification}},
	Url = {http://books.nips.cc/papers/files/nips17/NIPS2004{\_}0482.pdf},
	Year = {2005},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips17/NIPS2004%7B%5C_%7D0482.pdf}}

@article{benini1999policy,
	Author = {Benini, Luca and Bogliolo, Alessandro and Paleologo, Giuseppe and Micheli, Giovanni De},
	Journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	Number = {6},
	Pages = {813--833},
	Title = {{Policy Optimization for Dynamic Power Management}},
	Volume = {18},
	Year = {1999}}

@inproceedings{gopalan2014thompson,
	Author = {Gopalan, Aditya and Mannor, Shie and Mansour, Yishay},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Thompson sampling for complex online problems}},
	Url = {http://proceedings.mlr.press/v32/gopalan14.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v32/gopalan14.pdf}}

@inproceedings{yan2009fast,
	Author = {Yan, Donghui and Huang, Ling and Jordan, Michael},
	Booktitle = {Proceedings of the 15th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	Title = {{Fast Approximate Spectral Clustering}},
	Year = {2009}}

@article{ripeanu2002mapping,
	Abstract = {We studied the topology and protocols of the public Gnutella network. Its substantial user base and open architecture make it a good large-scale, if uncontrolled, testbed. We captured the network's topology, generated traffic, and dynamic behavior to determine its connectivity structure and how well (if at all) Gnutella's overlay network topology maps to the physical Internet infrastructure. Our analysis of the network allowed us to evaluate costs and benefits of the peer-to-peer (P2P) approach and to investigate possible improvements that would allow better scaling and increased reliability in Gnutella and similar networks. A mismatch between Gnutella's overlay network topology and the Internet infrastructure has critical performance implications},
	Author = {Ripeanu, Matei and Iamnitchi, Adriana and Foster, Ian},
	Doi = {10.1109/4236.978369},
	Issn = {10897801},
	Journal = {IEEE Internet Computing},
	Number = {1},
	Pages = {50--57},
	Title = {{Mapping the Gnutella network}},
	Volume = {6},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/4236.978369}}

@inproceedings{bartletthigh,
	Author = {Bartlett, Peter L and Dani, Varsha and Hayes, Thomas P and Kakade, Sham M and Rakhlin, Alexander and Tewari, Ambuj},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {335--342},
	Title = {{High-probability Regret Bounds for Bandit Online Linear Optimization}}}

@inproceedings{zhu2009some,
	Address = {Piscataway, NJ, USA},
	Author = {Zhu, Xiaojin and Goldberg, Andrew B and Khot, Tushar},
	Booktitle = {ICME'09: Proceedings of the 2009 IEEE international conference on Multimedia and Expo},
	Isbn = {978-1-4244-4290-4},
	Pages = {1504--1507},
	Publisher = {IEEE Press},
	Title = {{Some new directions in graph-based semi-supervised learning}},
	Year = {2009}}

@inproceedings{collins04perceptron,
	Address = {Morristown, NJ, USA},
	Author = {Collins, Michael and Roark, Brian},
	Booktitle = {ACL '04: Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics},
	Doi = {http://dx.doi.org/10.3115/1218955.1218970},
	Pages = {111--118},
	Publisher = {Association for Computational Linguistics},
	Title = {{Incremental parsing with the perceptron algorithm}},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.3115/1218955.1218970}}

@inproceedings{Jenatton2010,
	Author = {Jenatton, R and Obozinski, G and Bach, F},
	Booktitle = {Proc. AISTATS},
	Title = {{Structured sparse principal component analysis}},
	Year = {2009}}

@book{alpern2013search,
	Author = {Alpern, Steve and Fokkink, Robbert and Gasieniec, L and Lindelauf, Roy and Subrahmanian, V S},
	Publisher = {Springer},
	Title = {{Search theory}},
	Url = {https://www.springer.com/fr/book/9781461468240},
	Year = {2013},
	Bdsk-Url-1 = {https://www.springer.com/fr/book/9781461468240}}

@inproceedings{charniak05ctf,
	Address = {Morristown, NJ, USA},
	Author = {Charniak, Eugene and Johnson, Mark},
	Booktitle = {ACL '05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics},
	Doi = {http://dx.doi.org/10.3115/1219840.1219862},
	Pages = {173--180},
	Publisher = {Association for Computational Linguistics},
	Title = {{Coarse-to-fine n-best parsing and {\{}M{\}}ax{\{}E{\}}nt discriminative reranking}},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.3115/1219840.1219862}}

@inproceedings{neu10o-mdp,
	Author = {Neu, G and Gy{\"{o}}rgy, A and Szepesv{\'{a}}ri, $\backslash$relax Cs. and Antos, A},
	Pages = {1804--1812},
	Title = {{Online {\{}M{\}}arkov Decision Processes under Bandit Feedback}}}

@article{karlin1994competitive,
	Author = {Karlin, Anna and Manasse, Mark and McGeoch, Lyle and Owicki, Susan},
	Journal = {Algorithmica},
	Number = {6},
	Pages = {542--571},
	Title = {{Competitive Randomized Algorithms for Nonuniform Problems}},
	Volume = {11},
	Year = {1994}}

@inproceedings{calandriello2018improved,
	Abstract = {The representation and learning benefits of methods based on graph Laplacians, such as Laplacian smoothing or harmonic function solution for semi-supervised learning (SSL), are empirically and theoretically well supported. Nonetheless, the exact versions of these methods scale poorly with the number of nodes n of the graph. In this paper, we combine a spectral sparsification routine with Laplacian learning. Given a graph G as input, our algorithm computes a sparsifier in a distributed way in O(nlog3(n)) time, O(mlog3(n)) work and O(nlog(n)) memory, using only log(n) rounds of communication. Furthermore, motivated by the regularization often employed in learning algorithms, we show that constructing sparsifiers that preserve the spectrum of the Laplacian only up to the regularization level may drastically reduce the size of the final graph. By constructing a spectrally-similar graph, we are able to bound the error induced by the sparsification for a variety of downstream tasks (e.g., SSL). We empirically validate the theoretical guarantees on Amazon co-purchase graph and compare to the state-of-the-art heuristics.},
	Author = {Calandriello, Daniele and Koutis, Ioannis and Lazaric, Alessandro and Valko, Michal},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Improved large-scale graph learning through ridge spectral sparsification}},
	Year = {2018}}

@article{shi2000normalized,
	Abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph},
	Author = {Shi, J and Malik, J},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Pages = {888--905},
	Title = {{Normalized Cuts and Image Segmentation}},
	Volume = {22},
	Year = {2000}}

@article{Fre75,
	Author = {Freedman, D},
	Journal = {The Annals of Probability},
	Pages = {100--118},
	Title = {{On tail probabilities for martingales}},
	Volume = {3},
	Year = {1975}}

@article{mcpherson2001birds,
	Author = {McPherson, Miller and Smith-Lovin, Lynn and Cook, James},
	Journal = {Annual Review of Sociology},
	Pages = {415--444},
	Title = {{Birds of a feather: Homophily in social networks}},
	Url = {http://aris.ss.uci.edu/{~}lin/52.pdf},
	Volume = {27},
	Year = {2001},
	Bdsk-Url-1 = {http://aris.ss.uci.edu/%7B~%7Dlin/52.pdf}}

@inproceedings{lois2015online,
	Abstract = {This work studies two interrelated problems - online robust PCA (RPCA) and online low-rank matrix completion (MC). In recent work by Cand$\backslash$`{\{}e{\}}s et al., RPCA has been defined as a problem of separating a low-rank matrix (true data), {\$}L:=[\backslashell{\_}1, \backslashell{\_}2, \backslashdots \backslashell{\_}{\{}t{\}}, \backslashdots , \backslashell{\_}{\{}t{\_}{\{}\backslashmax{\}}{\}}]{\$} and a sparse matrix (outliers), {\$}S:=[x{\_}1, x{\_}2, \backslashdots x{\_}{\{}t{\}}, \backslashdots, x{\_}{\{}t{\_}{\{}\backslashmax{\}}{\}}]{\$} from their sum, {\$}M:=L+S{\$}. Our work uses this definition of RPCA. An important application where both these problems occur is in video analytics in trying to separate sparse foregrounds (e.g., moving objects) and slowly changing backgrounds. While there has been a large amount of recent work on both developing and analyzing batch RPCA and batch MC algorithms, the online problem is largely open. In this work, we develop a practical modification of our recently proposed algorithm to solve both the online RPCA and online MC problems. The main contribution of this work is that we obtain correctness results for the proposed algorithms under mild assumptions. The assumptions that we need are: (a) a good estimate of the initial subspace is available (easy to obtain using a short sequence of background-only frames in video surveillance); (b) the {\$}\backslashell{\_}t{\$}'s obey a `slow subspace change' assumption; (c) the basis vectors for the subspace from which {\$}\backslashell{\_}t{\$} is generated are dense (non-sparse); (d) the support of {\$}x{\_}t{\$} changes by at least a certain amount at least every so often; and (e) algorithm parameters are appropriately set},
	Archiveprefix = {arXiv},
	Arxivid = {1503.03525},
	Author = {Lois, Brian and Vaswani, Namrata},
	Booktitle = {IEEE International Symposium on Information Theory},
	Eprint = {1503.03525},
	Title = {{Online Matrix Completion and Online Robust PCA}},
	Year = {2015}}

@inproceedings{cesa-bianchi2013online,
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Dekel, Ofer and Shamir, Ohad},
	Booktitle = {Advances in Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Cesa-Bianchi, Dekel, Shamir - 2013 - Online Learning with Switching Costs and Other Adaptive Adversaries.pdf:pdf},
	Pages = {1160--1168},
	Title = {{Online Learning with Switching Costs and Other Adaptive Adversaries}},
	Url = {http://papers.nips.cc/paper/5151-online-learning-with-switching-costs-and-other-adaptive-adversaries},
	Year = {2013},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5151-online-learning-with-switching-costs-and-other-adaptive-adversaries}}

@incollection{combettes2010proximal,
	Author = {Combettes, P L and Pesquet, J C},
	Chapter = {Proximal S},
	Publisher = {New York: Springer-Verlag},
	Title = {{Fixed-Point Algorithms for Inverse Problems in Science and Engineering}},
	Year = {2010}}

@article{Chen1998,
	Author = {Chen, S S and Donoho, D L and Saunders, M A},
	Journal = {SIAM Journal on Scientific Computing},
	Number = {1},
	Pages = {33--61},
	Title = {{Atomic Decomposition by Basis Pursuit}},
	Volume = {20},
	Year = {1998}}

@article{TW03,
	Author = {Takimoto, E and Warmuth, M},
	Journal = {Journal of Machine Learning Research},
	Pages = {773--818},
	Title = {{Paths kernels and multiplicative updates}},
	Volume = {4},
	Year = {2003}}

@inproceedings{feng2004dynamic,
	Author = {Feng, Zhengzhu and Dearden, Richard and Meuleau, Nicolas and Washington, Richard},
	Booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
	Pages = {154--161},
	Title = {{Dynamic Programming for Structured Continuous {\{}Markov{\}} Decision Problems}},
	Year = {2004}}

@inproceedings{koolen2013pareto,
	Author = {{Wouter M. Koolen}},
	Booktitle = {Neural Information Processing Systems},
	Title = {{The Pareto regret frontier}},
	Year = {2013}}

@inproceedings{ding2013multi-armed,
	Abstract = {We study the multi-armed bandit problems with budget constraint and variable costs (MAB-BV). In this setting, pulling an arm will receive a random reward together with a random cost, and the objective of an algorithm is to pull a sequence of arms in order to maximize the expected total reward with the costs of pulling those arms complying with a budget constraint. This new setting models many Internet applications (e.g., ad exchange, sponsored search, and cloud computing) in a more accurate manner than previous settings where the pulling of arms is either costless or with a fixed cost.We propose two UCB based algorithms for the new setting. The first algorithm needs prior knowledge about the lower bound of the expected costs when computing the exploration term. The second algorithm eliminates this need by estimating the minimal expected costs from empirical observations, and therefore can be applied to more real-world applications where pri- or knowledge is not available.We prove that both algorithms have nice learning abilities, with regret bounds of O(lnB). Furthermore, we show that when applying our proposed algorithms to a previous setting with fixed costs (which can be regarded as our special case), one can improve the previously obtained regret bound. Our simulation results on real-time bidding in ad exchange verify the effectiveness of the algorithms and are consistent with our theoretical analysis},
	Author = {Ding, Wenkui and Qin, Tao and Zhang, Xu-dong and Liu, Tie-yan},
	Booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
	Isbn = {9781577356158},
	Title = {{Multi-Armed Bandit with Budget Constraint and Variable Costs}},
	Url = {http://dblp.uni-trier.de/db/conf/aaai/aaai2013.html{\#}DingQZL13},
	Year = {2013},
	Bdsk-Url-1 = {http://dblp.uni-trier.de/db/conf/aaai/aaai2013.html%7B%5C#%7DDingQZL13}}

@book{hansen2004global,
	Author = {Hansen, Eldon and Walster, William},
	Isbn = {9780824740597},
	Publisher = {Marcel Dekker},
	Series = {Pure and Applied Mathematics Series},
	Title = {{Global Optimization Using Interval Analysis: Revised and Expanded}},
	Url = {http://books.google.fr/books?id=tY2wAkb-zLcC},
	Year = {2004},
	Bdsk-Url-1 = {http://books.google.fr/books?id=tY2wAkb-zLcC}}

@inproceedings{audiffren2014messi,
	Abstract = {A popular approach to apprenticeship learning (AL) is to formulate it as an inverse reinforcement learning (IRL) problem. The MaxEnt-IRL algorithm successfully integrates the maximum entropy principle into IRL and unlike its predecessors, it resolves the ambiguity arising from the fact that a possibly large number of policies could match the expert's behavior. In this paper, we study an AL setting in which in addition to the expert's trajectories, a number of unsupervised trajectories is available. We introduce MESSI, a novel algorithm that combines MaxEnt-IRL with principles coming from semi-supervised learning. In particular, MESSI integrates the unsupervised data into the MaxEnt-IRL framework using a pairwise penalty on trajectories. Empirical results in a highway driving and grid-world problems indicate that MESSI is able to take advantage of the unsupervised trajectories and improve the performance of MaxEnt-IRL.},
	Author = {Audiffren, Julien and Valko, Michal and Lazaric, Alessandro and Ghavamzadeh, Mohammad},
	Booktitle = {NIPS Workshop on Novel Trends and Applications in Reinforcement Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Audiffren et al. - 2014 - MESSI Maximum entropy semi-supervised inverse reinforcement learning.pdf:pdf},
	Title = {{MESSI: Maximum entropy semi-supervised inverse reinforcement learning}},
	Year = {2014}}

@incollection{scholkopf1999kernel,
	Author = {Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J. and M{\"{u}}ller, Klaus-Robert},
	Booktitle = {Advances in kernel methods},
	Pages = {327--352},
	Publisher = {MIT Press Cambridge, MA, USA},
	Title = {{Kernel principal component analysis}},
	Year = {1999}}

@inproceedings{edmonds,
	Author = {Edmonds, J},
	Booktitle = {Combinatorial optimization - Eureka, you shrink!},
	Pages = {11--26},
	Publisher = {Springer},
	Title = {{Submodular functions, matroids, and certain polyhedra}},
	Year = {2003}}

@inproceedings{maes07sequencelabeling,
	Author = {Maes, Francis and Denoyer, Ludovic and Gallinari, Patrick},
	Pages = {648--657},
	Title = {{Sequence Labeling with Reinforcement Learning and Ranking Algorithms}}}

@phdthesis{osborne2010bayesian,
	Author = {Osborne, Michael},
	School = {University of Oxford},
	Title = {{Bayesian Gaussian processes for sequential prediction, optimisation and quadrature}},
	Year = {2010}}

@book{chapelle2010semi-supervised,
	Author = {Chapelle, Olivier and Schlkopf, Bernhard and Zien, Alexander},
	Publisher = {The MIT Press},
	Title = {{Semi-Supervised Learning}},
	Year = {2010}}

@article{bolton2002statistical,
	Abstract = {Summary: Fraud is increasing dramatically with the expansion of modern
technology and the global superhighways of communication, resulting
in the loss of billions of dollars worldwide each year. Although
prevention technologies are the best way to reduce fraud, fraudsters
are adaptive and, given time, will usually find ways to circumvent
such measures. Methodologies for the detection of fraud are essential
if we are to catch fraudsters once fraud prevention has failed. Statistics
and machine learning provide effective technologies for fraud detection
and have been applied successfully to detect activities such as money
laundering, e-commerce credit card fraud, telecommunications fraud
and computer intrusion, to name but a few. We describe the tools
available for statistical fraud detection and the areas in which
fraud detection technologies are most used.},
	Annote = {comps{\_}ano},
	Author = {Bolton, Richard J and Hand, David J},
	Doi = {doi:10.1214/ss/1042727940},
	Journal = {Stat. Sci.},
	Keywords = {fraud detection,fraud prevention,machine learnin},
	Number = {3},
	Pages = {235--255},
	Title = {{Statistical fraud detection: a review.}},
	Volume = {17},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1214/ss/1042727940}}

@inproceedings{sutton1996generalization,
	Author = {Sutton, Richard},
	Booktitle = {Advances in Neural Information Processing Systems 8},
	Pages = {1038--1044},
	Title = {{Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding}},
	Year = {1996}}

@article{Abernethy2008,
	Author = {Abernethy, Jacob and Berkeley, U C and Rakhlin, Alexander},
	Journal = {Online},
	Number = {3},
	Publisher = {Citeseer},
	Title = {{Competing in the Dark : An Efficient Algorithm for Bandit Linear Optimization}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.2096{\&}rep=rep1{\&}type=pdf},
	Volume = {3},
	Year = {2008},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.2096%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{ratliff2007boosting,
	Address = {Cambridge, MA},
	Author = {Ratliff, Nathan and Bradley, David and Bagnell, J Andrew (Drew) and Chestnutt, Joel},
	Booktitle = {Advances in Neural Information Processing Systems 19},
	Editor = {Sch{\"{o}}lkopf, B and Platt, J C and Hofmann, T},
	Publisher = {MIT Press},
	Title = {{Boosting Structured Prediction for Imitation Learning}},
	Url = {http://www-clmc.usc.edu/publications/B/bagnell-NIPS2006.pdf},
	Year = {2007},
	Bdsk-Url-1 = {http://www-clmc.usc.edu/publications/B/bagnell-NIPS2006.pdf}}

@inproceedings{bilenko2004integrating,
	Address = {New York, NY, USA},
	Annote = {comps{\_}distancX},
	Author = {Bilenko, Mikhail and Basu, Sugato and Mooney, Raymond J},
	Booktitle = {ICML '04: Proceedings of the twenty-first international conference on Machine learning},
	Doi = {http://doi.acm.org/10.1145/1015330.1015360},
	Isbn = {1-58113-828-5},
	Pages = {11},
	Publisher = {ACM},
	Title = {{Integrating constraints and metric learning in semi-supervised clustering}},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1015330.1015360}}

@article{hein2007graph,
	Author = {Hein, Matthias and Audibert, Jean-Yves and von Luxburg, Ulrike},
	Issn = {1532-4435},
	Journal = {J. Mach. Learn. Res.},
	Month = {dec},
	Pages = {1325--1370},
	Publisher = {JMLR.org},
	Title = {{Graph Laplacians and their Convergence on Random Neighborhood Graphs}},
	Url = {http://portal.acm.org/citation.cfm?id=1314498.1314544},
	Volume = {8},
	Year = {2007},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?id=1314498.1314544}}

@article{grotschel1981ellipsoid,
	Abstract = {L. G. Khachiyan recently published a polynomial algorithm to check feasibility of a system of linear inequalities. The method is an adaptation of an algorithm proposed by Shor for non-linear optimization problems. In this paper we show that the method also yields interesting results in combinatorial optimization. Thus it yields polynomial algorithms for vertex packing in perfect graphs; for the matching and matroid intersection problems; for optimum covering of directed cuts of a digraph; for the minimum value of a submodular set function; and for other important combinatorial problems. On the negative side, it yields a proof that weighted fractional chromatic number is NP-hard.},
	Author = {Gr{\"{o}}tschel, M and Lov{\'{a}}sz, L and Schrijver, A},
	Doi = {10.1007/BF02579273},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Gr{\"{o}}tschel, Lov{\'{a}}sz, Schrijver - 1981 - The ellipsoid method and its consequences in combinatorial optimization.pdf:pdf},
	Issn = {02099683},
	Journal = {Combinatorica},
	Number = {2},
	Pages = {169--197},
	Publisher = {Springer},
	Title = {{The ellipsoid method and its consequences in combinatorial optimization}},
	Url = {http://www.springerlink.com/index/10.1007/BF02579273},
	Volume = {1},
	Year = {1981},
	Bdsk-Url-1 = {http://www.springerlink.com/index/10.1007/BF02579273},
	Bdsk-Url-2 = {https://doi.org/10.1007/BF02579273}}

@inproceedings{Pol05,
	Author = {Poland, Jan},
	Booktitle = {In 3rd Symposium on Stochastic Algorithms, Foundations and Applications (SAGA'05)},
	Pages = {58--69},
	Title = {{{\{}FPL{\}} analysis for adaptive bandits}},
	Year = {2005}}

@article{rivas99dynamic,
	Address = {Department of Genetics, Washington University, St. Louis, MO, 63110, USA.},
	Author = {Rivas, E and Eddy, S R},
	Doi = {10.1006/jmbi.1998.2436},
	Issn = {0022-2836},
	Journal = {Journal of Molecular Biology},
	Keywords = {folding,pseudoknot,rna},
	Number = {5},
	Pages = {2053--2068},
	Title = {{A dynamic programming algorithm for {\{}RNA{\}} structure prediction including pseudoknots.}},
	Url = {http://dx.doi.org/10.1006/jmbi.1998.2436},
	Volume = {285},
	Year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1006/jmbi.1998.2436}}

@inproceedings{szummer2001partially,
	Abstract = {To classify a large number of unlabeled examples we combine a limited
number of labeled examples with a Markov random walk representation
over the unlabeled examples. The random walk representation exploits
any low dimensional structure in the data in a robust, probabilistic
manner. We develop and compare several estimation criteria/algorithms
suited to this representation. This includes in particular multi-way
classification with an average margin criterion which permits a closed
form...},
	Author = {Szummer, Martin and Jaakkola, Tommi},
	Booktitle = {Advances in Neural Information Processing Systems},
	Keywords = {classification,graph,machine-learning},
	Title = {{Partially labeled classification with Markov random walks}},
	Volume = {14},
	Year = {2001}}

@article{ShMe99,
	Author = {Shamir, G I and Merhav, N},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {1498--1519},
	Title = {{Low-complexity sequential lossless coding for piecewise-stationary memoryless sources}},
	Volume = {IT-45},
	Year = {1999}}

@inproceedings{HaSe09,
	Author = {Hazan, E and Seshadhri, C},
	Pages = {393--400},
	Title = {{Efficient learning algorithms for changing environments}}}

@incollection{JN11b,
	Author = {Juditsky, A and Nemirovski, A},
	Booktitle = {Optimization for Machine Learning},
	Editor = {Sra, S and Nowozin, S and Wright, S},
	Pages = {149--183},
	Publisher = {MIT press},
	Title = {{First-Order Methods for Nonsmooth Convex Large-Scale Optimization, II: Utilizing Problem's Structure}},
	Year = {2011}}

@book{Led01,
	Author = {Ledoux, M},
	Publisher = {American Mathematical Society},
	Title = {{The Concentration of Measure Phenomenon}},
	Year = {2001}}

@techreport{kapoor1996assessment,
	Author = {Kapoor, W N},
	Institution = {Agency for Health Policy and Research (AHCPR)},
	Title = {{Assessment of the Variantion and Outcomes of Pneumonia: Pneumonia Patient Outcomes Research Team ({\{}PORT{\}}) Final Report}},
	Year = {1996}}

@article{cornuejols1977uncapacitated,
	Author = {Cornuejols, G and Fisher, M and Nemhauser, G L},
	Journal = {Annals of Discrete Mathematics},
	Pages = {163--177},
	Publisher = {Elsevier},
	Title = {{On the Uncapacitated Location Problem}},
	Volume = {1},
	Year = {1977}}

@inproceedings{kveton2010semi--supervised,
	Abstract = {This paper proposes a novel algorithm for semisupervised learning. This algorithm learns graph cuts that maximize the margin with respect to the labels induced by the harmonic function solution. We motivate the approach, compare it to existing work, and prove a bound on its generalization error. The quality of our solutions is evaluated on a synthetic problem and three UCI ML repository datasets. In most cases, we outperform manifold regularization of support vector machines, which is a state-of-the-art approach to semi-supervised max-margin learning.},
	Author = {Kveton, Branislav and Valko, Michal and Rahimi, Ali and Huang, Ling},
	Booktitle = {Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)},
	Editor = {Teh, Y W and Titterington, M},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Number = {W{\&}CP 9},
	Pages = {421--428},
	Title = {{Semi-Supervised Learning with Max-Margin Graph Cuts}},
	Volume = {9},
	Year = {2010}}

@inproceedings{LaMu09,
	Author = {Lazaric, Alessandro and Munos, R{\'{e}}mi},
	Title = {{Hybrid Stochastic-Adversarial On-line Learning}}}

@misc{tran-thang2012knapsack,
	Author = {Tran-Thanh, Long and Chapman, Archie C. and Rogers, Alex and Jennings, Nicholas R.},
	Booktitle = {AAAI},
	Title = {{Knapsack Based Optimal Policies for Budget-Limited Multi-Armed Bandits.}},
	Urldate = {2014-10-16},
	Year = {2012}}

@article{chambolle2009total,
	Author = {Chambolle, A and Darbon, J},
	Journal = {International Journal of Computer Vision},
	Number = {3},
	Pages = {288--307},
	Publisher = {Springer},
	Title = {{On total variation minimization and surface evolution using parametric maximum flows}},
	Volume = {84},
	Year = {2009}}

@incollection{simunic2002dynamic,
	Address = {New York, NY},
	Author = {Simunic, Tajana},
	Booktitle = {Power Aware Computing},
	Publisher = {Kluwer Academic Publishers},
	Title = {{Dynamic Management of Power Consumption}},
	Year = {2002}}

@inproceedings{narasimhan2007local,
	Author = {Narasimhan, M and Bilmes, J},
	Booktitle = {Proc. IJCAI},
	Title = {{Local search for balanced submodular clusterings}},
	Year = {2007}}

@book{sutton1998reinforcement,
	Address = {Cambridge, MA},
	Author = {Sutton, Richard and Barto, Andrew},
	Publisher = {MIT Press},
	Title = {{Reinforcement Learning: An Introduction}},
	Url = {https://pdfs.semanticscholar.org/aa32/c33e7c832e76040edc85e8922423b1a1db77.pdf},
	Year = {1998},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/aa32/c33e7c832e76040edc85e8922423b1a1db77.pdf}}

@inproceedings{sun2012size,
	Author = {Sun, Yi and Schmidhuber, J{\"{u}}rgen and Gomez, Faustino J.},
	Booktitle = {International Conference on Machine Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Sun, Schmidhuber, Gomez - 2012 - On the Size of the Online Kernel Sparsification Dictionary.pdf:pdf},
	Title = {{On the size of the online kernel sparsification dictionary}},
	Year = {2012}}

@inproceedings{koutis2012improved,
	Author = {Koutis, Ioannis and Levin, Alex and Peng, Richard},
	Booktitle = {STACS'12 (29th Symposium on Theoretical Aspects of Computer Science)},
	Organization = {LIPIcs},
	Pages = {266--277},
	Title = {{Improved spectral sparsification and numerical algorithms for SDD matrices}},
	Volume = {14},
	Year = {2012}}

@inproceedings{smola2003kernels,
	Author = {Smola, A J and Kondor, R},
	Booktitle = {Proceedings of the Annual Conference on Computational Learning Theory and Kernel Workshop},
	Editor = {Sch{\"{o}}lkopf, B and Warmuth, M},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {{Kernels and Regularization on Graphs}},
	Year = {2003}}

@article{Bubeck2013,
	Abstract = {The stochastic multiarmed bandit problem is well understood when the reward distributions are sub-Gaussian. In this paper, we examine the bandit problem under the weaker assumption that the distributions have moments of order 1 + $\epsilon$, for some $\epsilon$ ∈ (0,1]. Surprisingly, moments of order 2 (i.e., finite variance) are sufficient to obtain regret bounds of the same order as under sub-Gaussian reward distributions. In order to achieve such regret, we define sampling strategies based on refined estimators of the mean such as the truncated empirical mean, Catoni's M-estimator, and the median-of-means estimator. We also derive matching lower bounds that also show that the best achievable regret deteriorates when $\epsilon$ {\textless}; 1. View full abstract},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1209.1727v1},
	Author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicol{\`{o}} and Lugosi, G{\'{a}}bor},
	Doi = {10.1109/TIT.2013.2277869},
	Eprint = {arXiv:1209.1727v1},
	Issn = {00189448},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Heavy-tailed distributions,regret bounds,robust estimators,stochastic multi-armed bandit},
	Number = {11},
	Pages = {7711--7717},
	Title = {{Bandits with heavy tail}},
	Volume = {59},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIT.2013.2277869}}

@inproceedings{kalenon,
	Author = {Kale, Satyen and Reyzin, Lev and Schapire, Robert E},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {1054--1062},
	Title = {{Non-Stochastic Bandit Slate Problems}}}

@inproceedings{melo2010analysis,
	Author = {Melo, F S and Lopes, M and Ferreira, R},
	Booktitle = {Proceedings of the 2010 conference on ECAI 2010: 19th European Conference on Artificial Intelligence},
	Organization = {IOS Press},
	Pages = {349--354},
	Title = {{Analysis of inverse reinforcement learning with perturbed demonstrations}},
	Url = {http://flowers.inria.fr/mlopes/myrefs/10-ecai.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://flowers.inria.fr/mlopes/myrefs/10-ecai.pdf}}

@article{ross2010reduction,
	Abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
	Author = {Ross, Stephane and Gordon, Geoffrey J and Bagnell, J Andrew},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ross, Gordon, Bagnell - 2010 - A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning.pdf:pdf},
	Journal = {AISTATS},
	Pages = {627--635},
	Title = {{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}},
	Url = {http://arxiv.org/abs/1011.0686},
	Volume = {15},
	Year = {2010},
	Bdsk-Url-1 = {http://arxiv.org/abs/1011.0686}}

@inproceedings{ghahramani2000graphical,
	Annote = {comps{\_}models},
	Author = {Ghahramani, Zoubin and Beal, Matthew J},
	Booktitle = {Advanced Mean Field Methods - Theory and Practice},
	Publisher = {MIT Press},
	Title = {{Graphical models and variational methods}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.7693},
	Year = {2000},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.7693}}

@misc{leskovec2014snapnets,
	Author = {Leskovec, Jure and Krevl, Andrej},
	Howpublished = {http://snap.stanford.edu/data},
	Month = {jun},
	Title = {{SNAP datasets: Stanford large network dataset collection}},
	Year = {2014}}

@book{pinter1995global,
	Author = {Pint{\'{e}}r, J{\'{a}}nos},
	Isbn = {9780792337577},
	Publisher = {Springer},
	Series = {Nonconvex Optimization and Its Applications},
	Title = {{Global Optimization in Action: Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications}},
	Url = {http://books.google.fr/books?id=G8pF982ckNsC},
	Year = {1995},
	Bdsk-Url-1 = {http://books.google.fr/books?id=G8pF982ckNsC}}

@book{Mas06,
	Author = {Massart, P},
	Publisher = {Springer},
	Title = {{Ecole d'Ete de Probabilites de Saint-Flour XXXIII - 2003}},
	Year = {2006}}

@techreport{garivier2017thresholding,
	Abstract = {We analyze the sample complexity of the thresholding bandit problem, with and without the assumption that the mean values of the arms are increasing. In each case, we provide a lower bound valid for any risk {\$}\backslashdelta{\$} and any {\$}\backslashdelta{\$}-correct algorithm; in addition, we propose an algorithm whose sample complexity is of the same order of magnitude for small risks. This work is motivated by phase 1 clinical trials, a practically important setting where the arm means are increasing by nature, and where no satisfactory solution is available so far.},
	Archiveprefix = {arXiv},
	Arxivid = {1711.04454},
	Author = {Garivier, Aur{\'{e}}lien and M{\'{e}}nard, Pierre and Rossi, Laurent},
	Eprint = {1711.04454},
	Month = {nov},
	Title = {{Thresholding Bandit for Dose-ranging: The Impact of Monotonicity}},
	Url = {http://arxiv.org/abs/1711.04454},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1711.04454}}

@inproceedings{Kocak2014,
	Abstract = {Copyright {\textcopyright} 2014, Association for the Advancement of Artificial Intelligence. Thompson Sampling (TS) has surged a lot of interest due to its good empirical performance, in particular in the computational advertising. Though successful, the tools for its performance analysis appeared only recently. In this paper, we describe and analyze SpectralTS algorithm for a bandit problem, where the payoffs of the choices are smooth given an underlying graph. In this setting, each choice is a node of a graph and the expected payoffs of the neighboring nodes are assumed to be similar. Although the setting has application both in recommender systems and advertising, the traditional algorithms would scale poorly with the number of choices. For that purpose we consider an effective dimension d, which is small in real-world graphs. We deliver the analysis showing that the regret of SpectralTS scales as d√T InN with high probability, where T is the time horizon and N is the number of choices. Since a d√T InNregret is comparable to the known results, SpectralTS offers a computationally more efficient alternative. We also show that our algorithm is competitive on both synthetic and real-world data.},
	Author = {Koc{\'{a}}k, T. and Munos, R. and Valko, M. and Agrawal, S.},
	Booktitle = {AAAI Conference on Artificial Intelligence},
	Isbn = {9781577356790},
	Title = {{Spectral thompson sampling}},
	Volume = {3},
	Year = {2014}}

@book{bertsekas1995dynamic,
	Address = {Belmont, MA},
	Author = {Bertsekas, Dimitri},
	Publisher = {Athena Scientific},
	Title = {{Dynamic Programming and Optimal Control}},
	Year = {1995}}

@article{calvin2017adaptive,
	Author = {Calvin, James M. and Hefter, Mario and Herzwurm, Andr{\'{e}}},
	Journal = {Journal of Complexity},
	Month = {apr},
	Number = {C},
	Pages = {17--37},
	Publisher = {Academic Press, Inc.},
	Title = {{Adaptive approximation of the minimum of Brownian motion}},
	Url = {https://arxiv.org/pdf/1601.01276.pdf},
	Volume = {39},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1601.01276.pdf}}

@inproceedings{daniel2012hreps,
	Author = {Daniel, C and Neumann, G and Peters, J},
	Booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
	Pages = {273--281},
	Series = {JMLR Workshop and Conference Proceedings},
	Title = {{Hierarchical Relative Entropy Policy Search}},
	Volume = {22},
	Year = {2012}}

@techreport{carpentier2015uncertainty,
	Abstract = {We construct minimax optimal non-asymptotic confidence sets for low rank matrix recovery algorithms such as the Matrix Lasso or Dantzig selector. These are employed to devise adaptive sequential sampling procedures that guarantee recovery of the true matrix in Frobenius norm after a data-driven stopping time {\$}\backslashhat n{\$} for the number of measurements that have to be taken. With high probability, this stopping time is minimax optimal. We detail applications to quantum tomography problems where measurements arise from Pauli observables. We also give a theoretical construction of a confidence set for the density matrix of a quantum state that has optimal diameter in nuclear norm. The non-asymptotic properties of our confidence sets are further investigated in a simulation study.},
	Archiveprefix = {arXiv},
	Arxivid = {1504.03234},
	Author = {Carpentier, Alexandra and Eisert, Jens and Gross, David and Nickl, Richard},
	Eprint = {1504.03234},
	Title = {{Uncertainty quantification for matrix compressed sensing and quantum tomography problems}},
	Year = {2015}}

@inproceedings{keshavan2009matrix,
	Author = {Keshavan, Raghunandan and Oh, Sewoong and Montanari, Andrea},
	Booktitle = {International Symposium on Information Theory},
	Title = {{Matrix completion from a few entries}},
	Url = {https://arxiv.org/pdf/0901.3150.pdf},
	Year = {2009},
	Bdsk-Url-1 = {https://arxiv.org/pdf/0901.3150.pdf}}

@inproceedings{drugan2013designing,
	Author = {Drugan, Madalina and Nowe, Ann},
	Booktitle = {International Joint Conference on Neural Networks},
	Title = {{Designing multi-objective multi-armed bandits algorithms: a study}},
	Year = {2013}}

@inproceedings{kocak2016online,
	Abstract = {We propose a new partial-observability model for online learning problems where the learner, besides its own loss, also observes some noisy feedback about the other actions, depending on the underlying structure of the problem. We represent this structure by a weighted directed graph, where the edge weights are related to the quality of the feedback shared by the connected nodes. Our main contribution is an efficient algorithm that guarantees a regret of O(sqrt(alpha{\^{}}* T) after T rounds, where alpha{\^{}}* is a novel graph property that we call the effective independence number. Our algorithm is completely parameter-free and does not require knowledge (or even estimation) of alpha{\^{}}*. For the special case of binary edge weights, our setting reduces to the partial-observability models of Mannor {\&} Shamir (2011) and Alon et al. (2013) and our algorithm recovers the near-optimal regret bounds.},
	Author = {Koc{\'{a}}k, Tom{\'{a}}{\v{s}} and Neu, Gergely and Valko, Michal},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{Online learning with noisy side observations}},
	Url = {http://proceedings.mlr.press/v51/kocak16-supp.pdf},
	Year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v51/kocak16-supp.pdf}}

@article{fot,
	Annote = {To appear},
	Author = {Bach, F and Jenatton, R and Mairal, J and Obozinski, G},
	Journal = {Foundations and Trends{\{}$\backslash$textregistered{\}} in Machine Learning},
	Publisher = {Now Publishers Inc.},
	Title = {{Optimization with sparsity-inducing penalties}},
	Year = {2011}}

@article{erkan2009semi-supervised,
	Abstract = {Various supervised inference methods can be analyzed as convex duals of the generalized maximum entropy (MaxEnt) framework. Generalized MaxEnt aims to find a distribution that maximizes an entropy function while respecting prior information represented as potential functions in miscellaneous forms of constraints and/or penalties. We extend this framework to semi-supervised learning by incorporating unlabeled data via modifications to these potential functions reflecting structural assumptions on the data geometry. The proposed approach leads to a family of discriminative semi-supervised algorithms, that are convex, scalable, inherently multi-class, easy to implement, and that can be kernelized naturally. Experimental evaluation of special cases shows the competitiveness of our methodology.},
	Author = {Erkan, Ayse Naz and Altun, Yasemin},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Erkan, Altun - 2009 - Semi-Supervised Learning via Generalized Maximum Entropy.pdf:pdf},
	Journal = {Proceedings of JMLR Workshop},
	Keywords = {computational,information theoretic learning with statistics},
	Number = {September},
	Pages = {209--216},
	Publisher = {New York University},
	Title = {{Semi-Supervised Learning via Generalized Maximum Entropy}},
	Url = {http://eprints.pascal-network.org/archive/00006122/},
	Volume = {9},
	Year = {2009},
	Bdsk-Url-1 = {http://eprints.pascal-network.org/archive/00006122/}}

@article{subramanya2014graph,
	Author = {Subramanya, Amarnag and Talukdar, Partha Pratim},
	Journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	Number = {4},
	Pages = {1--125},
	Publisher = {Morgan {\&} Claypool Publishers},
	Title = {{Graph-Based Semi-Supervised Learning}},
	Volume = {8},
	Year = {2014}}

@article{megiddo1974optimal,
	Author = {Megiddo, N},
	Journal = {Mathematical Programming},
	Number = {1},
	Pages = {97--107},
	Publisher = {Springer},
	Title = {{Optimal flows in networks with multiple sources and sinks}},
	Volume = {7},
	Year = {1974}}

@article{Yasin2015,
	Abstract = {We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of {\$}K{\$} actions in response to the observed context, and observes the reward only for that chosen action. Our method assumes access to an oracle for solving fully supervised cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only {\$}\backslashtilde{\{}O{\}}(\backslashsqrt{\{}KT/\backslashlog N{\}}){\$} oracle calls across all {\$}T{\$} rounds, where {\$}N{\$} is the number of policies in the policy class we compete against. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines.},
	Archiveprefix = {arXiv},
	Arxivid = {cs/9605103},
	Author = {Yasin, M. A. and Al-Ashwal, W. A M and Shire, A. M. and Hamzah, S. A. and Ramli, K. N.},
	Doi = {10.1613/jair.301},
	Eprint = {9605103},
	Isbn = {0-7803-3213-X},
	Issn = {18196608},
	Journal = {ARPN Journal of Engineering and Applied Sciences},
	Keywords = {Bluetooth,GSM,PIFA,Tri-band},
	Number = {19},
	Pages = {8740--8744},
	Pmid = {17255001},
	Primaryclass = {cs},
	Title = {{Tri-band planar inverted F-antenna (PIFA) for GSM bands and bluetooth applications}},
	Volume = {10},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1613/jair.301}}

@inproceedings{oyallon2018compressing,
	Abstract = {We study the first-order scattering transform as a candidate for reducing the signal processed by a convolutional neural network (CNN). We show theoretical and empirical evidence that in the case of natural images and sufficiently small translation invariance, this transform preserves most of the signal information needed for classification while substantially reducing the spatial resolution and total signal size. We demonstrate that cascading a CNN with this representation performs on par with ImageNet classification models, commonly used in downstream tasks, such as the ResNet-50. We subsequently apply our trained hybrid ImageNet model as a base model on a detection system, which has typically larger image inputs. On Pascal VOC and COCO detection tasks we demonstrate improvements in the inference speed and training memory consumption compared to models trained directly on the input image.},
	Author = {Oyallon, Edouard and Belilovsky, Eugene and Zagoruyko, Sergey and Valko, Michal},
	Booktitle = {European Conference on Computer Vision},
	Title = {{Compressing the input for CNNs with the first-order scattering transform}},
	Year = {2018}}

@inproceedings{yang2012simple,
	Abstract = {In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. We also verify the effectiveness of the proposed algorithm by an empirical study.},
	Archiveprefix = {arXiv},
	Arxivid = {1206.6412},
	Author = {Ji, Ming and Yang, Tianbao and Lin, Binbin and Jin, Rong and Han, Jiawei},
	Booktitle = {International Conference on Machine Learning},
	Eprint = {1206.6412},
	Month = {jun},
	Title = {{A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound}},
	Url = {http://arxiv.org/abs/1206.6412},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1206.6412}}

@incollection{altun2006maximum,
	Address = {Cambridge, MA},
	Author = {Altun, Yasemin and McAllester, David and Belkin, Mikhail},
	Booktitle = {Advances in Neural Information Processing Systems 18},
	Editor = {Weiss, Y and Sch{\"{o}}lkopf, B and Platt, J},
	Pages = {33--40},
	Publisher = {MIT Press},
	Title = {{Maximum Margin Semi-Supervised Learning for Structured Variables}},
	Year = {2006}}

@article{kikuta1994initial,
	Author = {Kikuta, Kensaku and Ruckle, William H},
	Doi = {10.1002/1520-6750(199410)41:6<821::AID-NAV3220410610>3.0.CO;2-Y},
	Journal = {Naval Research Logistics},
	Number = {6},
	Pages = {821--831},
	Publisher = {Wiley Online Library},
	Title = {{Initial point search on weighted trees}},
	Volume = {41},
	Year = {1994},
	Bdsk-Url-1 = {https://doi.org/10.1002/1520-6750(199410)41:6%3C821::AID-NAV3220410610%3E3.0.CO;2-Y}}

@inproceedings{singh2008unlabeled,
	Author = {Singh, Aarti and Nowak, Robert D and Zhu, Xiaojin},
	Booktitle = {Advances in Neural Information Processing Systems 21},
	Title = {{Unlabeled data: Now it helps, now it doesn't}},
	Year = {2008}}

@inproceedings{elliott84gibbs,
	Author = {Elliott, H and Derin, H and Cristi, R and Geman, D},
	Booktitle = {Proceeding of the 1984 Int. Conf. Acoust., Speech, Signal Processing, ICASSP'84},
	Pages = {32.5.1----32.5.4},
	Title = {{Application of the {\{}G{\}}ibbs distribution to image segmentation}},
	Year = {1984}}

@article{pwc2009,
	Author = {PricewaterhouseCoopers{\~{}}(PWC)},
	Journal = {Tech. Report},
	Title = {{Global city {\{}GDP{\}} rankings 2008-2025}},
	Year = {2009}}

@inproceedings{erraqabi2016pliable,
	Abstract = {Rejection sampling is a known technique for sampling from difficult distributions. However, its use is limited due to a high rejection rate. Common adaptive rejection sampling methods either work for very specific distributions or without performance guarantees. In this paper, we present pliable rejection sampling (PRS), a new approach to rejection sampling, where we adapt the sampling envelope using a kernel estimator. Since our method builds on rejection sampling, the samples obtained are i.i.d. and exactly distributed according to f. Another benefit of PRS is that it comes with a guarantee on the number of accepted samples.},
	Author = {Erraqabi, Akram and Valko, Michal and Carpentier, Alexandra and Maillard, Odalric-Ambrym},
	Booktitle = {International Conference on Machine Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Erraqabi et al. - 2016 - Pliable rejection sampling.pdf:pdf},
	Title = {{Pliable rejection sampling}},
	Year = {2016}}

@article{narayanan1995rounding,
	Author = {Narayanan, H},
	Journal = {Linear algebra and its applications},
	Pages = {41--57},
	Publisher = {Elsevier},
	Title = {{A rounding technique for the polymatroid membership problem}},
	Volume = {221},
	Year = {1995}}

@inproceedings{boutilier1996approximating,
	Author = {Boutilier, Craig and Dearden, Richard},
	Booktitle = {Proceedings of the 13th International Conference on Machine Learning},
	Pages = {54--62},
	Title = {{Approximating Value Trees in Structured Dynamic Programming}},
	Year = {1996}}

@article{CBCoGe04,
	Author = {Cesa-Bianchi, N and Conconi, A and Gentile, C},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {2050--2057},
	Title = {{On the Generalization Ability of On-Line Learning Algorithms}},
	Volume = {50},
	Year = {2004}}

@incollection{klein2012inverse,
	Author = {Klein, Edouard and Geist, Matthieu and PIOT, BILAL and Pietquin, Olivier},
	Booktitle = {Advances in Neural Information Processing Systems 25},
	Editor = {Bartlett, P and Pereira, F C N and Burges, C J C and Bottou, L and Weinberger, K Q},
	Pages = {1016--1024},
	Title = {{Inverse Reinforcement Learning through Structured Classification}},
	Url = {http://books.nips.cc/papers/files/nips25/NIPS2012{\_}0491.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips25/NIPS2012%7B%5C_%7D0491.pdf}}

@inproceedings{narasimhan2015learnability,
	Author = {Narasimhan, Harikrishna and Parkes, David C. and Singer, Yaron},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Narasimhan, Parkes, Singer - 2015 - Learnability of influence in networks.pdf:pdf},
	Title = {{Learnability of influence in networks}},
	Year = {2015}}

@article{groenevelt1991two,
	Author = {Groenevelt, H},
	Journal = {European Journal of Operational Research},
	Number = {2},
	Pages = {227--236},
	Publisher = {Elsevier},
	Title = {{Two algorithms for maximizing a separable concave function over a polymatroid feasible region}},
	Volume = {54},
	Year = {1991}}

@inproceedings{aktolga2010detecting,
	Author = {Aktolga, Elif and Ros, Irene and Assogba, Yannick},
	Booktitle = {Proceedings of SIGIR},
	Title = {{Detecting Outlier Sections in US Congressional Legislation}},
	Type = {IR},
	Year = {2010}}

@article{zong2016cascading,
	Author = {Zong, Shi and Ni, Hao and Sung, Kenny and Ke, Nan Rosemary and Wen, Zheng and Kveton, Branislav},
	Journal = {arXiv preprint arXiv:1603.05359},
	Title = {{Cascading Bandits for Large-Scale Recommendation Problems}},
	Year = {2016}}

@article{Pis75,
	Author = {Pisier, G},
	Journal = {Israel Journal of Mathematics},
	Pages = {326--350},
	Title = {{Martingales with values in uniformly convex spaces}},
	Volume = {20},
	Year = {1975}}

@article{chandola2009anomaly,
	Address = {New York, NY, USA},
	Author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
	Doi = {http://doi.acm.org/10.1145/1541880.1541882},
	Issn = {0360-0300},
	Journal = {ACM Comput. Surv.},
	Keywords = {Anomaly detection,outlier detection},
	Month = {jul},
	Number = {3},
	Pages = {15:1--15:58},
	Publisher = {ACM},
	Title = {{Anomaly detection: A survey}},
	Volume = {41},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1541880.1541882}}

@inproceedings{neu12o-ssp,
	Author = {Neu, G and Gy{\"{o}}rgy, A and Szepesv{\'{a}}ri, $\backslash$relax Cs. $\backslash$textCs},
	Booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)},
	Pages = {231--243},
	Title = {{The Online Loop-free Stochastic Shortest-Path Problem}},
	Year = {2013}}

@inproceedings{silva,
	Author = {da Silva, V F and Costa, A H R and Lima, P},
	Pages = {4246--4251},
	Title = {{Inverse Reinforcement Learning with Evaluation}}}

@article{streeter2006simple,
	Author = {Streeter, Matthew J. and Smith, Stephen F.},
	Isbn = {3-540-46267-8},
	Journal = {Principles and Practice of Constraint Programming},
	Keywords = {dblp},
	Pages = {560--574},
	Title = {{A Simple Distribution-Free Approach to the Max k-Armed Bandit Problem.}},
	Url = {http://dblp.uni-trier.de/db/conf/cp/cp2006.html{\#}StreeterS06},
	Volume = {4204},
	Year = {2006},
	Bdsk-Url-1 = {http://dblp.uni-trier.de/db/conf/cp/cp2006.html%7B%5C#%7DStreeterS06}}

@inproceedings{lussdecomposing,
	Author = {Luss, R and Rosset, S and Shahar, M},
	Booktitle = {Adv. NIPS},
	Title = {{Decomposing Isotonic Regression for Efficiently Solving Large Problems}},
	Volume = {23},
	Year = {2010}}

@inproceedings{klimt2004introducing,
	Abstract = {A large set of email messages, the Enron corpus, was made public during the legal investigation concerning the Enron corporation. This dataset, along with a thorough explanation of its origin, is available at http://www-2.cs.cmu.edu/{\~{}}enron/. This paper provides a brief introduction and analysis of the dataset. The raw Enron corpus contains 619,446 messages belonging to 158 users. We cleaned the corpus before this analysis by removing certain folders from each user, such as discussionthreads. These folders were present for most users, and did not appear to be used directly by the users, but rather were computer generated. Many, such as alldocuments, also contained large numbers of duplicate emails, which were already present in the users other folders. Our goal in this paper is to analyze the suitability of this corpus for exploring how to classify messages as organized by a human, so these folders would have likely been misleading.},
	Author = {Klimt, Bryan and Yang, Yiming},
	Booktitle = {Collaboration, Electronic messaging, Anti-Abuse and Spam Conference},
	Title = {{Introducing the Enron corpus}},
	Year = {2004}}

@article{daniel1973stability,
	Author = {Daniel, James},
	Journal = {Mathematical Programming},
	Pages = {41--53},
	Title = {{Stability of the solution of definite quadratic programs}},
	Volume = {5},
	Year = {1973}}

@inproceedings{valko2010feature,
	Abstract = {The objective of this paper is to understand what characteris-tics
and features of clinical data influence physician.s deci-sion about
ordering laboratory tests or prescribing medica-tions the most. We
conduct our analysis on data and decisions extracted from electronic
health records of 4486 post-surgical cardiac patients. The summary
statistics for 335 different lab order decisions and 407 medication
decisions are reported. We show that in many cases, physician.s lab-order
and medication decisions are predicted well by simple patterns such
as last value of a single test result, time since a certain lab test
was ordered or time since certain procedure was executed.},
	Author = {Valko, Michal and Hauskrecht, Milos},
	Booktitle = {13th International Congress on Medical Informatics MEDINFO 2010},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Title = {{Feature importance analysis for patient management decisions}},
	Year = {2010}}

@techreport{wainwright2003graphical,
	Abstract = {The formalism of probabilistic graphical models provides a unifying
framework for the development of large-scale multivariate statistical
models. Graphical models have become a focus of research in many
applied statistical and computational fields, including bioinformatics,
information theory, signal and image processing, information retrieval
and machine learning. Many problems that arise in specific instances�including
the key problems of computing marginals and modes of probability
distributions�are best studied in the general setting. Working with
exponential family representations, and exploiting the conjugate
duality between the cumulant generating function and the entropy
for exponential families, we develop general variational representations
of the problems of computing marginal probabilities and modes. We
describe how a wide variety of known computational algorithms�including
mean field methods and cluster variational techniques�can be understood
in terms of approximations of these variational representations.
We also present novel convex relaxations based on the variational
framework. The variational approach provides a complementary alternative
to Markov chain Monte Carlo as a general source of approximation
methods for inference in large-scale statistical models.},
	Annote = {comps{\_}models},
	Author = {Wainwright, Martin J and Jordan, Michael I},
	Howpublished = {Technical Report 649},
	Institution = {Dept. of Statistics},
	Keywords = {duality},
	Month = {sep},
	Title = {{Graphical models, exponential families, and variational inference}},
	Url = {http://www.eecs.berkeley.edu/{~}wainwrig/Papers/WaiJorVariational03.pdf},
	Year = {2003},
	Bdsk-Url-1 = {http://www.eecs.berkeley.edu/%7B~%7Dwainwrig/Papers/WaiJorVariational03.pdf}}

@article{Graham1979,
	Abstract = {The theory of deterministic sequencing and scheduling has expanded rapidly during the past years. In this paper we survey the state of the art with respect to optimization and approximation algorithms and interpret these in terms of computational complexity theory. Special cases considered are single machine scheduling, identical, uniform and unrelated parallel machine scheduling, and open shop, flow shop and job shop scheduling. We indicate some problems for future research and include a selective bibliography. {\textcopyright}1979, North-Holland Publishing Company.},
	Author = {Graham, R L and Lawler, E L and Lenstra, J K and Kan, A H.G.Rinnooy},
	Doi = {10.1016/S0167-5060(08)70356-X},
	Isbn = {9780080867670},
	Issn = {01675060},
	Journal = {Annals of Discrete Mathematics},
	Number = {C},
	Pages = {287--326},
	Pmid = {384},
	Title = {{Optimization and approximation in deterministic sequencing and scheduling: A survey}},
	Volume = {5},
	Year = {1979},
	Bdsk-Url-1 = {https://doi.org/10.1016/S0167-5060(08)70356-X}}

@article{andrieu2003introduction,
	Author = {Andrieu, Christophe and de Freitas, Nando and Doucet, Arnaud and Jordan, Michael},
	Journal = {Machine Learning},
	Pages = {5--43},
	Title = {{An Introduction to {\{}MCMC{\}} for Machine Learning}},
	Volume = {50},
	Year = {2003}}

@article{collins03head-driven,
	Address = {Cambridge, MA, USA},
	Author = {Collins, Michael},
	Doi = {http://dx.doi.org/10.1162/089120103322753356},
	Issn = {0891-2017},
	Journal = {Computational Linguistics},
	Number = {4},
	Pages = {589--637},
	Publisher = {MIT Press},
	Title = {{Head-Driven Statistical Models for Natural Language Parsing}},
	Volume = {29},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/089120103322753356}}

@inproceedings{valko2008conditional,
	Abstract = {Anomaly detection methods can be very useful in identifying unusual or interesting patterns in data. A recently proposed conditional anomaly detection framework extends anomaly detection to the problem of identifying anomalous patterns on a subset of attributes in the data. The anomaly always depends (is conditioned) on the value of remaining attributes. The work presented in this paper focuses on instance-based methods for detecting conditional anomalies. The methods rely on the distance metric to identify examples in the dataset that are most critical for detecting the anomaly. We investigate various metrics and metric learning methods to optimize the performance of the instance-based anomaly detection methods. We show the benefits of the instance-based methods on two real-world detection problems: detection of unusual admission decisions for patients with the community-acquired pneumonia and detection of unusual orders of an HPF4 test that is used to confirm Heparin induced thrombocytopenia - a life-threatening condition caused by the Heparin therapy.},
	Author = {Valko, Michal and Cooper, Gregory F and Seybert, Amy and Visweswaran, Shyam and Saul, Melissa and Hauskrecht, Milos},
	Booktitle = {Workshop on Machine Learning in Health Care Applications in The 25th International Conference on Machine Learning},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Title = {{Conditional anomaly detection methods for patient-management alert systems}},
	Year = {2008}}

@inproceedings{demiris96imitative,
	Author = {Demiris, John and Hayes, Gillian},
	Booktitle = {Proceedings of the 5th European Workshop on Learning Robots},
	Editor = {Klingspor, Volker},
	Pages = {9--16},
	Title = {{Imitative learning mechanisms in robots and humans}},
	Url = {citeseer.ist.psu.edu/demiris96imitative.html},
	Year = {1996},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/demiris96imitative.html}}

@inproceedings{Huang2009,
	Author = {Huang, J and Zhang, T and Metaxas, D},
	Booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
	Title = {{Learning with structured sparsity}},
	Year = {2009}}

@article{kivinen2004online,
	Abstract = {Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.},
	Author = {Kivinen, Jyrki and Smola, Alexander J. and Williamson, Robert C.},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {8},
	Pages = {2165--2176},
	Title = {{Online learning with kernels}},
	Volume = {52},
	Year = {2004}}

@techreport{gerchinovitz2017fano,
	Abstract = {We extend Fano's inequality, which controls the average probability of (disjoint) events in terms of the average of some Kullback-Leibler divergences, to work with arbitrary [0,1]-valued random variables. Our simple two-step methodology is general enough to cover the case of an arbitrary (possibly continuously infinite) family of distributions as well as [0,1]-valued random variables not necessarily summing up to 1. Several novel applications are provided, in which the consideration of random variables is particularly handy. The most important applications deal with the problem of Bayesian posterior concentration (minimax or distribution-dependent) rates and with a lower bound on the regret in non-stochastic sequential learning. We also improve in passing some earlier fundamental results: in particular, we provide a simple and enlightening proof of the refined Pinsker's inequality of Ordentlich and Weinberger and derive a sharper Bretagnolle-Huber inequality.},
	Archiveprefix = {arXiv},
	Arxivid = {1702.05985},
	Author = {Gerchinovitz, Sebastien and M{\'{e}}nard, Pierre and Stoltz, Gilles},
	Eprint = {1702.05985},
	Month = {feb},
	Title = {{Fano's inequality for random variables}},
	Url = {http://arxiv.org/abs/1702.05985},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1702.05985}}

@article{kirkpatrick1983optimization,
	Author = {Kirkpatrick, S and Gelatt, C D and Vecchi, M P},
	Journal = {Science},
	Number = {4598},
	Pages = {671--680},
	Title = {{Optimization by Simulated Annealing}},
	Volume = {220},
	Year = {1983}}

@article{fowlkes2004spectral,
	Author = {Fowlkes, Charless and Belongie, Serge and Chung, Fan and Malik, Jitendra},
	Journal = {IEEE Transactions on PAMI},
	Number = {2},
	Title = {{Spectral Grouping Using the Nystrom Method}},
	Volume = {26},
	Year = {2004}}

@article{charniak1991bayesian,
	Address = {Menlo Park, CA, USA},
	Annote = {comps{\_}models},
	Author = {Charniak, Eugene},
	Issn = {0738-4602},
	Journal = {AI Mag.},
	Number = {4},
	Pages = {50--63},
	Publisher = {American Association for Artificial Intelligence},
	Title = {{Bayesian networks without tears: making Bayesian networks more accessible to the probabilistically unsophisticated}},
	Url = {http://www.idi.ntnu.no/emner/it3704/lectures/papers/AIMag12-04-007.pdf},
	Volume = {12},
	Year = {1991},
	Bdsk-Url-1 = {http://www.idi.ntnu.no/emner/it3704/lectures/papers/AIMag12-04-007.pdf}}

@misc{gyorfi08survey,
	Address = {Tuebingen, Germany},
	Author = {Gyorfi, L and Ottucs{\'{a}}k, Gy. and Urb{\'{a}}n, A},
	Howpublished = {Machine Learning Summer School 2007, MLSS 2007 (invited lecture)},
	Title = {{Empirical log-optimal portfolio selections: a survey}},
	Year = {2008}}

@article{RP11,
	Author = {Ryzhov, I and Powell, W},
	Journal = {Operations Research},
	Pages = {188--201},
	Title = {{Information Collection on a Graph}},
	Volume = {59},
	Year = {2011}}

@article{ghashami2016frequent,
	Abstract = {We describe a new algorithm called Frequent Directions for deterministic matrix sketching in the row-updates model. The algorithm is presented an arbitrary input matrix {\$}A \backslashin R{\^{}}{\{}n \backslashtimes d{\}}{\$} one row at a time. It performed {\$}O(d \backslashtimes \backslashell){\$} operations per row and maintains a sketch matrix {\$}B \backslashin R{\^{}}{\{}\backslashell \backslashtimes d{\}}{\$} such that for any {\$}k {\textless} \backslashell{\$} {\$}\backslash|A{\^{}}TA - B{\^{}}TB \backslash|{\_}2 \backslashleq \backslash|A - A{\_}k\backslash|{\_}F{\^{}}2 / (\backslashell-k){\$} and {\$}\backslash|A - \backslashpi{\_}{\{}B{\_}k{\}}(A)\backslash|{\_}F{\^{}}2 \backslashleq \backslashbig(1 + \backslashfrac{\{}k{\}}{\{}\backslashell-k{\}}\backslashbig) \backslash|A-A{\_}k\backslash|{\_}F{\^{}}2 {\$} . Here, {\$}A{\_}k{\$} stands for the minimizer of {\$}\backslash|A - A{\_}k\backslash|{\_}F{\$} over all rank {\$}k{\$} matrices (similarly {\$}B{\_}k{\$}) and {\$}\backslashpi{\_}{\{}B{\_}k{\}}(A){\$} is the rank {\$}k{\$} matrix resulting from projecting {\$}A{\$} on the row span of {\$}B{\_}k{\$}. We show both of these bounds are the best possible for the space allowed. The summary is mergeable, and hence trivially parallelizable. Moreover, Frequent Directions outperforms exemplar implementations of existing streaming algorithms in the space-error tradeoff.},
	Author = {Ghashami, Mina and Liberty, Edo and Phillips, Jeff M and Woodruff, David P.},
	Journal = {The SIAM Journal of Computing},
	Pages = {1--28},
	Title = {{Frequent directions: Simple and deterministic matrix sketching}},
	Year = {2016}}

@book{gelman2004bayesian,
	Author = {Gelman, A},
	Publisher = {CRC press},
	Title = {{Bayesian data analysis}},
	Year = {2004}}

@book{pearl1988probabilistic,
	Address = {San Francisco, CA, USA},
	Author = {Pearl, Judea},
	Isbn = {0-934613-73-7},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Probabilistic reasoning in intelligent systems: networks of plausible inference}},
	Year = {1988}}

@article{goldengorin1999data,
	Author = {Goldengorin, B and Sierksma, G and Tijssen, G A and Tso, M},
	Journal = {Management Science},
	Pages = {1539--1551},
	Publisher = {JSTOR},
	Title = {{The data-correcting algorithm for the minimization of supermodular functions}},
	Year = {1999}}

@unpublished{BaPaSzSz11-online,
	Annote = {From Duplicate 1 (Online learning - Bartok, G; Pal, D; Szepesvari, C; Szita, I)

Lecture Notes

From Duplicate 2 (Online Learning - Bart{\'{o}}k, G; P{\'{a}}l, D; Szepesv{\'{a}}ri, Cs.; Szita, I)

https://moodle.cs.ualberta.ca/file.php/354/notes.pdf},
	Author = {Bart{\'{o}}k, G and P{\'{a}}l, D and Szepesv{\'{a}}ri, Cs. and Szita, I and Bartok, G and Pal, D and Szepesvari, C and Szita, I},
	Howpublished = {Lecture notes, University of Alberta},
	Title = {{Online learning}},
	Year = {2011}}

@book{catch-22,
	Author = {Heller, Joseph},
	Publisher = {Simon {\&} Schuster},
	Title = {{Catch-22}},
	Year = {1961}}

@inproceedings{younes2004solving,
	Author = {Younes, Hakan and Simmons, Reid},
	Booktitle = {Proceedings of the 19th National Conference on Artificial Intelligence},
	Pages = {742--747},
	Title = {{Solving Generalized Semi-{\{}Markov{\}} Decision Processes Using Continuous Phase-Type Distributions}},
	Year = {2004}}

@inproceedings{williams2001using,
	Abstract = {A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n ), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystr{\"{o}}m method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m {\textless} n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m n). We report experiments on the USPS and abalone data sets and show that we can set m n without any significant decrease in the accuracy of the solution.},
	Author = {Williams, Christopher and Seeger, Matthias},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Using the Nystrom method to speed up kernel machines}},
	Year = {2001}}

@inproceedings{carpentier2014extreme,
	Abstract = {In many areas of medicine, security, and life sciences, we want to allocate limited resources to different sources in order to detect extreme values. In this paper, we study an efficient way to allocate these resources sequentially under limited feedback. While sequential design of experiments is well studied in bandit theory, the most commonly optimized property is the regret with respect to the maximum mean reward. However, in other problems such as network intrusion detection, we are interested in detecting the most extreme value output by the sources. Therefore, in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail. We propose the ExtremeHunter algorithm, provide its analysis, and evaluate it empirically on synthetic and real-world experiments.},
	Author = {Carpentier, Alexandra and Valko, Michal},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Carpentier, Valko - 2014 - Extreme bandits.pdf:pdf},
	Title = {{Extreme bandits}},
	Year = {2014}}

@article{lovasz1982submodular,
	Author = {Lov{\'{a}}sz, L},
	Journal = {Mathematical programming: The state of the art, Bonn},
	Pages = {235--257},
	Title = {{Submodular functions and convexity}},
	Year = {1982}}

@inproceedings{lei2015online,
	Abstract = {Social networks are commonly used for marketing purposes. For example, free samples of a product can be given to a few influential social network users (or "seed nodes"), with the hope that they will convince their friends to buy it. One way to formalize marketers' objective is through influence maximization (or IM), whose goal is to find the best seed nodes to activate under a fixed budget, so that the number of people who get influenced in the end is maximized. Recent solutions to IM rely on the influence probability that a user influences another one. However, this probability information may be unavailable or incomplete. In this paper, we study IM in the absence of complete information on influence probability. We call this problem Online Influence Maximization (OIM) since we learn influence probabilities at the same time we run influence campaigns. To solve OIM, we propose a multiple-trial approach, where (1) some seed nodes are selected based on existing influence information; (2) an influence campaign is started with these seed nodes; and (3) users' feedback is used to update influence information. We adopt the Explore-Exploit strategy, which can select seed nodes using either the current influence probability estimation (exploit), or the confidence bound on the estimation (explore). Any existing IM algorithm can be used in this framework. We also develop an incremental algorithm that can significantly reduce the overhead of handling users' feedback information. Our experiments show that our solution is more effective than traditional IM methods on the partial information.},
	Author = {Lei, Siyu and Maniu, Silviu and Mo, Luyi and Cheng, Reynold and Senellart, Pierre},
	Booktitle = {Knowledge Discovery and Data mining},
	Title = {{Online influence maximization}},
	Year = {2015}}

@inproceedings{even-dar02pacbounds,
	Author = {Even-dar, Eyal and Mannor, Shie and Mansour, Yishay},
	Booktitle = {In Fifteenth Annual Conference on Computational Learning Theory (COLT)},
	Pages = {255--270},
	Title = {{{\{}PAC{\}} bounds for multi-armed bandit and {\{}M{\}}arkov decision processes}},
	Year = {2002}}

@article{jones1993lipschitzian,
	Abstract = {We present a new algorithm for finding the global minimum of a multivariate function subject to simple bounds. The algorithm is a modification of the standard Lipschitzian approach that eliminates the need to specify a Lipschitz constant. This is done by carrying out simultaneous searches using all possible constants from zero to infinity. On nine standard test functions, the new algorithm converges in fewer function evaluations than most competing methods.},
	Author = {Jones, David and Perttunen, Cary and Stuckman, Bruce},
	Journal = {Journal of Optimization Theory and Applications},
	Number = {1},
	Pages = {157--181},
	Publisher = {Springer},
	Title = {{Lipschitzian optimization without the Lipschitz constant}},
	Volume = {79},
	Year = {1993}}

@inproceedings{sanner2005approximate,
	Author = {Sanner, Scott and Boutilier, Craig},
	Booktitle = {Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence},
	Title = {{Approximate Linear Programming for First-Order {\{}MDPs{\}}}},
	Year = {2005}}

@book{nara,
	Annote = {Second edition},
	Author = {Narayanan, H},
	Publisher = {North-Holland},
	Title = {{Submodular Functions and Electrical Networks}},
	Year = {2009}}

@inproceedings{gramacy2003adaptive,
	Author = {Gramacy, Robert and Warmuth, Manfred and Brandt, Scott and Ari, Ismail},
	Booktitle = {Advances in Neural Information Processing Systems 15},
	Pages = {1465--1472},
	Title = {{Adaptive Caching by Refetching}},
	Year = {2003}}

@phdthesis{littman1996algorithms,
	Author = {Littman, Michael},
	School = {Brown University},
	Title = {{Algorithms for Sequential Decision Making}},
	Year = {1996}}

@inproceedings{valizadegan2007kernel,
	Author = {Valizadegan, Hamed and Tan, Pang-Ning},
	Booktitle = {Proceedings of the Seventh SIAM International Conference on Data Mining, April 26-28, 2007, Minneapolis, Minnesota, USA},
	Title = {{Kernel Based Detection of Mislabeled Training Examples}},
	Year = {2007}}

@article{geman1984stochastic,
	Author = {Geman, Stuart and Geman, Donald},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Number = {6},
	Pages = {721--741},
	Title = {{Stochastic Relaxation, {\{}Gibbs{\}} Distribution, and the {\{}Bayesian{\}} Restoration of Images}},
	Volume = {6},
	Year = {1984}}

@book{BaHoSchSmTaVi07,
	Author = {Bakir, G{\"{u}}khan H and Hofmann, Thomas and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J and Taskar, Ben and Vishwanathan, S V N},
	Isbn = {0262026171},
	Publisher = {The MIT Press},
	Title = {{Predicting Structured Data (Neural Information Processing)}},
	Year = {2007}}

@article{chomsky56,
	Author = {Chomsky, N},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {3mdl,cfg,chomsky,fsm,language},
	Number = {3},
	Pages = {113--124},
	Title = {{Three models for the description of language}},
	Url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1056813},
	Volume = {2},
	Year = {1956},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/xpls/abs%7B%5C_%7Dall.jsp?arnumber=1056813}}

@article{brafman2003r-max,
	Author = {Brafman, Ronen and Tennenholtz, Moshe},
	Journal = {Journal of Machine Learning Research},
	Pages = {213--231},
	Title = {{R-MAX -- A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning}},
	Volume = {3},
	Year = {2003}}

@inproceedings{weinberger2008fast,
	Address = {New York, NY, USA},
	Annote = {comps{\_}distance},
	Author = {Weinberger, Kilian Q and Saul, Lawrence K},
	Booktitle = {ICML '08: Proceedings of the 25th international conference on Machine learning},
	Doi = {http://doi.acm.org/10.1145/1390156.1390302},
	Isbn = {978-1-60558-205-4},
	Pages = {1160--1167},
	Publisher = {ACM},
	Title = {{Fast solvers and efficient implementations for distance metric learning}},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1390156.1390302}}

@incollection{torresani2007large,
	Address = {Cambridge, MA},
	Annote = {comps{\_}distance},
	Author = {Torresani, Lorenzo and Lee, Kuang-chih},
	Booktitle = {Advances in Neural Information Processing Systems 19},
	Editor = {Scholkopf, B and Platt, J and Hoffman, T},
	Pages = {1385--1392},
	Publisher = {MIT Press},
	Title = {{Large Margin Component Analysis}},
	Url = {http://books.nips.cc/papers/files/nips19/NIPS2006{\_}0791.pdf},
	Year = {2007},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips19/NIPS2006%7B%5C_%7D0791.pdf}}

@inproceedings{bao2016online,
	Abstract = {Social networks have been popular platforms for information propagation. An important use case is viral marketing: given a promotion budget, an advertiser can choose some influential users as the seed set and provide them free or discounted sample products; in this way, the advertiser hopes to increase the popularity of the product in the users' friend circles by the world-of-mouth effect, and thus maximizes the number of users that information of the production can reach. There has been a body of literature studying the influence maximization problem. Nevertheless, the existing studies mostly investigate the problem on a one-off basis, assuming fixed known influence probabilities among users, or the knowledge of the exact social network topology. In practice, the social network topology and the influence probabilities are typically unknown to the advertiser, which can be varying over time, i.e., in cases of newly established, strengthened or weakened social ties. In this paper, we focus on a dynamic non-stationary social network and design a randomized algorithm, RSB, based on multi-armed bandit optimization, to maximize influence propagation over time. The algorithm produces a sequence of online decisions and calibrates its explore-exploit strategy utilizing outcomes of previous decisions. It is rigorously proven to achieve an upper-bounded regret in reward and applicable to large-scale social networks. Practical effectiveness of the algorithm is evaluated using both synthetic and real-world datasets, which demonstrates that our algorithm outperforms previous stationary methods under non-stationary conditions.},
	Author = {Bao, Yixin and Wang, Xiaoke and Wang, Zhi and Wu, Chuan and Lau, Francis C. M.},
	Booktitle = {International Symposium on Quality of Service},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Bao et al. - 2016 - Online Influence Maximization in Non-Stationary Social Networks.pdf:pdf},
	Month = {apr},
	Title = {{Online influence maximization in non-stationary social networks}},
	Year = {2016}}

@book{Dud99,
	Author = {Dudley, R},
	Publisher = {Cambridge University Press},
	Title = {{Uniform Central Limit Theorems}},
	Year = {1999}}

@article{durrett1977functionals,
	Author = {Durrett, Richard T. and Iglehart, Donald L.},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Durrett, Iglehart - 1977 - Functionals of Brownian meander and Brownian excursion.pdf:pdf},
	Journal = {The Annals of Probability},
	Number = {1},
	Pages = {130--135},
	Title = {{Functionals of Brownian meander and Brownian excursion}},
	Volume = {5},
	Year = {1977}}

@inproceedings{SST11,
	Author = {Srebro, N and Sridharan, K and Tewari, A},
	Booktitle = {Advances in Neural Information Processing Systems (NIPS)},
	Title = {{On the Universality of Online Mirror Descent}},
	Year = {2011}}

@article{lazaric11stoch_adv,
	Author = {Lazaric, A and Munos, R},
	Journal = {Journal of Computer and System Sciences (Special issue: Cloud Computing 2011)},
	Pages = {1516--1537},
	Title = {{Learning with Stochastic Inputs and Adversarial Outputs}},
	Volume = {78(5)},
	Year = {2012}}

@article{hanley1982meaning,
	Abstract = {A representation and interpretation of the area under a receiver operating
characteristic (ROC) curve obtained by the "rating" method, or by
mathematical predictions based on patient characteristics, is presented.
It is shown that in such a setting the area represents the probability
that a randomly chosen diseased subject is (correctly) rated or ranked
with greater suspicion than a randomly chosen non-diseased subject.
Moreover, this probability of a correct ranking is the same quantity
that is estimated by the already well-studied nonparametric Wilcoxon
statistic. These two relationships are exploited to (a) provide rapid
closed-form expressions for the approximate magnitude of the sampling
variability, i.e., standard error that one uses to accompany the
area under a smoothed ROC curve, (b) guide in determining the size
of the sample required to provide a sufficiently reliable estimate
of this area, and (c) determine how large sample sizes should be
to ensure that one can statistically detect differences in the accuracy
of diagnostic techniques.},
	Author = {Hanley, J A and Mcneil, B J},
	Issn = {0033-8419},
	Journal = {Radiology},
	Keywords = {auc,auroc,hanley,mcneil,roc},
	Month = {apr},
	Number = {1},
	Pages = {29--36},
	Title = {{The meaning and use of the area under a receiver operating characteristic (ROC) curve.}},
	Volume = {143},
	Year = {1982}}

@book{dehaan2006extreme,
	Author = {de Haan, Laurens and Ferreira, Ana},
	Publisher = {Springer},
	Series = {Springer Series in Operations Research and Financial Engineering},
	Title = {{Extreme Value Theory: An Introduction}},
	Year = {2006}}

@article{levine2010feature,
	Author = {Levine, S and Popovic, Z and Koltun, V},
	Journal = {Advances in Neural Information Processing Systems},
	Title = {{Feature construction for inverse reinforcement learning}},
	Url = {http://www.stanford.edu/{~}svlevine/papers/firl.pdf},
	Volume = {23},
	Year = {2010},
	Bdsk-Url-1 = {http://www.stanford.edu/%7B~%7Dsvlevine/papers/firl.pdf}}

@article{fisher1928limiting,
	Abstract = {SummaryThe limiting distribution, when n is large, of the greatest or least of a sample of n, must satisfy a functional equation which limits its form to one of two main types. Of these one has, apart from size and position, a single parameter h, while the other is the limit to which it tends when h tends to zero.The appropriate limiting distribution in any case may be found from the manner in which the probability of exceeding any value x tends to zero as x is increased. For the normal distribution the limiting distribution has h = 0.From the normal distribution the limiting distribution is approached with extreme slowness; the final series of forms passed through as the ultimate form is approached may be represented by the series of limiting distributions in which h tends to zero in a definite manner as n increases to infinity.Numerical values are given for the comparison of the actual with the penultimate distributions for samples of 60 to 1000, and of the penultimate with the ultimate distributions for larger samples.},
	Author = {Fisher, Ronald Aylmer and Tippett, Leonard Henry Caleb},
	Doi = {10.1017/S0305004100015681},
	Isbn = {1469-8064},
	Issn = {0305-0041},
	Journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	Pages = {180},
	Title = {{Limiting forms of the frequency distribution of the largest or smallest member of a sample}},
	Volume = {24},
	Year = {1928},
	Bdsk-Url-1 = {https://doi.org/10.1017/S0305004100015681}}

@incollection{howard1984influence,
	Address = {Menlo Park, CA},
	Author = {Howard, Ronald and Matheson, James},
	Booktitle = {Readings on the Principles and Applications of Decision Analysis},
	Pages = {719--762},
	Publisher = {Strategic Decisions Group},
	Title = {{Influence Diagrams}},
	Volume = {2},
	Year = {1984}}

@article{feige1998threshold,
	Author = {Feige, U},
	Journal = {Journal of the ACM (JACM)},
	Number = {4},
	Pages = {634--652},
	Publisher = {ACM},
	Title = {{A threshold of {\{}$\backslash$rm ln{\}} n for approximating set cover}},
	Volume = {45},
	Year = {1998}}

@book{boyd,
	Author = {Boyd, S P and Vandenberghe, L},
	Publisher = {Cambridge University Press},
	Title = {{Convex Optimization}},
	Year = {2004}}

@inproceedings{koutis2011a-nearly-m,
	Author = {Koutis, Ioannis and Miller, Gary L and Peng, Richard},
	Booktitle = {{\{}IEEE{\}} 52nd Annual Symposium on Foundations of Computer Science, {\{}FOCS{\}}},
	Pages = {590--598},
	Title = {{A Nearly-m log n Time Solver for {\{}SDD{\}} Linear Systems}},
	Year = {2011}}

@inproceedings{xing2003generalized,
	Annote = {comps{\_}models},
	Author = {Xing, Eric P and Jordan, Michael I and Russell, Stuart J},
	Booktitle = {Proceedings of UAI},
	Pages = {583--591},
	Title = {{A generalized mean field algorithm for variational inference in exponential families}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.6058},
	Year = {2003},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.6058}}

@article{engel2004kernel,
	Abstract = {We present a nonlinear version of the recursive least squares (RLS) algorithm. Our algorithm performs linear regression in a high-dimensional feature space induced by a Mercer kernel and can therefore be used to recursively construct minimum mean-squared-error solutions to nonlinear least-squares problems that are frequently encountered in signal processing applications. In order to regularize solutions and keep the complexity of the algorithm bounded, we use a sequential sparsification process that admits into the kernel representation a new input sample only if its feature space image cannot be sufficiently well approximated by combining the images of previously admitted samples. This sparsification procedure allows the algorithm to operate online, often in real time. We analyze the behavior of the algorithm, compare its scaling properties to those of support vector machines, and demonstrate its utility in solving two signal processing problems-time-series prediction and channel equalization.},
	Author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {8},
	Pages = {2275--2285},
	Title = {{The kernel recursive least-squares algorithm}},
	Volume = {52},
	Year = {2004}}

@inproceedings{streeter2008online,
	Author = {{Matthew J. Streeter} and {Daniel Golovin}},
	Booktitle = {NIPS},
	Pages = {1577--1584},
	Title = {{An Online Algorithm for Maximizing Submodular Functions}},
	Year = {2008}}

@inproceedings{zhang2005learning-based,
	Address = {New York, NY, USA},
	Author = {Zhang, Jian and Rexford, Jennifer and Feigenbaum, Joan},
	Booktitle = {MineNet '05: Proceedings of the 2005 ACM SIGCOMM workshop on Mining network data},
	Doi = {http://doi.acm.org/10.1145/1080173.1080189},
	Isbn = {1-59593-026-4},
	Pages = {219--220},
	Publisher = {ACM},
	Title = {{Learning-based anomaly detection in BGP updates}},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1080173.1080189}}

@inproceedings{BLLRS11,
	Author = {Beygelzimer, Alina and Langford, John and Li, Lihong and Reyzin, Lev and Schapire, Robert E},
	Pages = {19--26},
	Title = {{Contextual Bandit Algorithms with Supervised Learning Guarantees}}}

@techreport{statscience,
	Author = {Bach, F and Jenatton, R and Mairal, J and Obozinski, G},
	Institution = {HAL},
	Number = {00621245},
	Title = {{Structured sparsity through convex optimization}},
	Year = {2011}}

@article{alpern2013mining,
	Abstract = {We show how to optimize the search for a hidden object, terrorist, or simply Hider, located at a point H according to a known or unknown distribution {\$}\nu{\$} on a rooted network Q. We modify the traditional ``pathwise search'' approach to a more general notion of ``expanding search.'' When the Hider is restricted to the nodes of Q, an expanding search S consists of an ordering {\$}\backslash{\$}documentclass{\{}aastex{\}} {\$}\backslash{\$}usepackage{\{}amsbsy{\}} {\$}\backslash{\$}usepackage{\{}amsfonts{\}} {\$}\backslash{\$}usepackage{\{}amssymb{\}} {\$}\backslash{\$}usepackage{\{}bm{\}} {\$}\backslash{\$}usepackage{\{}mathrsfs{\}} {\$}\backslash{\$}usepackage{\{}pifont{\}} {\$}\backslash{\$}usepackage{\{}stmaryrd{\}} {\$}\backslash{\$}usepackage{\{}textcomp{\}} {\$}\backslash{\$}usepackage{\{}portland,xspace{\}} {\$}\backslash{\$}usepackage{\{}amsmath,amsxtra{\}} {\$}\backslash{\$}pagestyle{\{}empty{\}} {\$}\backslash{\$}DeclareMathSizes{\{}10{\}}{\{}9{\}}{\{}7{\}}{\{}6{\}} {\$}\backslash{\$}begin{\{}document{\}} {\$}(a{\_}{\{}1{\}},a{\_}{\{}2{\}},\backslashbackslashldots){\$} {\$}\backslash{\$}end{\{}document{\}} of the arcs of a spanning subtree such that the root node is in a1 and every arc ai is adjacent to a previous arc aj, j {\textless}i. If ak contains H, the search time T is {\$}\backslash{\$}documentclass{\{}aastex{\}} {\$}\backslash{\$}usepackage{\{}amsbsy{\}} {\$}\backslash{\$}usepackage{\{}amsfonts{\}} {\$}\backslash{\$}usepackage{\{}amssymb{\}} {\$}\backslash{\$}usepackage{\{}bm{\}} {\$}\backslash{\$}usepackage{\{}mathrsfs{\}} {\$}\backslash{\$}usepackage{\{}...{\}}},
	Author = {Alpern, Steve and Lidbetter, Thomas},
	Doi = {10.1287/opre.1120.1134},
	Issn = {0030-364X},
	Journal = {Operations Research},
	Keywords = {games/group decisions,networks/graphs,search/surveillance,teams,tree algorithms},
	Month = {apr},
	Number = {2},
	Pages = {265--279},
	Publisher = {INFORMS},
	Title = {{Mining coal or finding terrorists: The expanding search paradigm}},
	Url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.1120.1134},
	Volume = {61},
	Year = {2013},
	Bdsk-Url-1 = {http://pubsonline.informs.org/doi/abs/10.1287/opre.1120.1134},
	Bdsk-Url-2 = {https://doi.org/10.1287/opre.1120.1134}}

@inproceedings{Verbaeten-2003-MisLabeled,
	Author = {Verbaeten, S and Assche., A V},
	Booktitle = {Proceeding of 4th International Workshop on Multiple Classifier Systems},
	Title = {{Ensemble Methods for Noise Elimination in Classification Problems.}},
	Year = {2003}}

@article{Ipsen11ergodicitycoeff,
	Author = {Ipsen, Ilse C F and Selee, Teresa M},
	Journal = {SIAM J. Matrix Analysis Applications},
	Number = {1},
	Pages = {153--200},
	Title = {{Ergodicity Coefficients Defined by Vector Norms}},
	Volume = {32},
	Year = {2011}}

@inproceedings{mahadevan2005samuel,
	Author = {Mahadevan, Sridhar},
	Booktitle = {Proceedings of the 20th National Conference on Artificial Intelligence},
	Pages = {1000--1005},
	Title = {{Samuel Meets {\{}Amarel{\}}: Automating Value Function Approximation Using Global State Space Analysis}},
	Year = {2005}}

@article{yu,
	Author = {Yu, B},
	Journal = {The Annals of Probability},
	Number = {1},
	Pages = {94--116},
	Title = {{Rates of convergence for empirical processes of stationary mixing sequences}},
	Volume = {22},
	Year = {1994}}

@inproceedings{goldberger2004neighbourhood,
	Author = {Goldberger, Jacob and Roweis, Sam T and Hinton, Geoffrey E and Salakhutdinov, Ruslan},
	Booktitle = {NIPS},
	Title = {{Neighbourhood Components Analysis.}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.7841},
	Year = {2004},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.7841}}

@misc{urlhttp://mplab.ucsd.edumplab,
	Author = {$\backslash$urlhttp://mplab.ucsd.edu},
	Title = {{MPLab GENKI Database}}}

@inproceedings{abbasi-yadkori2018best,
	Abstract = {We study bandit best-arm identification with arbitrary and potentially adversarial rewards. A simple random uniform learner obtains the optimal rate of error in the adversarial scenario. However, this type of strategy is suboptimal when the rewards are sampled stochastically. Therefore, we ask: Can we design a learner that performs optimally in both the stochastic and adversarial problems while not being aware of the nature of the rewards? First, we show that designing such a learner is impossible in general. In particular, to be robust to adversarial rewards, we can only guarantee optimal rates of error on a subset of the stochastic problems. We give a lower bound that characterizes the optimal rate in stochastic problems if the strategy is constrained to be robust to adversarial rewards. Finally, we design a simple parameter-free algorithm and show that its probability of error matches (up to log factors) the lower bound in stochastic problems, and it is also robust to adversarial ones.},
	Author = {Abbasi-Yadkori, Yasin and Bartlett, Peter and Gabillon, Victor and Malek, Alan and Valko, Michal},
	Booktitle = {Conference on Learning Theory},
	Title = {{Best of both worlds: Stochastic {\&} adversarial best-arm identification}},
	Year = {2018}}

@article{casella1996rao-blackwellisation,
	Author = {Casella, George and Robert, Christian},
	Journal = {Biometrika},
	Number = {1},
	Pages = {81--94},
	Title = {{{\{}Rao-Blackwellisation{\}} of Sampling Schemes}},
	Volume = {83},
	Year = {1996}}

@article{brodley1999identifying,
	Author = {Brodley, Carla E and Friedl, Mark A},
	Journal = {J. Artif. Intell. Res. (JAIR)},
	Pages = {131--167},
	Title = {{Identifying Mislabeled Training Data}},
	Volume = {11},
	Year = {1999}}

@unpublished{Bub11,
	Annote = {Lecture Notes},
	Author = {Bubeck, S},
	Title = {{Introduction to Online Optimization}},
	Year = {2011}}

@inproceedings{xing2003distance,
	Annote = {comps{\_}distance},
	Author = {Xing, Eric P and Ng, Andrew Y and Jordan, Michael I and Russell, Stuart},
	Booktitle = {Advances in Neural Information Processing Systems 15},
	Pages = {505--512},
	Publisher = {MIT Press},
	Title = {{Distance metric learning, with application to clustering with side-information}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.3667},
	Year = {2003},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.3667}}

@inproceedings{chawla2003smoteboost:,
	Author = {Chawla, Nitesh V and Lazarevic, Aleksandar and Hall, Lawrence O and Bowyer, Kevin W},
	Booktitle = {PKDD},
	Pages = {107--119},
	Title = {{SMOTEBoost: Improving Prediction of the Minority Class in Boosting.}},
	Year = {2003}}

@article{chen2015combinatorial,
	Abstract = {We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which more based arms could be probabilistically triggered based on the outcomes of already triggered arms. The reward of the super arm depends on the outcomes of all played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an offline ($\backslash$alpha,$\backslash$beta)-approximation oracle that takes the means of the outcome distributions of arms and outputs a super arm that with probability {\{}$\backslash$beta{\}} generates an {\{}$\backslash$alpha{\}} fraction of the optimal expected reward. The objective of an online learning algorithm for CMAB is to minimize ($\backslash$alpha,$\backslash$beta)-approximation regret, which is the difference between the $\backslash$alpha{\{}$\backslash$beta{\}} fraction of the expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(log n) distribution-dependent regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound of UCB1 algorithm (up to a constant factor) for the classical MAB problem, and it significantly improves the regret bound in a earlier paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage and social influence maximization, both having nonlinear reward structures. In particular, application to social influence maximization requires our extension on probabilistically triggered arms.},
	Author = {Chen, Wei and Wang, Yajun and Yuan, Yang},
	Journal = {Journal of Machine Learning Research},
	Title = {{Combinatorial multi-armed bandit and its extension to probabilistically triggered arms}},
	Url = {http://www.jmlr.org/papers/volume17/14-298/14-298.pdf},
	Volume = {17},
	Year = {2016},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume17/14-298/14-298.pdf}}

@inproceedings{charikar1997incremental,
	Author = {Charikar, Moses and Chekuri, Chandra and Feder, Tomas and Motwani, Rajeev},
	Booktitle = {Proceedings of the 29th Annual ACM Symposium on Theory of Computing},
	Pages = {626--635},
	Title = {{Incremental Clustering and Dynamic Information Retrieval}},
	Year = {1997}}

@techreport{asadi2016new,
	Abstract = {A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one's weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study an alternative softmax operator that, among other properties, is both a non-expansion (ensuring convergent behavior in learning and planning) and differentiable (making it possible to improve decisions via gradient descent methods). We provide proofs of these properties and present empirical comparisons between various softmax operators.},
	Archiveprefix = {arXiv},
	Arxivid = {1612.05628},
	Author = {Asadi, Kavosh and Littman, Michael L.},
	Eprint = {1612.05628},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Asadi, Littman - 2016 - A new softmax operator for reinforcement learning.pdf:pdf},
	Title = {{A new softmax operator for reinforcement learning}},
	Url = {http://arxiv.org/abs/1612.05628},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1612.05628}}

@article{azuma1967weighted,
	Author = {Azuma, Kazuoki},
	Journal = {Tohoku Mathematical Journal},
	Keywords = {bound,math},
	Number = {3},
	Pages = {357--367},
	Title = {{Weighted sums of certain dependent random variables}},
	Url = {https://projecteuclid.org/download/pdf{\_}1/euclid.tmj/1178243286},
	Volume = {19},
	Year = {1967},
	Bdsk-Url-1 = {https://projecteuclid.org/download/pdf%7B%5C_%7D1/euclid.tmj/1178243286}}

@article{farias2003linear,
	Author = {de Farias, Daniela Pucci and {Van Roy}, Benjamin},
	Journal = {Mathematics of Operations Research},
	Number = {3},
	Pages = {462--478},
	Title = {{On Constraint Sampling for the Linear Programming Approach to Approximate Dynamic Programming}},
	Volume = {29},
	Year = {2003}}

@book{DGL96,
	Author = {Devroye, L and Gy{\"{o}}rfi, L and Lugosi, G},
	Publisher = {Springer},
	Title = {{A Probabilistic Theory of Pattern Recognition}},
	Year = {1996}}

@article{fujishige2006minimum,
	Author = {Fujishige, Satoru and Isotani, S},
	Journal = {Pacific Journal of Optimization},
	Pages = {3--17},
	Title = {{A Submodular Function Minimization Algorithm Based on the Minimum-Norm Base}},
	Volume = {7},
	Year = {2011}}

@techreport{musco2016provably,
	Abstract = {We give the first algorithms for kernel matrix approximation that run in time linear in the number of data points and output an approximation which gives provable guarantees when used in many downstream learning tasks, including kernel principal component analysis, kernel {\$}k{\$}-means clustering, kernel ridge regression, and kernel canonical correlation analysis. Our methods require just {\$}\backslashtilde O(n\backslashcdot k){\$} kernel evaluations and {\$}\backslashtilde O(n \backslashcdot k{\^{}}2){\$} additional runtime, where {\$}n{\$} is the number of training data points and {\$}k{\$} is a target rank or effective dimensionality parameter. These runtimes are significantly sub-linear in the size of the {\$}n \backslashtimes n{\$} kernel matrix and apply to any kernel matrix, without assuming regularity or incoherence conditions. The algorithms are based on a ridge leverage score Nystr$\backslash$"om sampling scheme (RLS-Nystr$\backslash$"om) which was recently shown to yield strong kernel approximations, but which had no efficient implementation. We address this shortcoming by introducing fast recursive sampling methods for RLS-Nystr$\backslash$"om, while at the same time proving extended approximation guarantees for this promising new method.},
	Archiveprefix = {arXiv},
	Arxivid = {1605.07583},
	Author = {Musco, Cameron and Musco, Christopher},
	Eprint = {1605.07583},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Musco, Musco - 2016 - Provably useful kernel matrix approximation in linear time.pdf:pdf},
	Month = {may},
	Title = {{Provably useful kernel matrix approximation in linear time}},
	Url = {http://arxiv.org/abs/1605.07583},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1605.07583}}

@inproceedings{quattoni2009efficient,
	Author = {Quattoni, A and Carreras, X and Collins, M and Darrell, T},
	Booktitle = {Proc. ICML},
	Title = {{An efficient projection for $\backslash$ell{\_}1-$\backslash$ell{\_}$\backslash$infty regularization}},
	Year = {2009}}

@article{zheng2000lazy,
	Address = {Hingham, MA, USA},
	Annote = {comps{\_}models},
	Author = {Zheng, Zijian and Webb, Geoffrey I},
	Doi = {http://dx.doi.org/10.1023/A:1007613203719},
	Issn = {0885-6125},
	Journal = {Mach. Learn.},
	Number = {1},
	Pages = {53--84},
	Publisher = {Kluwer Academic Publishers},
	Title = {{Lazy Learning of Bayesian Rules}},
	Volume = {41},
	Year = {2000},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1007613203719}}

@techreport{heckerman1995tutorial,
	Address = {Redmond, Washington},
	Annote = {Revised June 96},
	Author = {Heckerman, D},
	Institution = {Microsoft Research},
	Title = {{A Tutorial on Learning with Bayesian Networks}},
	Year = {1995}}

@article{nagamochi1998note,
	Author = {Nagamochi, H and Ibaraki, T},
	Journal = {Information Processing Letters},
	Number = {5},
	Pages = {239--244},
	Publisher = {Elsevier},
	Title = {{A note on minimizing submodular functions}},
	Volume = {67},
	Year = {1998}}

@inproceedings{cai2014comparison,
	Author = {Cai, Zhuhua and Gao, Zekai J and Luo, Shangyu and Perez, Luis L and Vagena, Zografoula and Jermaine, Christopher},
	Booktitle = {SIGMOD},
	Title = {{A comparison of platforms for implementing and running very large scale machine learning algorithms}},
	Year = {2014}}

@phdthesis{shalev-shwartz2007online,
	Author = {Shalev-Shwartz, Shai},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {jul},
	School = {The Hebrew University of Jerusalem},
	Title = {{Online Learning: Theory, Algorithms, and Applications}},
	Year = {2007}}

@inproceedings{mannor2006online,
	Author = {Mannor, Shie and Tsitsiklis, John},
	Booktitle = {Proceedings of 19th Annual Conference on Learning Theory},
	Pages = {529--543},
	Title = {{Online Learning with Constraints}},
	Year = {2006}}

@article{kveton2016learning,
	Abstract = {Many important optimization problems, such as the minimum spanning tree and minimum-cost flow, can be solved optimally by a greedy method. In this work, we study a learning variant of these problems, where the model of the problem is unknown and has to be learned by interacting repeatedly with the environment in the bandit setting. We formalize our learning problem quite generally, as learning how to maximize an unknown modular function on a known polymatroid. We propose a computationally efficient algorithm for solving our problem and bound its expected cumulative regret. Our gap-dependent upper bound is tight up to a constant and our gap-free upper bound is tight up to polylogarithmic factors. Finally, we evaluate our method on three problems and demonstrate that it is practical.},
	Author = {Kveton, Branislav and Wen, Zheng and Ashkan, Azin and Valko, Michal},
	Journal = {Journal of Machine Learning Research},
	Title = {{Learning to act greedily: Polymatroid semi-bandits}},
	Year = {2016}}

@article{montague,
	Author = {Montague, P R and Dayan, P and Person, C and Sejnowski, T J},
	Journal = {Nature},
	Pages = {725--728},
	Title = {{Bee foraging in uncertain environments using predictive Hebbian learning}},
	Volume = {377},
	Year = {1995}}

@article{metropolis1953equation,
	Author = {Metropolis, Nicholas and Rosenbluth, Arianna and Rosenbluth, Marshall and Teller, Augusta and Teller, Edward},
	Journal = {Journal of Chemical Physics},
	Pages = {1087--1092},
	Title = {{Equation of State Calculations by Fast Computing Machines}},
	Volume = {21},
	Year = {1953}}

@article{bradski2000opencv,
	Author = {Bradski, G},
	Journal = {Dr. Dobb's Journal of Software Tools},
	Keywords = {bibtex-import},
	Title = {{The OpenCV Library}},
	Year = {2000}}

@book{guestrin2008beyond,
	Address = {Helsinki, Finland},
	Author = {Guestrin, Carlos and Krause, Andreas},
	Publisher = {Tutorial at the 25rd International Conference on Machine Learning (ICML)},
	Title = {{Beyond convexity - submodularity in machine learning}},
	Year = {2008}}

@article{hohzaki2016search,
	Author = {Hohzaki, Ryusuke},
	Journal = {Journal of the Operations Research Society of Japan},
	Number = {1},
	Pages = {1--34},
	Title = {{Search games: Literature and survey}},
	Url = {http://www.orsj.or.jp/{~}archive/pdf/e{\_}mag/Vol.59{\_}01{\_}001.pdf},
	Volume = {59},
	Year = {2016},
	Bdsk-Url-1 = {http://www.orsj.or.jp/%7B~%7Darchive/pdf/e%7B%5C_%7Dmag/Vol.59%7B%5C_%7D01%7B%5C_%7D001.pdf}}

@book{bertsekas1999nonlinear,
	Address = {Belmont, MA},
	Author = {Bertsekas, Dimitri},
	Publisher = {Athena Scientific},
	Title = {{Nonlinear Programming}},
	Year = {1999}}

@inproceedings{dekel13det,
	Author = {Dekel, Ofer and Hazan, Elad},
	Booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
	Editor = {Dasgupta, Sanjoy and McAllester, David},
	Number = {3},
	Pages = {675--683},
	Publisher = {JMLR Workshop and Conference Proceedings},
	Title = {{Better Rates for Any Adversarial Deterministic MDP}},
	Volume = {28},
	Year = {2013}}

@article{CBMS07,
	Author = {Cesa-Bianchi, Nicol{\`{o}} and Mansour, Yishay and Stoltz, Gilles},
	Journal = {Machine Learning},
	Number = {2-3},
	Pages = {321--352},
	Title = {{Improved second-order bounds for prediction with expert advice}},
	Volume = {66},
	Year = {2007}}

@phdthesis{valko2005evolving,
	Abstract = {Real biological networks are able to make decisions. We will show
that this behavior can be observed even in some simple architectures
of biologically plausible neural models. The great interest of this
thesis is also to contribute to methods of statistical decision theory
by giving a lead how to evolve the neural networks to solve miscellaneous
decision tasks.},
	Author = {Valko, Michal},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Month = {jun},
	School = {Comenius University, Bratislava, Slovakia},
	Title = {{Evolving Neural Networks for Statistical Decision Theory}},
	Year = {2005}}

@inproceedings{saha2011improved,
	Abstract = {The study of online convex optimization in the bandit setting was initiated by Klein- berg (2004) and Flaxman et al. (2005). Such a setting models a decision maker that has to make decisions in the face of adversari- ally chosen convex loss functions. Moreover, the only information the decision maker re- ceives are the losses. The identities of the loss functions themselves are not revealed. In this setting, we reduce the gap between the best known lower and upper bounds for the class of smooth convex functions, i.e. convex functions with a Lipschitz continuous gradi- ent. Building upon existing work on self- concordant regularizers and one-point gradi- ent estimation, we give the first algorithm whose expected regret is O(T2/3), ignoring constant and logarithmic factors.},
	Author = {Saha, Ankan and Tewari, Ambuj},
	Booktitle = {AISTATS},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Saha, Tewari - 2011 - Improved Regret Guarantees for Online Smooth Convex Optimization with Bandit Feedback.pdf:pdf},
	Title = {{Improved Regret Guarantees for Online Smooth Convex Optimization with Bandit Feedback}},
	Url = {http://jmlr.csail.mit.edu/proceedings/papers/v15/saha11a/saha11a.pdf},
	Volume = {15},
	Year = {2011},
	Bdsk-Url-1 = {http://jmlr.csail.mit.edu/proceedings/papers/v15/saha11a/saha11a.pdf}}

@article{spectralbandits,
	Author = {Authors},
	Journal = {Supplementary Material},
	Title = {{Spectral Bandits for Smooth Graph Functions}}}

@book{papadimitriou1998combinatorial,
	Author = {Papadimitriou, Christos and Steiglitz, Kenneth},
	Publisher = {Dover Publications},
	Title = {{Combinatorial Optimization}},
	Year = {1998}}

@article{bikel04intricacies,
	Address = {Cambridge, MA, USA},
	Author = {Bikel, Daniel M},
	Doi = {http://dx.doi.org/10.1162/0891201042544929},
	Issn = {0891-2017},
	Journal = {Computational Linguistics},
	Number = {4},
	Pages = {479--511},
	Publisher = {MIT Press},
	Title = {{Intricacies of {\{}C{\}}ollins' Parsing Model}},
	Volume = {30},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/0891201042544929}}

@article{turner2010fast,
	Abstract = {We present methods to do fast online anomaly detection using scan statistics. Scan statistics have long been used to detect statistically significant bursts of events. We extend the scan statistics framework to handle many practical issues that occur in application: dealing with an unknown background rate of events, allowing for slow natural changes in background frequency, the inverse problem of finding an unusual lack of events, and setting the test parameters to maximize power. We demonstrate its use on real and synthetic data sets with comparison to other methods.},
	Author = {Turner, Ryan and Ghahramani, Zoubin and Bottone, Steven},
	Doi = {10.1109/MLSP.2010.5589151},
	Isbn = {978-1-4244-7876-7},
	Issn = {1551-2541},
	Journal = {IEEE Workshop on Machine Learning for Signal Processing},
	Keywords = {scan statistics},
	Mendeley-Tags = {scan statistics},
	Title = {{Fast online anomaly detection using scan statistics}},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/MLSP.2010.5589151}}

@inproceedings{yang2007bayesian,
	Annote = {comps{\_}distancX},
	Author = {Yang, Liu and Jin, Rong and Sukthankar, Rahul},
	Booktitle = {Proceedings of Uncertainty in AI},
	Title = {{Bayesian Active Distance Metric Learning.}},
	Url = {http://www.cs.cmu.edu/{~}rahuls/pub/uai2007-rahuls.pdf},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.cmu.edu/%7B~%7Drahuls/pub/uai2007-rahuls.pdf}}

@inproceedings{Varoquaux2010a,
	Author = {Varoquaux, G and Jenatton, R and Gramfort, A and Obozinski, G and Thirion, B and Bach, F},
	Booktitle = {NIPS Workshop on Practical Applications of Sparse Modeling: Open Issues and New Directions},
	Title = {{Sparse Structured Dictionary Learning for Brain Resting-State Activity Modeling}},
	Year = {2010}}

@inproceedings{agosta2013mixture,
	Abstract = {We model a little studied type of traffic, namely the network traffic generated from endhosts. We introduce a parsimonious model of the marginal distribution for connection arrivals consisting of mixture models with both heavy and light-tailed component distributions. Our methodology assumes that the underlying user data can be fitted to one of several models, and we apply Bayesian model selection criterion to choose the preferred combination of components. Our experiments show that a simple Pareto-exponential mixture model is preferred over more complex alternatives, for a wide range of users. This model has the desirable property of modeling the entire distribution, effectively clustering the traffic into the heavy-tailed as well as the non-heavy-tailed components. Also this method quantifies the wide diversity in the observed endhost traffic.},
	Author = {Agosta, John Mark and Chandrashekar, Jaideep and Crovella, Mark and Taft, Nina and Ting, Daniel},
	Booktitle = {IEEE Proceedings of INFOCOM,},
	Doi = {10.1109/INFCOM.2013.6566768},
	Issn = {0743-166X},
	Keywords = {Approximation methods,Bayes methods,Bayesian model selection criterion,Computational modeling,Data models,Educational institutions,Mathematical model,Maximum likelihood estimation,Pareto distribution,connection arrivals,endhost network traffic,heavy-tailed component distributions,light-tailed component distributions,marginal distribution,parsimonious model,simple Pareto-exponential mixture,telecommunication networks,telecommunication traffic,traffic clustering,wide diversity},
	Pages = {225--229},
	Title = {{Mixture models of endhost network traffic}},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/INFCOM.2013.6566768}}

@inproceedings{valko2011conditional,
	Abstract = {Timely detection of concerning events is an important problem in clinical
practice. In this paper, we consider the problem of conditional anomaly
detection that aims to identify data instances with an unusual response,
such as the omission of an important lab test. We develop a new non-parametric
approach for conditional anomaly detection based on the soft harmonic
solution, with which we estimate the confidence of the label to detect
anomalous mislabeling. We further regularize the solution to avoid
the detection of isolated examples and examples on the boundary of
the distribution support. We demonstrate the efficacy of the proposed
method in detecting unusual labels on a real-world electronic health
record dataset and compare it to several baseline approaches.},
	Author = {Valko, Michal and Valizadegan, Hamed and Kveton, Branislav and Cooper, Gregory F and Hauskrecht, Milos},
	Booktitle = {The 28th International Conference on Machine Learning Workshop on Machine Learning for Global Challenges},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Month = {jun},
	Title = {{Conditional Anomaly Detection Using Soft Harmonic Functions: An Application to Clinical Alerting}},
	Year = {2011}}

@article{kulesza2011kdpp,
	Abstract = {Determinantal point processes ( DPPs ) have recently been proposed$\backslash$nas models for set selection problems where diversity is pre- ferred.$\backslash$nFor example, they can be used to select diverse sets of sentences$\backslash$nto form doc- ument summaries, or to find multiple non- overlapping$\backslash$nhuman ...},
	Author = {Kulesza, a and Taskar, B},
	Isbn = {978-1-4503-0619-5},
	Issn = {{\textless}null{\textgreater}},
	Journal = {International Conference on Machine Learning},
	Keywords = {To Read Urgently},
	Pages = {1193--1200},
	Title = {{k-DPPs: Fixed-Size Determinantal Point Processes}},
	Url = {http://158.130.69.163/{~}taskar/pubs/kdpps{\_}icml11.pdf{\%}5Cnpapers2://publication/uuid/49BE74F9-9BC7-433A-8533-9508003732F8},
	Year = {2011}}

@inproceedings{chang2009inferring,
	Author = {Chang, Keng$\backslash$-hao and Hightower, Jeffrey and Kveton, Branislav},
	Booktitle = {Proceedings of the 7th International Conference on Pervasive Computing},
	Pages = {151--167},
	Title = {{Inferring Identity Using Accelerometers in Television Remote Controls}},
	Year = {2009}}

@inproceedings{szorenyi2014optimistic,
	Abstract = {We consider the problem of online planning in a Markov decision process with discounted rewards for any given initial state. We consider the PAC sample com-plexity problem of computing, with probability 1−$\delta$, an �-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples). We design an algorithm, called StOP (for Stochastic-Optimistic Planning), based on the " optimism in the face of uncertainty " princi-ple. StOP can be used in the general setting, requires only a generative model, and enjoys a complexity bound that only depends on the local structure of the MDP.},
	Author = {Sz{\"{o}}r{\'{e}}nyi, Bal{\'{a}}zs and Kedenburg, Gunnar and Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Optimistic planning in Markov decision processes using a generative model}},
	Year = {2014}}

@inproceedings{Cevher2009,
	Author = {Cevher, V and Indyk, P and Hegde, C and Baraniuk, R G},
	Booktitle = {Sampling Theory and Applications (SAMPTA)},
	Title = {{Recovery of clustered sparse signals from compressive measurements}},
	Year = {2009}}

@article{devroye13rwalk_it,
	Author = {Devroye, L and Lugosi, G and Neu, G},
	Journal = {Submitted to the IEEE Transactions on Information Theory},
	Title = {{Prediction by random-walk perturbation}},
	Year = {2013}}

@article{Rapaport2008,
	Author = {Rapaport, F and Barillot, E and Vert, J.-P.},
	Journal = {Bioinformatics},
	Month = {jul},
	Number = {13},
	Pages = {i375----i382},
	Title = {{Classification of array{\{}CGH{\}} data using fused {\{}SVM{\}}}},
	Volume = {24},
	Year = {2008}}

@article{ching2015one,
	Author = {Ching, Avery and Edunov, Sergey and Kabiljo, Maja and Logothetis, Dionysios and Muthukrishnan, Sambavi},
	Journal = {Proceedings of the VLDB Endowment},
	Number = {12},
	Pages = {1804--1815},
	Publisher = {VLDB Endowment},
	Title = {{One trillion edges: graph processing at Facebook-scale}},
	Volume = {8},
	Year = {2015}}

@article{Jensen2001,
	Author = {Jensen, Finn V and Kjaerulff, Uffe and Kristiansen, Brian and Langseth, Helge and Skaanning, Claus and Vomlel, Jiri and Vomlelova, Marta},
	Doi = {10.1017/S0890060401154065},
	Isbn = {0890060401},
	Issn = {08900604},
	Journal = {Artificial Intelligence for Engineering Design, Analysis and Manufacturing},
	Keywords = {Bayesian network,Decision theory,Troubleshooting},
	Number = {4},
	Pages = {321--333},
	Title = {{The SACSO methodology for troubleshooting complex systems}},
	Volume = {15},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1017/S0890060401154065}}

@inproceedings{kveton2013learning,
	Abstract = {Face recognition from a single image per person is a challenging problem because the training sample is extremely small. We consider a variation of this problem. In our problem, we recognize only one person, and there are no labeled data for any other person. This setting naturally arises in authentication on personal computers and mobile devices, and poses additional challenges because it lacks negative examples. We formalize our problem as one-class classification, and propose and analyze an algorithm that learns a non-parametric model of the face from a single labeled image and a stream of unlabeled data. In many domains, for instance when a person interacts with a computer with a camera, unlabeled data are abundant and easy to utilize. This is the first paper that investigates how these data can help in learning better models in the single-image-per-person setting. Our method is evaluated on a dataset of 43 people and we show that these people can be recognized 90{\%} of time at nearly zero false positives. This recall is 25+{\%} higher than the recall of our best performing baseline. Finally, we conduct a comprehensive sensitivity analysis of our algorithm and provide a guideline for setting its parameters in practice.},
	Address = {Shanghai, China},
	Author = {Kveton, Branislav and Valko, Michal},
	Booktitle = {10th IEEE International Conference on Automatic Face and Gesture Recognition},
	Title = {{Learning from a Single Labeled Face and a Stream of Unlabeled Data}},
	Year = {2013}}

@article{queyranne1998minimizing,
	Author = {Queyranne, M},
	Journal = {Mathematical Programming},
	Number = {1},
	Pages = {3--12},
	Publisher = {Springer},
	Title = {{Minimizing symmetric submodular functions}},
	Volume = {82},
	Year = {1998}}

@inproceedings{EvDaKaMa04,
	Author = {Even-Dar, E and Kakade, S M and Mansour, Y},
	Pages = {401--408},
	Title = {{Experts in a {\{}M{\}}arkov Decision Process}}}

@article{Wil96,
	Author = {Willems, F M J},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {2210--2217},
	Title = {{Coding for a binary independent piecewise-identically-distributed source}},
	Volume = {IT-42},
	Year = {1996}}

@inproceedings{bresina2002planning,
	Author = {Bresina, John and Dearden, Richard and Meuleau, Nicolas and Ramakrishnan, Sailesh and Smith, David and Washington, Rich},
	Booktitle = {Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence},
	Pages = {77--84},
	Title = {{Planning Under Continuous Time and Resource Uncertainty: A Challenge for {\{}AI{\}}}},
	Year = {2002}}

@inproceedings{shalev-shwartz2004online,
	Address = {New York, NY, USA},
	Annote = {comps{\_}distance},
	Author = {Shalev-Shwartz, Shai and Singer, Yoram and Ng, Andrew Y},
	Booktitle = {ICML '04: Proceedings of the twenty-first international conference on Machine learning},
	Doi = {http://doi.acm.org/10.1145/1015330.1015376},
	Isbn = {1-58113-828-5},
	Pages = {94},
	Publisher = {ACM},
	Title = {{Online and batch learning of pseudo-metrics}},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1015330.1015376}}

@inproceedings{UNK10,
	Author = {Uchiya, T and Nakamura, A and Kudo, M},
	Booktitle = {Proceedings of the 21st International Conference on Algorithmic Learning Theory (ALT)},
	Title = {{Algorithms for Adversarial Bandit Problems with Multiple Plays}},
	Year = {2010}}

@article{zhao2003face,
	Author = {Zhao, Wen-Yi and Chellappa, Rama and Phillips, P and Rosenfeld, Azriel},
	Journal = {ACM Computing Surveys},
	Number = {4},
	Pages = {399--458},
	Title = {{Face Recognition: A Literature Survey}},
	Volume = {35},
	Year = {2003}}

@inproceedings{calandriello2016analysis,
	Abstract = {Large-scale kernel ridge regression (KRR) is limited by the need to store a large kernel matrix Kt. To avoid storing the entire matrix Kt, Nystr{\"o}m methods subsample a subset of columns of the kernel matrix, and efficiently find an approximate KRR solution on the reconstructed Kt . The chosen subsampling distribution in turn affects the statistical and computational tradeoffs. For KRR problems, [15, 1] show that a sampling distribution proportional to the ridge leverage scores (RLSs) provides strong reconstruction guarantees for Kt. While exact RLSs are as difficult to compute as a KRR solution, we may be able to approximate them well enough. In this paper, we study KRR problems in a sequential setting and introduce the INK-ESTIMATE algorithm, that incrementally computes the RLSs estimates. INK-ESTIMATE maintains a small sketch of Kt, that at each step is used to compute an intermediate es- timate of the RLSs. First, our sketch update does not require access to previously seen columns, and therefore a single pass over the kernel ma- trix is sufficient. Second, the algorithm requires a fixed, small space budget to run dependent only on the effective dimension of the kernel matrix. Finally, our sketch provides strong approximation guarantees on the distance ∥Kt−Kt∥2 , and on the statistical risk of the approximate KRR solution at any time, because all our guarantees hold at any intermediate step.},
	Author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal},
	Booktitle = {Uncertainty in Artificial Intelligence},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Calandriello, Lazaric, Valko - 2016 - Analysis of Nystr{\"{o}}m method with sequential ridge leverage scores.pdf:pdf},
	Title = {{Analysis of Nystr{\"{o}}m method with sequential ridge leverage scores}},
	Year = {2016}}

@article{gyorgy07sp,
	Author = {Gy{\"{o}}rgy, A and Linder, T and Lugosi, G and Ottucs{\'{a}}k, $\backslash$relax Gy.},
	Issn = {1532-4435},
	Journal = {Journal of Machine Learning Research},
	Pages = {2369--2403},
	Publisher = {JMLR.org},
	Title = {{The On-Line Shortest Path Problem Under Partial Monitoring}},
	Volume = {8},
	Year = {2007}}

@incollection{filippi2010parametric,
	Author = {Filippi, Sarah and Cappe, Olivier and Garivier, Aur{\'{e}}lien and Szepesvari, Csaba},
	Booktitle = {Advances in Neural Information Processing Systems 23},
	Editor = {Lafferty, J and Williams, C K I and Shawe-Taylor, J and Zemel, R S and Culotta, A},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {586--594},
	Title = {{Parametric Bandits: The Generalized Linear Case}},
	Year = {2010}}

@inproceedings{ADX10,
	Author = {Agarwal, Alekh and Dekel, Ofer and Xiao, Lin},
	Booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {28--40},
	Title = {{Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback}},
	Url = {http://www.cs.berkeley.edu/{~}alekh/bandits-colt.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://www.cs.berkeley.edu/%7B~%7Dalekh/bandits-colt.pdf}}

@article{JNTV05,
	Author = {Juditsky, A and Nazin, A and Tsybakov, A and Vayatis, N},
	Journal = {Problems of Information Transmission},
	Number = {4},
	Pages = {368--384},
	Title = {{Recursive Aggregation of Estimators by the Mirror Descent Algorithm with Averaging}},
	Volume = {41},
	Year = {2005}}

@article{hodge2004survey,
	Address = {Norwell, MA, USA},
	Annote = {comps{\_}ano},
	Author = {Hodge, Victoria and Austin, Jim},
	Doi = {http://dx.doi.org/10.1023/B:AIRE.0000045502.10941.a9},
	Issn = {0269-2821},
	Journal = {Artif. Intell. Rev.},
	Number = {2},
	Pages = {85--126},
	Publisher = {Kluwer Academic Publishers},
	Title = {{A Survey of Outlier Detection Methodologies}},
	Volume = {22},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/B:AIRE.0000045502.10941.a9}}

@phdthesis{kassel1995comparison,
	Address = {Cambridge, MA, USA},
	Author = {Kassel, Robert Howard},
	Publisher = {Massachusetts Institute of Technology},
	School = {MIT Spoken Language Systems Group},
	Title = {{A comparison of approaches to on-line handwritten character recognition}},
	Year = {1995}}

@article{Hemelrijk1966,
	Author = {Hemelrijk, J},
	Doi = {10.1111/j.1467-9574.1966.tb00488.x},
	Issn = {14679574},
	Journal = {Statistica Neerlandica},
	Number = {1},
	Pages = {1--7},
	Title = {{Underlining random variables}},
	Volume = {20},
	Year = {1966},
	Bdsk-Url-1 = {https://doi.org/10.1111/j.1467-9574.1966.tb00488.x}}

@inproceedings{poupart2002piecewise,
	Author = {Poupart, Pascal and Boutilier, Craig and Patrascu, Relu and Schuurmans, Dale},
	Booktitle = {Proceedings of the 18th National Conference on Artificial Intelligence},
	Pages = {292--299},
	Title = {{Piecewise Linear Value Function Approximation for Factored {\{}MDPs{\}}}},
	Year = {2002}}

@inproceedings{auer1995gambling,
	Author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert},
	Booktitle = {Proceedings of the 36th Annual Symposium on Foundations of Computer Science},
	Pages = {322--331},
	Title = {{Gambling in a Rigged Casino: The Adversarial Multi-Armed Bandit problem}},
	Year = {1995}}

@article{joachims1999making,
	Abstract = {Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, o -the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. S V Mlight1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for S V MlightV2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains},
	Author = {Joachims, Thorsten},
	Chapter = {11},
	Doi = {10.1109/ICEMI.2009.5274151},
	Editor = {Sch{\"{o}}lkopf, B and Burges, C and Smola, A},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Joachims - 1999 - Making large-Scale SVM Learning Practical.pdf:pdf},
	Institution = {University of Dortmund},
	Isbn = {9781424438631},
	Journal = {Advances in Kernel Methods Support Vector Learning},
	Pages = {169--184},
	Publisher = {MIT Press},
	Series = {Advances in Kernel Methods - Support Vector Learning},
	Title = {{Making large-Scale SVM Learning Practical}},
	Url = {http://www-ai.cs.uni-dortmund.de/DOKUMENTE/joachims{\_}99a.ps.gz},
	Year = {1999},
	Bdsk-Url-1 = {http://www-ai.cs.uni-dortmund.de/DOKUMENTE/joachims%7B%5C_%7D99a.ps.gz},
	Bdsk-Url-2 = {https://doi.org/10.1109/ICEMI.2009.5274151}}

@inproceedings{dechter1997mini-buckets:,
	Author = {Dechter, Rina},
	Booktitle = {Proceedings of the 15th International Joint Conference on Artificial Intelligence},
	Pages = {1297--1303},
	Title = {{Mini-Buckets: A General Scheme for Generating Approximations in Automated Reasoning}},
	Year = {1997}}

@article{spielman_spectral_2011,
	Author = {Spielman, Daniel A and Teng, Shang-Hua},
	Journal = {SIAM Journal on Computing},
	Number = {4},
	Pages = {981--1025},
	Title = {{Spectral sparsification of graphs}},
	Url = {http://epubs.siam.org/doi/abs/10.1137/08074489X},
	Volume = {40},
	Year = {2011},
	Bdsk-Url-1 = {http://epubs.siam.org/doi/abs/10.1137/08074489X}}

@article{bubeck2013optimal,
	Author = {Bubeck, S{\'{e}}bastien and Ernst, Damien and Garivier, Aur{\'{e}}lien},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Bubeck, Ernst, Garivier - 2013 - Optimal discovery with probabilistic expert advice finite time analysis and macroscopic optimality.pdf:pdf},
	Journal = {Journal of Machine Learning Research},
	Pages = {601--623},
	Title = {{Optimal discovery with probabilistic expert advice: finite time analysis and macroscopic optimality}},
	Url = {http://www.jmlr.org/papers/volume14/bubeck13a/bubeck13a.pdf},
	Volume = {14},
	Year = {2013},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume14/bubeck13a/bubeck13a.pdf}}

@inproceedings{levine2011nonlinear,
	Author = {Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
	Booktitle = {NIPS},
	Isbn = {9781457700798},
	Pages = {1--9},
	Title = {{Nonlinear Inverse Reinforcement Learning with Gaussian Processes}},
	Url = {http://www.stanford.edu/{~}svlevine/papers/gpirl.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://www.stanford.edu/%7B~%7Dsvlevine/papers/gpirl.pdf}}

@inproceedings{kocak2014efficient,
	Abstract = {We consider online learning problems under a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback. In the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions. The revealed losses depend on the learner's action and a directed observation system chosen by the environment. For this setting, we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines, we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback. As the predictions of our first algorithm cannot be always computed efficiently in this setting, we propose another algorithm with similar properties and with the benefit of always being computationally efficient, at the price of a slightly more complicated tuning mechanism. Both algorithms rely on a novel exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.},
	Author = {Koc{\'{a}}k, Tom{\'{a}}{\v{s}} and Neu, Gergely and Valko, Michal and Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Efficient learning by implicit exploration in bandit problems with side observations}},
	Url = {https://papers.nips.cc/paper/5462-efficient-learning-by-implicit-exploration-in-bandit-problems-with-side-observations.pdf},
	Year = {2014},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/5462-efficient-learning-by-implicit-exploration-in-bandit-problems-with-side-observations.pdf}}

@article{hastings1970monte,
	Author = {Hastings, W K},
	Journal = {Biometrika},
	Pages = {97--109},
	Title = {{{\{}Monte Carlo{\}} Sampling Methods Using {\{}Markov{\}} Chains and Their Application}},
	Volume = {57},
	Year = {1970}}

@techreport{submodular_tutorial,
	Author = {Bach, F},
	Institution = {HAL},
	Number = {00527714},
	Title = {{Convex Analysis and Optimization with Submodular Functions: a Tutorial}},
	Year = {2010}}

@article{edmonds70submodular,
	Abstract = {The viewpoint of the subject of matroids, and related areas of lattice theory, has always been, in one way or another, abstraction$\backslash$nof algebraic dependence or, equivalently, abstraction of the incidence relations in geometric representations of algebra.$\backslash$nOften one of the main derived facts is that all bases have the same cardinality. (See Van der Waerden, Section 33.)},
	Author = {Edmonds, Jack},
	Journal = {Combinatorial Structures and Their Applications},
	Pages = {69--87},
	Title = {{Submodular functions, matroids, and certain polyhedra}},
	Year = {1970}}

@article{dreiseitl2002logistic,
	Address = {San Diego, USA},
	Author = {Dreiseitl, Stephan and Ohno-Machado, Lucila},
	Doi = {http://dx.doi.org/10.1016/S1532-0464(03)00034-0},
	Issn = {1532-0464},
	Journal = {J. of Biomedical Informatics},
	Number = {5/6},
	Pages = {352--359},
	Publisher = {Elsevier Science},
	Title = {{Logistic regression and artificial neural network classification models: a methodology review}},
	Volume = {35},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/S1532-0464(03)00034-0}}

@inproceedings{Vem10,
	Author = {Vempala, S},
	Booktitle = {IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS 2010)},
	Editor = {Lodaya, K and Mahajan, M},
	Pages = {42--64},
	Publisher = {Schloss Dagstuhl--Leibniz-Zentrum fur Informatik},
	Series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	Title = {{Recent Progress and Open Problems in Algorithmic Convex Geometry}},
	Volume = {8},
	Year = {2010}}

@book{Powell07,
	Author = {Powell, Warren B},
	Publisher = {John Wiley and Sons, New York},
	Title = {{Approximate Dynamic Programming: Solving the curses of dimensionality}},
	Year = {2007}}

@article{tesauro1994td-gammon,
	Author = {Tesauro, Gerald},
	Journal = {Neural Computation},
	Number = {2},
	Pages = {215--219},
	Title = {{{\{}TD-Gammon{\}}, a Self-Teaching Backgammon Program, Achieves Master-Level Play}},
	Volume = {6},
	Year = {1994}}

@inproceedings{narasimhan2006submodular,
	Author = {Narasimhan, M and Bilmes, J},
	Booktitle = {Adv. NIPS},
	Title = {{A submodular-supermodular procedure with applications to discriminative structure learning}},
	Volume = {19},
	Year = {2006}}

@article{denisov1984random,
	Author = {Denisov, I. V.},
	Journal = {Theory of Probability {\&} Its Applications},
	Number = {4},
	Pages = {821--824},
	Title = {{A random walk and a Wiener process near a maximum}},
	Url = {https://epubs.siam.org/doi/pdf/10.1137/1128082},
	Volume = {28},
	Year = {1984},
	Bdsk-Url-1 = {https://epubs.siam.org/doi/pdf/10.1137/1128082}}

@article{tibshirani2005sparsity,
	Author = {Tibshirani, R and Saunders, M and Rosset, S and Zhu, J and Knight, K},
	Journal = {Journal of the Royal Statistical Society. Series B, Statistical Methodology},
	Pages = {91--108},
	Title = {{Sparsity and smoothness via the fused {\{}L{\}}asso}},
	Year = {2005}}

@inproceedings{amit02parametric,
	Author = {Amit, R and Mataric, Maja J},
	Pages = {944--945},
	Title = {{A Correspondence Metric for Imitation}}}

@inproceedings{gyorgy13nearoptimal,
	Author = {Gy{\"{o}}rgy, Andr{\'{a}}s and Neu, Gergely},
	Booktitle = {Submitted to the IEEE Transactions on Information Theory},
	Title = {{Near-Optimal Rates for Limited-Delay Universal Lossy Source Coding}},
	Year = {2013}}

@techreport{zhu2003semi-superviseda,
	Author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
	Booktitle = {Proceedings of the 20th International Conference on Machine Learning},
	Institution = {School of CS, CMU},
	Pages = {912--919},
	Title = {{Semi-Supervised Learning: From Gaussian Fields to Gaussian Processes}},
	Year = {2003}}

@inproceedings{hauskrecht2007evidence-based,
	Abstract = {Anomaly detection methods can be very useful in identifying interesting or concerning events. In this work, we develop and examine new probabilistic anomaly detection methods that let us evaluate management decisions for a specific patient and identify those decisions that are highly unusual with respect to patients with the same or similar condition. The statistics used in this detection are derived from probabilistic models such as Bayesian networks that are learned from a database of past patient cases. We evaluate our methods on the problem of detection of unusual hospitalization patterns for patients with community acquired pneumonia. The results show very encouraging detection performance with 0.5 precision at 0.53 recall and give us hope that these techniques may provide the basis of intelligent monitoring systems that alert clinicians to the occurrence of unusual events or decisions.},
	Author = {Hauskrecht, Milos and Valko, Michal and Kveton, Branislav and Visweswaran, Shyam and Cooper, Gregory F},
	Booktitle = {Annual American Medical Informatics Association Symposium},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Month = {nov},
	Pages = {319--324},
	Title = {{Evidence-based anomaly detection}},
	Year = {2007}}

@article{carpentier2013honest,
	Abstract = {We study the problem of constructing honest and adaptive confidence intervals for the tail coefficient in the second order Pareto model, when the second order coefficient is unknown. This problem is translated into a testing problem on the second order parameter. By constructing an appropriate model and an associated test statistic, we provide a uniform and adaptive confidence interval for the first order parameter. We also provide an almost matching lower bound, which proves that the result is minimax optimal up to a logarithmic factor.},
	Archiveprefix = {arXiv},
	Arxivid = {1312.2968},
	Author = {Carpentier, Alexandra and Kim, Arlene K. H.},
	Eprint = {1312.2968},
	Journal = {Electronic Journal of Statistics},
	Title = {{Honest and adaptive confidence interval for the tail coefficient in the Pareto model}},
	Year = {2014}}

@inproceedings{combes2015combinatorial,
	Author = {Combes, Richard and Talebi, Mohammad Sadegh and Prouti{\`{e}}re, Alexandre and Lelarge, Marc},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Combinatorial bandits revisited}},
	Url = {https://papers.nips.cc/paper/5831-combinatorial-bandits-revisited.pdf},
	Year = {2015},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/5831-combinatorial-bandits-revisited.pdf}}

@inproceedings{K01,
	Author = {Kakade, Sham},
	Booktitle = {Advances in Neural Information Processing Systems 14 (NIPS)},
	Pages = {1531--1538},
	Title = {{A Natural Policy Gradient}},
	Year = {2001}}

@inproceedings{valko2013stochastic,
	Abstract = {We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known.},
	Author = {Valko, Michal and Carpentier, Alexandra and Munos, R{\'{e}}mi},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Stochastic simultaneous optimistic optimization}},
	Url = {http://proceedings.mlr.press/v28/valko13.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v28/valko13.pdf}}

@inproceedings{ma2003online,
	Address = {New York, NY, USA},
	Author = {Ma, Junshui and Perkins, Simon},
	Booktitle = {KDD '03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining},
	Doi = {http://doi.acm.org/10.1145/956750.956828},
	Isbn = {1-58113-737-0},
	Pages = {613--618},
	Publisher = {ACM},
	Title = {{Online novelty detection on temporal sequences}},
	Year = {2003},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/956750.956828}}

@article{friedman2010note,
	Author = {Friedman, J and Hastie, T and Tibshirani, R},
	Journal = {preprint},
	Title = {{A note on the group lasso and a sparse group lasso}},
	Year = {2010}}

@article{williams1992simple,
	Author = {Williams, Ronald},
	Journal = {Machine Learning},
	Number = {3-4},
	Pages = {229--256},
	Title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
	Volume = {8},
	Year = {1992}}

@article{titsias2009efficient,
	Abstract = {Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efficient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by continuously minimizing an objective function. We demonstrate the algorithm on regression and classification problems and we use it to estimate the parameters of a differential equation model of gene regulation.},
	Author = {Titsias, Michalis K and Lawrence, Neil D and Rattray, Magnus},
	Editor = {Koller, D and Schuurmans, D and Bengio, Y and Bottou, L},
	Journal = {Advances in Neural Information Processing Systems 21},
	Pages = {1681--1688},
	Publisher = {Citeseer},
	Title = {{Efficient Sampling for Gaussian Process Inference using Control Variables}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.74{\&}rep=rep1{\&}type=pdf},
	Volume = {21},
	Year = {2009},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.74%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@article{rohde2011estimation,
	Abstract = {Suppose that we observe entries or, more generally, linear combinations of entries of an unknown {\$}m\backslashtimes T{\$}-matrix {\$}A{\$} corrupted by noise. We are particularly interested in the high-dimensional setting where the number {\$}mT{\$} of unknown entries can be much larger than the sample size {\$}N{\$}. Motivated by several applications, we consider estimation of matrix {\$}A{\$} under the assumption that it has small rank. This can be viewed as dimension reduction or sparsity assumption. In order to shrink toward a low-rank representation, we investigate penalized least squares estimators with a Schatten-{\$}p{\$} quasi-norm penalty term, {\$}p\backslashleq1{\$}. We study these estimators under two possible assumptions---a modified version of the restricted isometry condition and a uniform bound on the ratio "empirical norm induced by the sampling operator/Frobenius norm." The main results are stated as nonasymptotic upper bounds on the prediction risk and on the Schatten-{\$}q{\$} risk of the estimators, where {\$}q\backslashin[p,2]{\$}. The rates that we obtain for the prediction risk are of the form {\$}rm/N{\$} (for {\$}m=T{\$}), up to logarithmic factors, where {\$}r{\$} is the rank of {\$}A{\$}. The particular examples of multi-task learning and matrix completion are worked out in detail. The proofs are based on tools from the theory of empirical processes. As a by-product, we derive bounds for the {\$}k{\$}th entropy numbers of the quasi-convex Schatten class embeddings {\$}S{\_}p{\^{}}M\backslashhookrightarrow S{\_}2{\^{}}M{\$}, {\$}p{\textless}1{\$}, which are of independent interest.},
	Author = {Rohde, Angelika and Tsybakov, Alexandre B.},
	Journal = {Annals of Statistics},
	Keywords = {Empirical process,High-dimensional low-rank matrices,Penalized least-squares estimator,Quasi-convex Schatten class embeddings,Schatten norm,Sparse recovery},
	Number = {2},
	Pages = {887--930},
	Title = {{Estimation of high-dimensional low-rank matrices}},
	Volume = {39},
	Year = {2011}}

@inproceedings{shental2003learning,
	Annote = {comps{\_}distance},
	Author = {Shental, Noam and Weinshall, Daphna},
	Booktitle = {In Proceedings of the Twentieth International Conference on Machine Learning},
	Pages = {11--18},
	Title = {{Learning Distance Functions using Equivalence Relations}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.8086},
	Year = {2003},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.8086}}

@inproceedings{valko2012semi-supervised,
	Abstract = {In apprenticeship learning we aim to learn a good policy by observing the behavior of an expert or a set of experts. In particular, we consider the case where the expert acts so as to maximize an unknown reward function defined as a linear combination of a set of state features. In this paper, we consider the setting where we observe many sample trajectories (i.e., sequences of states) but only one or a few of them are labeled as experts' trajectories. We investigate the conditions under which the remaining unlabeled trajectories can help in learning a policy with a good performance. In particular, we define an extension to the max-margin inverse reinforcement learning proposed by Abbeel and Ng (2004) where, at each iteration, the max-margin optimization step is replaced by a semi-supervised optimization problem which favors classifiers separating clusters of trajectories. Finally, we report empirical results on two grid-world domains showing that the semi-supervised algorithm is able to output a better policy in fewer iterations than the related algorithm that does not take the unlabeled trajectories into account.},
	Author = {Valko, Michal and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
	Booktitle = {The 24th Journal of Machine Learning Research Proceedings of the 10th European Workshop on Reinforcement Learning},
	Month = {jun},
	Pages = {131--241},
	Publisher = {Sparc},
	Title = {{Semi-supervised apprenticeship learning}},
	Url = {http://researchers.lille.inria.fr/{~}valko/hp/serve.php?what=publications/valko2012semi-supervised.pdf},
	Volume = {24},
	Year = {2012},
	Bdsk-Url-1 = {http://researchers.lille.inria.fr/%7B~%7Dvalko/hp/serve.php?what=publications/valko2012semi-supervised.pdf}}

@inproceedings{guestrin2003generalizing,
	Author = {Guestrin, Carlos and Koller, Daphne and Gearhart, Chris and Kanodia, Neal},
	Booktitle = {Proceedings of the 18th International Joint Conference on Artificial Intelligence},
	Pages = {1003--1010},
	Title = {{Generalizing Plans to New Environments in Relational {\{}MDPs{\}}}},
	Year = {2003}}

@inproceedings{narang2013signal,
	Abstract = {In this paper, we propose a novel algorithm to interpolate data defined on graphs, using signal processing concepts. The interpolation of missing values from known samples appears in various applications, such as matrix/vector completion, sampling of high-dimensional data, semi-supervised learning etc. In this paper, we formulate the data interpolation problem as a signal reconstruction problem on a graph, where a graph signal is defined as the information attached to each node (scalar or vector values mapped to the set of vertices/edges of the graph). We use recent results for sampling in graphs to find classes of bandlimited (BL) graph signals that can be reconstructed from their partially observed samples. The interpolated signal is obtained by projecting the input signal into the appropriate BL graph signal space. Additionally, we impose a `bilateral' weighting scheme on the links between known samples, which further improves accuracy. We use our proposed method for collaborative filtering in recommendation systems. Preliminary results show a very favorable trade-off between accuracy and complexity, compared to state of the art algorithms.},
	Author = {Narang, Sunil K. and Gadde, Akshay and Ortega, Antonio},
	Booktitle = {International Conference on Acoustics, Speech and Signal Processing},
	Keywords = {Graph signal processing,recommendation systems,sampling in graphs,spectral graph theory},
	Title = {{Signal processing techniques for interpolation in graph structured data}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.650.2525{\&}rep=rep1{\&}type=pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.650.2525%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{jebara2009graph,
	Author = {Jebara, Tony and Wang, Jun and Chang, Shih-Fu},
	Booktitle = {Proceedings of ICML},
	Pages = {441--448},
	Title = {{Graph construction and b-matching for semi-supervised learning}},
	Year = {2009}}

@book{bertsimas1997introduction,
	Author = {Bertsimas, Dimitris and Tsitsiklis, John},
	Publisher = {Athena Scientific},
	Title = {{Introduction to linear optimization}},
	Year = {1997}}

@inproceedings{lazarevic2005feature,
	Address = {New York, NY, USA},
	Annote = {comps{\_}ano},
	Author = {Lazarevic, Aleksandar and Kumar, Vipin},
	Booktitle = {KDD '05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining},
	Doi = {http://doi.acm.org/10.1145/1081870.1081891},
	Isbn = {1-59593-135-X},
	Pages = {157--166},
	Publisher = {ACM},
	Title = {{Feature bagging for outlier detection}},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1081870.1081891}}

@inproceedings{chapman2006comparison,
	Abstract = {Automated syndromic surveillance systems often classify patients into syndromic categories based on free-text chief complaints. Chief complaints (CC) demonstrate low to moderate sensitivity in identify-ing syndromic cases. Emergency Department (ED) reports promise more detailed clinical information that may increase sensitivity of detection.},
	Author = {Chapman, Wendy W and Dowling, John N and Cooper, Gregory F and Hauskrecht, Milos and Valko, Michal},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Chapman et al. - 2006 - A Comparison of Chief Complaints and Emergency Department Reports for Identifying Patients with Acute Lower Resp.pdf:pdf},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Title = {{A Comparison of Chief Complaints and Emergency Department Reports for Identifying Patients with Acute Lower Respiratory Syndrome}},
	Year = {2006}}

@article{gyorgy04efficient,
	Author = {Gy{\"{o}}rgy, A and Linder, T and Lugosi, G},
	Journal = {IEEE Transactions on Signal Processing},
	Pages = {2337--2347},
	Title = {{Efficient Adaptive Algorithms and Minimax Bounds for Zero-Delay Lossy Source Coding}},
	Volume = {52},
	Year = {2004}}

@inproceedings{tsang2006large-scale,
	Author = {Tsang, Ivor W and Kwok, James T},
	Booktitle = {NIPS},
	Title = {{Large-Scale Sparsified Manifold Regularization.}},
	Year = {2006}}

@techreport{calandriello2016analysis,
	Abstract = {We derive a new proof to show that the incremental resparsification algorithm proposed by Kelner and Levin (2013) produces a spectral sparsifier in high probability. We rigorously take into account the dependencies across subsequent resparsifications using martingale inequalities, fixing a flaw in the original analysis.},
	Archiveprefix = {arXiv},
	Arxivid = {1609.03769},
	Author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal},
	Eprint = {1609.03769},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Calandriello, Lazaric, Valko - 2016 - Analysis of Kelner and Levin graph sparsification algorithm for a streaming setting.pdf:pdf},
	Title = {{Analysis of Kelner and Levin graph sparsification algorithm for a streaming setting}},
	Url = {http://arxiv.org/abs/1609.03769},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1609.03769}}

@inproceedings{davis2007information-theoretic,
	Address = {New York, NY, USA},
	Annote = {comps{\_}distancX},
	Author = {Davis, Jason V and Kulis, Brian and Jain, Prateek and Sra, Suvrit and Dhillon, Inderjit S},
	Booktitle = {ICML '07: Proceedings of the 24th international conference on Machine learning},
	Doi = {http://doi.acm.org/10.1145/1273496.1273523},
	Isbn = {978-1-59593-793-3},
	Pages = {209--216},
	Publisher = {ACM},
	Title = {{Information-theoretic metric learning}},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1273496.1273523}}

@book{NY83,
	Author = {Nemirovski, A and Yudin, D},
	Publisher = {Wiley Interscience},
	Title = {{Problem Complexity and Method Efficiency in Optimization}},
	Year = {1983}}

@misc{seeger-submod,
	Annote = {$\backslash$url{\{}http://lapmal.epfl.ch/papers/subm{\_}lindesign.pdf{\}}},
	Author = {Seeger, M},
	Title = {{On the Submodularity of Linear Experimental Design}},
	Year = {2009}}

@inproceedings{mahadevan2006value,
	Author = {Mahadevan, Sridhar and Maggioni, Mauro},
	Booktitle = {Advances in Neural Information Processing Systems 18},
	Pages = {843--850},
	Title = {{Value Function Approximation with Diffusion Wavelets and {\{}Laplacian{\}} Eigenfunctions}},
	Year = {2006}}

@article{gal2001optimality,
	Author = {Gal, Shmuel},
	Journal = {International Journal of Game Theory},
	Number = {4},
	Pages = {533--542},
	Title = {{On the optimality of a simple strategy for searching graphs}},
	Url = {https://pdfs.semanticscholar.org/232a/ebaee2320105a12d2b47aed1e2ef21aec8e2.pdf},
	Volume = {29},
	Year = {2001},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/232a/ebaee2320105a12d2b47aed1e2ef21aec8e2.pdf}}

@article{EKMW08,
	Author = {Even-Dar, Eyal and Kearns, Michael and Mansour, Yishay and Wortman, Jennifer},
	Journal = {Machine Learning},
	Number = {1-2},
	Pages = {21--37},
	Title = {{Regret to the best vs. regret to the average}},
	Volume = {72},
	Year = {2008}}

@incollection{chickering1996learning,
	Author = {Chickering, David M},
	Booktitle = {Learning from Data: Artificial Intelligence and Statistics V},
	Editor = {Fisher, D and Lenz, H},
	Keywords = {complexity,graphical-models},
	Pages = {121--130},
	Publisher = {Springer-Verlag},
	Title = {{Learning {\{}Bayesian{\}} Networks is {\{}NP{\}}-{\{}Complete{\}}}},
	Url = {http://research.microsoft.com/copyright/accept.asp?path=http://research.microsoft.com/{~}dmax/publications/lns96.pdf{\&}{\#}38;pub=15},
	Year = {1996},
	Bdsk-Url-1 = {http://research.microsoft.com/copyright/accept.asp?path=http://research.microsoft.com/%7B~%7Ddmax/publications/lns96.pdf%7B%5C&%7D%7B%5C#%7D38;pub=15}}

@phdthesis{meila1999learning,
	Author = {Meila, Marina},
	School = {Massachusetts Institute of Technology},
	Title = {{Learning with Mixtures of Trees}},
	Year = {1999}}

@inproceedings{RaSriTe11,
	Author = {Rakhlin, A and Sridharan, K and Tewari, A},
	Title = {{Online Learning: Stochastic and Constrained Adversaries}}}

@article{hoefling910path,
	Author = {Hoefling, H},
	Journal = {Journal of Computational and Graphical Statistics},
	Number = {4},
	Pages = {984--1006},
	Title = {{A path algorithm for the fused {\{}L{\}}asso signal approximator}},
	Volume = {19},
	Year = {2010}}

@misc{sailing,
	Annote = {http://www.sor.princeton.edu/{\~{}}rvdb/sail/sail.html},
	Author = {Vanderbei, R},
	Publisher = {University of Princeton},
	Title = {{Optimal sailing strategies, statistics and operations research program}},
	Year = {1996}}

@article{choquet1953theory,
	Author = {Choquet, G},
	Journal = {Ann. Inst. Fourier},
	Pages = {131--295},
	Title = {{Theory of capacities}},
	Volume = {5},
	Year = {1954}}

@article{hary12cooperation,
	Author = {Hartmann, B and D{\'{a}}n, A},
	Journal = {IEEE Transactions on Sustainable Energy},
	Number = {1},
	Pages = {49--56},
	Title = {{Cooperation of a grid-connected wind farm and an energy storage unit demonstration of a simulation tool}},
	Volume = {3},
	Year = {2012}}

@inproceedings{HuPo04,
	Author = {Hutter, Marcus and Poland, Jan},
	Booktitle = {Algorithmic Learning Theory},
	Pages = {279--293},
	Title = {{Prediction with Expert Advice by Following the Perturbed Leader for General Weights}},
	Year = {2004}}

@book{bellman1957dynamic,
	Address = {Princeton, NJ},
	Author = {Bellman, Richard},
	Publisher = {Princeton University Press},
	Title = {{Dynamic Programming}},
	Year = {1957}}

@inproceedings{busoniu2012optimistic,
	Abstract = {The reinforcement learning community has recently intensified its interest in online plan-ning methods, due to their relative inde-pendence on the state space size. However, tight near-optimality guarantees are not yet available for the general case of stochastic Markov decision processes and closed-loop, state-dependent planning policies. We there-fore consider an algorithm related to AO* that optimistically explores a tree represen-tation of the space of closed-loop policies, and we analyze the near-optimality of the action it returns after n tree node expan-sions. While this optimistic planning requires a finite number of actions and possible next states for each transition, its asymptotic per-formance does not depend directly on these numbers, but only on the subset of nodes that significantly impact near-optimal poli-cies. We characterize this set by introduc-ing a novel measure of problem complexity, called the near-optimality exponent. Special-izing the exponent and performance bound for some interesting classes of MDPs illus-trates the algorithm works better when there are fewer near-optimal policies and less uni-form transition probabilities.},
	Author = {Bu{\c s}oniu, Lucian and Munos, R{\'{e}}mi},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{Optimistic planning for Markov decision processes}},
	Year = {2012}}

@article{krause11submodularity,
	Author = {Krause, Andreas and Guestrin, Carlos},
	Journal = {ACM Transactions on Intelligent Systems and Technology},
	Number = {4},
	Title = {{Submodularity and its Applications in Optimized Information Gathering}},
	Volume = {2},
	Year = {2011}}

@techreport{meuleau2001exploration,
	Author = {Meuleau, Nicolas and Peshkin, Leonid and Kim, Kee-Eung},
	Institution = {Massachusetts Institute of Technology},
	Number = {1713 (AI Memo 2001-003)},
	Title = {{Exploration in Gradient-Based Reinforcement Learning}},
	Year = {2001}}

@article{iwata2001combinatorial,
	Author = {Iwata, S and Fleischer, L and Fujishige, Satoru},
	Journal = {Journal of the ACM},
	Number = {4},
	Pages = {761--777},
	Publisher = {ACM},
	Title = {{A combinatorial strongly polynomial algorithm for minimizing submodular functions}},
	Volume = {48},
	Year = {2001}}

@article{agarwal2011stochastic,
	Abstract = {This paper addresses the problem of minimizing a convex, Lipschitz function f over a convex, compact set xset under a stochastic bandit feedback model. In this model, the algorithm is allowed to observed noisy realizations of the function value f(x) at any query point x in xset. The quantity of interest is regret of the algorithm, which is the sum of the function values at algorithm's query points minus the optimal function value. We demonstrate a generalization of the ellipsoid algorithm that incurs otil(poly(d)sqrtT) regret. Since any algorithm has regret at least Omega(sqrtT) on this problem, our algorithm is optimal in terms of the scaling with T.},
	Author = {Agarwal, Alekh and Foster, Dean P and Hsu, Daniel and Kakade, Sham M and Rakhlin, Alexander},
	Journal = {Statistics},
	Pages = {1--24},
	Title = {{Stochastic convex optimization with bandit feedback}},
	Url = {http://arxiv.org/abs/1107.1744},
	Year = {2011},
	Bdsk-Url-1 = {http://arxiv.org/abs/1107.1744}}

@inproceedings{farajtabar2016multistage,
	Abstract = {We consider the problem of how to optimize multi-stage campaigning over social networks. The dynamic programming framework is employed to balance the high present reward and large penalty on low future outcome in the presence of extensive uncertainties. In particular, we establish theoretical foundations of optimal campaigning over social networks where the user activities are modeled as a multivariate Hawkes process, and we derive a time dependent linear relation between the intensity of exogenous events and several commonly used objective functions of campaigning. We further develop a convex dynamic programming framework for determining the optimal intervention policy that prescribes the required level of external drive at each stage for the desired campaigning result. Experiments on both synthetic data and the real-world MemeTracker dataset show that our algorithm can steer the user activities for optimal campaigning much more accurately than baselines.},
	Author = {Farajtabar, Mehrdad and Ye, Xiaojing and Harati, Sahar and Song, Le and Zha, Hongyuan},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Farajtabar et al. - 2016 - Multistage Campaigning in Social Networks.pdf:pdf},
	Title = {{Multistage campaigning in social networks}},
	Year = {2016}}

@unpublished{gautier2018dppy,
	Abstract = {Determinantal point processes (DPPs) are specific probability distributions over clouds of points that are used as models and computational tools across physics, probability, statistics, and more recently machine learning. Sampling from DPPs is a challenge and therefore we present DPPy, a Python toolbox that gathers known exact and approximate sampling algorithms. The project is hosted on GitHub and equipped with an extensive documentation. This documentation takes the form of a short survey of DPPs and relates each mathematical property with DPPy objects.},
	Author = {Gautier, Guillaume and Bardenet, R{\'{e}}mi and Valko, Michal},
	Title = {{DPPy: Sampling determinantal point processes with Python}},
	Year = {2018}}

@article{bala2001conformism,
	Abstract = {When there are competing technologies or products with unknown payoffs an important question is which technology will prevail and whether technologies with different payoffs can coexist in the long run. In this paper, we use a social learning model with local interactions to study this question. We show that the adoption of technologies as well as the prospects of conformism/diversity depend crucially on the nature of interaction between individuals and the heterogeneity of preferences in a society.},
	Author = {Bala, Venkatesh and Goyal, Sanjeev},
	Journal = {Economic Theory},
	Pages = {101--120},
	Title = {{Conformism and diversity under social learning}},
	Volume = {17},
	Year = {2001}}

@article{fujishige1980lexicographically,
	Author = {Fujishige, Satoru},
	Journal = {Mathematics of Operations Research},
	Number = {2},
	Pages = {186--196},
	Publisher = {JSTOR},
	Title = {{Lexicographically optimal base of a polymatroid with respect to a weight vector}},
	Volume = {5},
	Year = {1980}}

@article{RM51,
	Author = {Robbins, H and Monro, S},
	Journal = {Annals of Mathematical Statistics},
	Pages = {400--407},
	Title = {{A stochastic approximation method}},
	Volume = {22},
	Year = {1951}}

@book{shawe2004kernel,
	Author = {Shawe-Taylor, John and Cristianini, Nelo},
	Publisher = {Cambridge University Press},
	Title = {{Kernel methods for pattern analysis}},
	Year = {2004}}

@inproceedings{audibert09minimax,
	Annote = {From Duplicate 1 (Minimax Policies for Bandits Games - Audibert, J.-Y.; Bubeck, S)

Submitted},
	Author = {Audibert, J.-Y. and Bubeck, S},
	Booktitle = {Journal of Machine Learning Research},
	Title = {{Minimax Policies for Bandits Games}},
	Year = {2010}}

@inproceedings{hazan2011beyond,
	Author = {Hazan, Elad and Kale, Satyen},
	Booktitle = {Conference on Learning Theory},
	Title = {{Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization.}},
	Year = {2011}}

@article{foster11neigh,
	Author = {Foster, Dean P and Rakhlin, Alexander},
	Journal = {CoRR},
	Title = {{No Internal Regret via Neighborhood Watch}},
	Volume = {abs/1108.6},
	Year = {2011}}

@article{zhan2010distributions,
	Abstract = {In this paper, the important issue of Laplacian eigenvalue distributions is investigated through theory-guided extensive numerical simulations, for four typical complex network models, namely, the ER random-graph networks, WS and NW small-world networks, and BA scale-free networks. It is found that these four types of complex networks share some common features, particularly similarities between the Laplacian eigenvalue distributions and the node degree distributions. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
	Author = {Zhan, Choujun and Chen, Guanrong and Yeung, Lam F.},
	Journal = {Physica A: Statistical Mechanics and its Applications},
	Keywords = {Complex network,Eigenvalue,Graph theory,Laplacian matrix,Node-degree,Random-graph network,Scale-free network,Small-world network},
	Number = {8},
	Pages = {1779--1788},
	Title = {{On the distributions of Laplacian eigenvalues versus node degrees in complex networks}},
	Volume = {389},
	Year = {2010}}

@techreport{bouttier2017convergence,
	Abstract = {In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.},
	Archiveprefix = {arXiv},
	Arxivid = {1703.00329},
	Author = {Bouttier, Cl{\'{e}}ment and Gavra, Ioana},
	Eprint = {1703.00329},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Bouttier, Gavra - 2017 - Convergence rate of a simulated annealing algorithm with noisy observations.pdf:pdf},
	Title = {{Convergence rate of a simulated annealing algorithm with noisy observations}},
	Url = {http://arxiv.org/abs/1703.00329},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.00329}}

@phdthesis{valko2011adaptive,
	Abstract = {We develop graph-based methods for semi-supervised learning based on label propagation on a data similarity graph. When data is abundant or arrive in a stream, the problems of computation and data storage arise for any graph-based method. We propose a fast approximate online algorithm that solves for the harmonic solution on an approximate graph. We show, both empirically and theoretically, that good behavior can be achieved by collapsing nearby points into a set of local representative points that minimize distortion. Moreover, we regularize the harmonic solution to achieve better stability properties. We also present graph-based methods for detecting conditional anomalies and apply them to the identification of unusual clinical actions in hospitals. Our hypothesis is that patient-management actions that are unusual with respect to the past patients may be due to errors and that it is worthwhile to raise an alert if such a condition is encountered. Conditional anomaly detection extends standard unconditional anomaly framework but also faces new problems known as fringe and isolated points. We devise novel nonparametric graph-based methods to tackle these problems. Our methods rely on graph connectivity analysis and soft harmonic solution. Finally, we conduct an extensive human evaluation study of our conditional anomaly methods by 15 experts in critical care.},
	Author = {Valko, Michal},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Month = {aug},
	School = {University of Pittsburgh},
	Title = {{Adaptive Graph-Based Algorithms for Conditional Anomaly Detection and Semi-Supervised Learning}},
	Url = {http://researchers.lille.inria.fr/{~}valko/hp/serve.php?what=publications/valko2011adaptive.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://researchers.lille.inria.fr/%7B~%7Dvalko/hp/serve.php?what=publications/valko2011adaptive.pdf}}

@inproceedings{sha2003shallow,
	Address = {Morristown, NJ, USA},
	Annote = {c{\_}omps{\_}models},
	Author = {Sha, Fei and Pereira, Fernando},
	Booktitle = {NAACL '03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology},
	Doi = {http://dx.doi.org/10.3115/1073445.1073473},
	Pages = {134--141},
	Publisher = {Association for Computational Linguistics},
	Title = {{Shallow parsing with conditional random fields}},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.3115/1073445.1073473}}

@article{grant2006disciplined,
	Abstract = {Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, o -the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. S V Mlight1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for S V MlightV2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains},
	Author = {Grant, Michael and Boyd, Stephen and Ye, Yinyu},
	Doi = {10.1007/0-387-30528-9_7},
	Editor = {{Leo Liberti} and {Nelson Maculan}},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Grant, Boyd, Ye - 2006 - Disciplined Convex Programming and CVX.pdf:pdf},
	Institution = {Stanford},
	Isbn = {9780387282602},
	Journal = {Review Literature And Arts Of The Americas},
	Number = {3},
	Pages = {1--26},
	Publisher = {Springer},
	Series = {Nonconvex Optimization and Its Applications},
	Title = {{Disciplined Convex Programming and CVX}},
	Url = {http://www.springerlink.com/content/p00314m582n01743/},
	Volume = {C},
	Year = {2006},
	Bdsk-Url-1 = {http://www.springerlink.com/content/p00314m582n01743/},
	Bdsk-Url-2 = {https://doi.org/10.1007/0-387-30528-9_7}}

@inproceedings{calandriello2016pack,
	Abstract = {Most kernel-based methods, such as kernel regression, kernel PCA, ICA, or k-means clustering, do not scale to large datasets, because constructing and storing the kernel matrix Kn requires at least O(n2) time and space for n samples. Recent works (Alaoui 2014, Musco 2016) show that sampling points with replacement according to their ridge leverage scores (RLS) generates small dictionaries of relevant points with strong spectral approximation guarantees for Kn. The drawback of RLS-based methods is that computing exact RLS requires constructing and storing the whole kernel matrix. In this paper, we introduce SQUEAK, a new algorithm for kernel approximation based on RLS sampling that sequentially processes the dataset, storing a dictionary which creates accurate kernel matrix approximations with a number of points that only depends on the effective dimension deffgamma of the dataset. Moreover since all the RLS estimations are efficiently performed using only the small dictionary, SQUEAK never constructs the whole matrix kermatrixn, runs in linear time widetildeO(ndeffgamma3) w.r.t.n, and requires only a single pass over the dataset.},
	Author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal},
	Booktitle = {Adaptive and Scalable Nonparametric Methods in Machine Learning at Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Calandriello, Lazaric, Valko - 2016 - Pack only the essentials Adaptive dictionary learning for kernel ridge regression.pdf:pdf},
	Title = {{Pack only the essentials: Adaptive dictionary learning for kernel ridge regression}},
	Year = {2016}}

@book{rockafellar_subgrad,
	Author = {Rockafellar, R T},
	Publisher = {Heldermann Verlag, Berlin, D},
	Title = {{The theory of subgradients and its applications to problems of optimization: {\{}C{\}}onvex and nonconvex functions}},
	Year = {1981}}

@book{haykin1994neural,
	Address = {Upper Saddle River, NJ, USA},
	Author = {Haykin, Simon},
	Edition = {1st},
	Isbn = {0023527617},
	Publisher = {Prentice Hall PTR},
	Title = {{Neural Networks: A Comprehensive Foundation}},
	Year = {1994}}

@phdthesis{daume06thesis,
	Address = {Los Angeles, CA},
	Author = {{Daum{\'{e}} III}, Hal},
	School = {University of Southern California},
	Title = {{Practical Structured Learning Techniques for Natural Language Processing}},
	Url = {http://pub.hal3.name/{\#}daume06thesis},
	Year = {2006},
	Bdsk-Url-1 = {http://pub.hal3.name/%7B%5C#%7Ddaume06thesis}}

@book{BoLuMa13,
	Author = {Boucheron, S and Lugosi, G and Massart, P},
	Publisher = {Oxford University Press},
	Title = {{Concentration inequalities:A Nonasymptotic Theory of Independence}},
	Year = {2013}}

@article{burnetas1996optimal,
	Abstract = {Consider the problem of sequential sampling from m statistical populations to maximize the expected sum of outcomes in the long run. Under suitable assumptions on the unknown parameters u gQpopulations to maximize the expected sum of outcomes in the lon, it is shown that there exists a class CR of adaptive policies with the following properties: i. The expected n horizon reward p 0Vn u . under any policy p 0 in CR is equal to nm*u .yMu .log nqolog n., as n{\textordfeminine}`, where m*u . is the largest population mean and Mu . is a constant. ii. Policies in CR are asymptotically optimal within a larger class CUF of ``uniformly fast convergent'' policies in the sense that lim   . p 0 .. n{\textordfeminine}` nm* u y Vn u r nm*u .yVnp u ..F1, for any p gCUF and any u gQ such that Mu .)0. Policies in CR are specified via easily computable indices, defined as unique solutions to dual problems that arise naturally from the functional form of Mu .. In addition, the assumptions are verified for populations specified by nonparametric discrete univariate distributions with finite support. In the case of normal populations with unknown means and variances, we leave as an open problem the verification of one assumption.},
	Author = {Burnetas, Apostolos N. and Katehakis, Michael N.},
	Journal = {Advances in Applied Mathematics},
	Number = {2},
	Pages = {122--142},
	Title = {{Optimal adaptive policies for sequential allocation problems}},
	Volume = {17},
	Year = {1996}}

@book{golub83matrix,
	Author = {Golub, G H and Loan, C F Van},
	Publisher = {Johns Hopkins University Press},
	Title = {{Matrix Computations}},
	Year = {1996}}

@inproceedings{priebe2005scan,
	Abstract = {We introduce a theory of scan statistics on graphs and apply the ideas to the problem of anomaly detection in a time series of Enron email graphs.},
	Author = {Priebe, Carey E. and Conroy, John M. and Marchette, David J. and Park, Youngser},
	Booktitle = {Computational and Mathematical Organization Theory},
	Doi = {10.1007/s10588-005-5378-z},
	Issn = {1381-298X},
	Pages = {229--247},
	Title = {{Scan Statistics on Enron Graphs}},
	Volume = {11},
	Year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10588-005-5378-z}}

@article{jenatton2009structured,
	Author = {Jenatton, R and Audibert, J-Y. and Bach, F},
	Journal = {Journal of Machine Learning Research},
	Pages = {2777--2824},
	Title = {{Structured Variable Selection with Sparsity-Inducing Norms}},
	Volume = {12},
	Year = {2011}}

@article{gilks1995adaptive,
	Abstract = {Gibbs sampling is a powerful technique for statistical inference. It involves little more than sampling from full conditional distributions, which can be both complex and computationally expensive to evaluate. Gilks and Wild have shown that in practice full conditionals are often log-concave, and they proposed a method of adaptive rejection sampling for efficiently sampling from univariate log-concave distributions. In this paper, to deal with non-log-concave full conditional distributions, we generalize adaptive rejection sampling to include a Hastings-Metropolis algorithm step. One important field of application in which statistical models may lead to non-log-concave full conditionals is population pharmacokinetics. Here, the relationship between drug dose and blood or plasma concentration in a group of patients typically is modelled by using non-linear mixed effects models. Often, the data used for analysis are routinely collected hospital measurements, which tend to be noisy and irregular. Consequently, a robust (t-distributed) error structure is appropriate to account for outlying observations and/or patients. We propose a robust non-linear full probability model for population pharmacokinetic data. We demonstrate that our method enables Bayesian inference for this model, through an analysis of antibiotic administration in new-born babies.},
	Author = {Gilks, W. R. and Best, N. G. and Tan, K. K. C.},
	Journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	Keywords = {bayesian computation,gibbs sampling,markov chain monte carlo,method,metropolis algorithm,pharmacokinetic model,random variate generation},
	Number = {4},
	Pages = {455--472},
	Title = {{Adaptive rejection metropolis sampling within Gibbs sampling}},
	Volume = {44},
	Year = {1995}}

@inproceedings{subramanya2009large,
	Author = {Subramanya, Amarnag and Bilmes, Jeff},
	Booktitle = {Workshop on Large-Scale Machine Learning: Parallelism and Massive Datasets at Neural Information Processing Systems},
	Title = {{Large-Scale Graph-based Transductive Inference}},
	Year = {2009}}

@inproceedings{chang2004locally,
	Address = {New York, NY, USA},
	Annote = {comps{\_}distancX},
	Author = {Chang, Hong and Yeung, Dit-Yan},
	Booktitle = {ICML '04: Proceedings of the twenty-first international conference on Machine learning},
	Doi = {http://doi.acm.org/10.1145/1015330.1015391},
	Isbn = {1-58113-828-5},
	Pages = {20},
	Publisher = {ACM},
	Title = {{Locally linear metric adaptation for semi-supervised clustering}},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1015330.1015391}}

@inproceedings{grill2016blazing,
	Abstract = {You are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). But you do not want to StOP with exponential running time, you want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.},
	Author = {Grill, Jean-Bastien and Valko, Michal and Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Grill, Valko, Munos - 2016 - Blazing the trails before beating the path Sample-efficient Monte-Carlo planning.pdf:pdf},
	Title = {{Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning}},
	Url = {https://papers.nips.cc/paper/6253-blazing-the-trails-before-beating-the-path-sample-efficient-monte-carlo-planning},
	Year = {2016},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/6253-blazing-the-trails-before-beating-the-path-sample-efficient-monte-carlo-planning}}

@inproceedings{grill2018optimistic,
	Abstract = {We address the problem of optimizing a Brownian motion. We consider a (random) realization W of a Brownian motion with input space in [0,1]. Given W, our goal is to return an epsilon-approximation of its maximum using the smallest possible number of function evaluations, the sample complexity of the algorithm. We provide an algorithm with sample complexity of order log2(1/epsilon). This improves over previous results of Al-Mharmah and Calvin (1996) and Calvin et al. (2017) which provided only polynomial rates. Our algorithm is adaptive---each query depends on previous values---and is an instance of the optimism-in-the-face-of-uncertainty principle.},
	Author = {Grill, Jean-Bastien and Valko, Michal and Munos, R{\'{e}}mi},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Optimistic optimization of a Brownian}},
	Year = {2018}}

@article{schrijver2000combinatorial,
	Author = {Schrijver, A},
	Journal = {Journal of Combinatorial Theory, Series B},
	Number = {2},
	Pages = {346--355},
	Publisher = {Elsevier},
	Title = {{A combinatorial algorithm minimizing submodular functions in strongly polynomial time}},
	Volume = {80},
	Year = {2000}}

@incollection{gorban2009principal,
	Author = {Gorban, Alexander and Zinovyev, Andrei},
	Booktitle = {Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods and Techniques},
	Pages = {28--59},
	Publisher = {Information Science Reference},
	Title = {{Principal Graphs and Manifolds}},
	Year = {2009}}

@article{Lawler1978,
	Abstract = {Suppose n jobs are to be sequenced for processing by a single machine, with the object of minimizing total weighted completion time. It is shown that the problem is NP-complete if there are arbitrary precedence constraints. However, if precedence constraints are ``series parallel'', the problem can be solved in O(n log n) time. This result generalizes previous results for the more special case of rooted trees. It is also shown how a decomposition procedure suggested by Sidney can be implemented in polynomial-bounded time. Equivalence of the sequencing problem with the optimal linear ordering problem for directed graphs is discussed. {\textcopyright}1978 North-Holland Publishing Company},
	Author = {Lawler, E L},
	Doi = {10.1016/S0167-5060(08)70323-6},
	Isbn = {9780720410433},
	Issn = {01675060},
	Journal = {Annals of Discrete Mathematics},
	Number = {C},
	Pages = {75--90},
	Title = {{Sequencing jobs to minimize total weighted completion time subject to precedence constraints}},
	Volume = {2},
	Year = {1978},
	Bdsk-Url-1 = {https://doi.org/10.1016/S0167-5060(08)70323-6}}

@inproceedings{audiffren2015maximum,
	Abstract = {A popular approach to apprenticeship learning (AL) is to formulate it as an inverse reinforcement learning (IRL) problem. The MaxEnt-IRL algorithm successfully integrates the maximum entropy principle into IRL and unlike its predecessors, it resolves the ambiguity arising from the fact that a possibly large number of policies could match the expert's behavior. In this paper, we study an AL setting in which in addition to the expert's trajectories, a number of unsupervised trajectories is available. We introduce MESSI, a novel algorithm that combines MaxEnt-IRL with principles coming from semi-supervised learning. In particular, MESSI integrates the unsupervised data into the MaxEnt-IRL framework using a pairwise penalty on trajectories. Empirical results in a highway driving and grid-world problems indicate that MESSI is able to take advantage of the unsupervised trajectories and improve the performance of MaxEnt-IRL.},
	Author = {Audiffren, Julien and Valko, Michal and Lazaric, Alessandro and Ghavamzadeh, Mohammad},
	Booktitle = {International Joint Conferences on Artificial Intelligence},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Audiffren et al. - 2015 - MESSI Maximum entropy semi-supervised inverse reinforcement learning.pdf:pdf},
	Title = {{MESSI: Maximum entropy semi-supervised inverse reinforcement learning}},
	Year = {2015}}

@book{kohn2000to,
	Address = {Washington DC},
	Author = {Kohn, L and Corrigan, J and Donaldson, M},
	Keywords = {imported},
	Publisher = {National Academy Press},
	Title = {{To Err Is Human: Building a Safer Health System}},
	Year = {2000}}

@misc{baxter99direct,
	Author = {Baxter, J and Bartlett, P},
	Title = {{Direct Gradient-Based Reinforcement Learning}},
	Url = {citeseer.ist.psu.edu/baxter99direct.html},
	Year = {1999},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/baxter99direct.html}}

@incollection{sinha2009semi,
	Author = {Sinha, K and M.Belkin},
	Booktitle = {Advances in Neural Information Processing Systems 22},
	Publisher = {NIPS Foundation (http://books.nips.cc)},
	Title = {{Semi-supervised Learning using Sparse Eigenfunction Bases}},
	Year = {2009}}

@incollection{wahba1999support,
	Address = {Cambridge, MA},
	Author = {Wahba, Grace},
	Booktitle = {Advances in Kernel Methods: Support Vector Learning},
	Pages = {69--88},
	Publisher = {MIT Press},
	Title = {{Support Vector Machines, Reproducing Kernel Hilbert Spaces, and Randomized GACV}},
	Year = {1999}}

@inproceedings{koolen10comphedge,
	Author = {Koolen, Wouter M. and Warmuth, Manfred K. and Kivinen, Jyrki},
	Booktitle = {Conference on Learning Theory},
	Title = {{Hedging structured concepts}},
	Year = {2010}}

@article{gyorfi2007sequential,
	Author = {Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and Ottucs{\'{a}}k, Gy{\"{o}}rgy},
	Journal = {IEEE Transactions on Information Theory},
	Number = {5},
	Pages = {1866--1872},
	Title = {{Sequential prediction of unbounded stationary time series}},
	Volume = {53},
	Year = {2007}}

@incollection{easley2010networks,
	Abstract = {Are all film stars linked to Kevin Bacon? Why do the stock markets rise and fall sharply on the strength of a vague rumour? How does gossip spread so quickly? Are we all related through six degrees of separation? There is a growing awareness of the complex networks that pervade modern society. We see them in the rapid growth of the Internet, the ease of global communication, the swift spread of news and information, and in the way epidemics and financial crises develop with startling speed and intensity. This introductory book on the new science of networks takes an interdisciplinary approach, using economics, sociology, computing, information science and applied mathematics to address fundamental questions about the links that connect us, and the ways that our decisions can have consequences for others.},
	Author = {Easley, David and Kleinberg, Jon},
	Publisher = {Cambridge University Press},
	Title = {{Networks, Crowds, and Markets: Reasoning About a Highly Connected World}},
	Year = {2010}}

@inproceedings{ziebart2008maximum,
	Abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces the problem of learning to recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decisions, while providing the same performance guarantees as existing methods.We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
	Author = {Ziebart, Brian and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
	Booktitle = {Proc AAAI},
	Editor = {Archer, M},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ziebart et al. - 2008 - Maximum Entropy Inverse Reinforcement Learning.pdf:pdf},
	Isbn = {9781577353683},
	Keywords = {irl},
	Mendeley-Tags = {irl},
	Pages = {1433--1438},
	Publisher = {AAAI Press},
	Title = {{Maximum Entropy Inverse Reinforcement Learning}},
	Url = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf}}

@article{guestrin2003efficient,
	Author = {Guestrin, Carlos and Koller, Daphne and Parr, Ronald and Venkataraman, Shobha},
	Journal = {Journal of Artificial Intelligence Research},
	Pages = {399--468},
	Title = {{Efficient Solution Algorithms for Factored {\{}MDPs{\}}}},
	Volume = {19},
	Year = {2003}}

@inproceedings{kocak2014spectral,
	Abstract = {Thompson Sampling (TS) has surged a lot of interest due to its good empirical performance, in particular in the computational advertising. Though successful, the tools for its performance analysis appeared only recently. In this paper, we describe and analyze SpectralTS algorithm for a bandit problem, where the payoffs of the choices are smooth given an underlying graph. In this setting, each choice is a node of a graph and the expected payoffs of the neighboring nodes are assumed to be similar. Although the setting has application both in recommender systems and advertising, the traditional algorithms would scale poorly with the number of choices. For that purpose we consider an effective dimension d, which is small in real-world graphs. We deliver the analysis showing that the regret of SpectralTS scales as d$\backslash$sqrt(T $\backslash$ln N) with high probability, where T is the time horizon and N is the number of choices. Since a d$\backslash$sqrt(T $\backslash$ln N) regret is comparable to the known results, SpectralTS offers a computationally more efficient alternative. We also show that our algorithm is competitive on both synthetic and real-world data.},
	Author = {Koc{\'{a}}k, Tom{\'{a}}{\v{s}} and Valko, Michal and Munos, R{\'{e}}mi and Agrawal, Shipra},
	Booktitle = {AAAI Conference on Artificial Intelligence},
	Title = {{Spectral Thompson sampling}},
	Url = {https://hal.inria.fr/hal-00981575v2/document},
	Year = {2014},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-00981575v2/document}}

@inproceedings{florez-larrahondo2005efficient,
	Abstract = {Anomaly detection systems are developed by learning a baseline-model
from a set of events captured from a computer system operating under
normal conditions. The model is then used to recognize unusual activities
as deviations from normality. Hidden Markov models (HMMs) are powerful
probabilistic finite state machines that have been used to acquire
these baseline-models. Although previous research has indicated that
HMMs can effectively represent complex sequences, the traditional
learning algorithm for HMMs is too computationally expensive for
use with real-world anomaly detection systems. This paper describes
the use of a novel incremental learning algorithm for HMMs that allows
the efficient acquisition of anomaly detection models. The new learning
algorithm requires less memory and training time than previous approaches
for learning discrete HMMs and can be used to perform online learning
of accurate baseline-models from complex computer applications to
support anomaly detection.},
	Annote = {comps{\_}anX},
	Author = {Florez-Larrahondo, German and Bridges, Susan M and Vaughn, Rayford},
	Booktitle = {Information Security},
	Doi = {10.1007/11556992_38},
	Isbn = {978-3-540-29001-8},
	Issn = {0302-9743 (Print) 1611-3349 (Online)},
	Pages = {506--514},
	Publisher = {Springer Berlin / Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {{Efficient Modeling of Discrete Events for Anomaly Detection Using Hidden Markov Models}},
	Url = {http://www.springerlink.com/content/eqdqtr9hwfyxwg3k/},
	Volume = {3650/2005},
	Year = {2005},
	Bdsk-Url-1 = {http://www.springerlink.com/content/eqdqtr9hwfyxwg3k/},
	Bdsk-Url-2 = {https://doi.org/10.1007/11556992_38}}

@inproceedings{globerson2006metric,
	Address = {Cambridge, MA},
	Annote = {comps{\_}distance},
	Author = {Globerson, Amir and Roweis, Sam},
	Booktitle = {Advances in Neural Information Processing Systems 18},
	Editor = {Weiss, Y and Sch{\"{o}}lkopf, B and Platt, J},
	Pages = {451--458},
	Publisher = {MIT Press},
	Title = {{Metric Learning by Collapsing Classes}},
	Url = {http://books.nips.cc/papers/files/nips18/NIPS2005{\_}0388.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips18/NIPS2005%7B%5C_%7D0388.pdf}}

@incollection{Haz11,
	Author = {Hazan, E},
	Booktitle = {Optimization for Machine Learning},
	Editor = {Sra, S and Nowozin, S and Wright, S},
	Pages = {287--303},
	Publisher = {MIT press},
	Title = {{The convex optimization approach to regret minimization}},
	Year = {2011}}

@inproceedings{erraqabi2016rewards,
	Abstract = {In multi-armed bandits, the most common objective is the maximization of the cumulative reward. Alternative settings include active exploration, where a learner tries to gain accurate estimates of the rewards of all arms. While these objectives are contrasting, in many scenarios it is desirable to trade off rewards and errors. For instance, in educational games the designer wants to gather generalizable knowledge about the behavior of the students and teaching strategies (small estimation errors) but, at the same time, the system needs to avoid giving a bad experience to the players, who may leave the system permanently (large reward). In this paper, we formalize this tradeoff and introduce the ForcingBalance algorithm whose performance is provably close to the best possible tradeoff strategy. Finally, we demonstrate on real-world educational data that ForcingBalance returns useful information about the arms without compromising the overall reward.},
	Author = {Erraqabi, Akram and Lazaric, Alessandro and Valko, Michal and Brunskill, Emma and Liu, Yun-en},
	Booktitle = {Challenges in Machine Learning: Gaming and Education workshop at Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Erraqabi et al. - 2016 - Rewards and errors in multi-arm bandits for interactive education.pdf:pdf},
	Title = {{Rewards and errors in multi-arm bandits for interactive education}},
	Year = {2016}}

@inproceedings{feige2007maximizing,
	Author = {Feige, U and Mirrokni, V S and Vondrak, J},
	Booktitle = {Proc. Symposium on Foundations of Computer Science},
	Organization = {IEEE Computer Society},
	Pages = {461--471},
	Title = {{Maximizing Non-Monotone Submodular Functions}},
	Year = {2007}}

@inproceedings{smola2000sparse,
	Author = {Smola, A and Scholkopf, B},
	Booktitle = {Proceedings of the 17th International Conference on Machine Learning},
	Title = {{Sparse greedy matrix approximation for machine learning}},
	Year = {2000}}

@article{boykov2001fast,
	Author = {Boykov, Y and Veksler, O and Zabih, R},
	Journal = {IEEE Trans. PAMI},
	Number = {11},
	Pages = {1222--1239},
	Title = {{Fast approximate energy minimization via graph cuts}},
	Volume = {23},
	Year = {2001}}

@article{manne1960linear,
	Author = {Manne, Alan},
	Journal = {Management Science},
	Number = {3},
	Pages = {259--267},
	Title = {{Linear Programming and Sequential Decisions}},
	Volume = {6},
	Year = {1960}}

@inproceedings{neu2014online,
	Abstract = {Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.},
	Author = {Neu, Gergely and Valko, Michal},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Neu, Valko - 2014 - Online combinatorial optimization with stochastic decision sets and adversarial losses.pdf:pdf},
	Title = {{Online combinatorial optimization with stochastic decision sets and adversarial losses}},
	Year = {2014}}

@incollection{weinberger2006distance,
	Address = {Cambridge, MA},
	Annote = {comps{\_}distance},
	Author = {Weinberger, Kilian and Blitzer, John and Saul, Lawrence},
	Booktitle = {Advances in Neural Information Processing Systems 18},
	Editor = {Weiss, Y and Sch{\"{o}}lkopf, B and Platt, J},
	Pages = {1473--1480},
	Publisher = {MIT Press},
	Title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
	Url = {http://books.nips.cc/papers/files/nips18/NIPS2005{\_}0265.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips18/NIPS2005%7B%5C_%7D0265.pdf}}

@inproceedings{neu12ssp-trans,
	Author = {Neu, Gergely and Gy{\"{o}}rgy, Andr{\'{a}}s and Szepesv{\'{a}}ri, $\backslash$textCsaba},
	Pages = {805--813},
	Title = {{The adversarial stochastic shortest path problem with unknown transition probabilities}}}

@article{gilks1992derivative,
	Author = {Gilks, W. R.},
	Journal = {Bayesian Statistics},
	Title = {{Derivative-free adaptive rejection sampling for Gibbs sampling}},
	Volume = {4},
	Year = {1992}}

@article{kapralov_single_2014,
	Abstract = {We present the first single pass algorithm for computing spectral sparsifiers of graphs in the dynamic semi-streaming model. Given a single pass over a stream containing insertions and deletions of edges to a graph G, our algorithm maintains a randomized linear sketch of the incidence matrix of G into dimension O((1/epsilon{\{}$\backslash$textasciicircum{\}}2) n polylog(n)). Using this sketch, at any point, the algorithm can output a (1 +/- epsilon) spectral sparsifier for G with high probability. While O((1/epsilon{\{}$\backslash$textasciicircum{\}}2) n polylog(n)) space algorithms are known for computing "cut sparsifiers" in dynamic streams [AGM12b, GKP12] and spectral sparsifiers in "insertion-only" streams [KL11], prior to our work, the best known single pass algorithm for maintaining spectral sparsifiers in dynamic streams required sketches of dimension Omega((1/epsilon{\{}$\backslash$textasciicircum{\}}2) n{\{}$\backslash$textasciicircum{\}}(5/3)) [AGM14]. To achieve our result, we show that, using a coarse sparsifier of G and a linear sketch of G's incidence matrix, it is possible to sample edges by effective resistance, obtaining a spectral sparsifier of arbitrary precision. Sampling from the sketch requires a novel application of ell{\_}2/ell{\_}2 sparse recovery, a natural extension of the ell{\_}0 methods used for cut sparsifiers in [AGM12b]. Recent work of [MP12] on row sampling for matrix approximation gives a recursive approach for obtaining the required coarse sparsifiers. Under certain restrictions, our approach also extends to the problem of maintaining a spectral approximation for a general matrix A{\{}$\backslash$textasciicircum{\}}T A given a stream of updates to rows in A.},
	Annote = {arXiv: 1407.1289},
	Author = {Kapralov, Michael and Lee, Yin Tat and Musco, Cameron and Musco, Christopher and Sidford, Aaron},
	Journal = {arXiv:1407.1289 [cs]},
	Keywords = {Computer Science - Data Structures and Algorithms},
	Month = {jul},
	Title = {{Single {\{}Pass{\}} {\{}Spectral{\}} {\{}Sparsification{\}} in {\{}Dynamic{\}} {\{}Streams{\}}}},
	Url = {http://arxiv.org/abs/1407.1289},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1407.1289}}

@article{bates2003improving,
	Author = {Bates, David W and Gawande, Atul. A},
	Journal = {New England Journal of Medicine},
	Number = {25},
	Pages = {2526--2534},
	Title = {{Improving Safety with Information Technology}},
	Volume = {348},
	Year = {2003}}

@inproceedings{NgJo01,
	Author = {Ng, Andrew Y and Jordan, Michael I},
	Pages = {841--848},
	Title = {{On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes}}}

@book{strongin2000global,
	Author = {Strongin, Roman and Sergeyev, Yaroslav},
	Isbn = {9780792364900},
	Publisher = {Springer},
	Series = {Nonconvex Optimization and Its Applications},
	Title = {{Global Optimization with Non-Convex Constraints: Sequential and Parallel Algorithms}},
	Url = {http://books.google.fr/books?id=xh{\_}GF9Dor3AC},
	Year = {2000},
	Bdsk-Url-1 = {http://books.google.fr/books?id=xh%7B%5C_%7DGF9Dor3AC}}

@article{arandjelovic2009methodology,
	Author = {Arandjelovic, Ognjen and Cipolla, Roberto},
	Journal = {Computer Vision and Image Understanding},
	Number = {2},
	Pages = {159--171},
	Title = {{A Methodology for Rapid Illumination-Invariant Face Recognition using Image Processing Filters}},
	Volume = {113},
	Year = {2009}}

@inproceedings{zhu2003semi-supervised,
	Author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
	Booktitle = {Proceedings of the 20th International Conference on Machine Learning},
	Pages = {912--919},
	Title = {{Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions}},
	Year = {2003}}

@inproceedings{wipf,
	Author = {Wipf, D and Nagarajan, S},
	Booktitle = {Adv. NIPS 22},
	Title = {{Sparse Estimation Using General Likelihoods and Non-Factorial Priors}},
	Year = {2009}}

@article{heard2010bayesian,
	Author = {Heard, Nicholas A and Weston, David J and Platanioti, Kiriaki and Hand, David J},
	Doi = {10.1214/10-AOAS329SUPPB},
	Journal = {Annals of Applied Statistics},
	Pages = {645--662},
	Title = {{Bayesian anomaly detection methods for social networks}},
	Volume = {4},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1214/10-AOAS329SUPPB}}

@phdthesis{neu13thesis,
	Author = {Neu, Gergely},
	School = {Budapest University of Technology and Economics},
	Title = {{Online learning in non-stochastic {\{}M{\}}arkov decision processes}},
	Year = {2013}}

@incollection{BL13,
	Author = {Bubeck, Sebastien and Liu, Che-yu},
	Booktitle = {Advances in Neural Information Processing Systems 26},
	Editor = {Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and Weinberger, K Q},
	Pages = {638--646},
	Title = {{Prior-free and prior-dependent regret bounds for Thompson Sampling}},
	Year = {2013}}

@inproceedings{chen2010scalable,
	Author = {Chen, Wei and Wang, Chi and Wang, Yajun},
	Booktitle = {Knowledge Discovery and Data Mining},
	Title = {{Scalable influence maximization for prevalent viral marketing in large-scale social networks}},
	Year = {2010}}

@inproceedings{bartletthigh,
	Author = {Bartlett, Peter L and Dani, Varsha and Hayes, Thomas P and Kakade, Sham M and Rakhlin, Alexander and Tewari, Ambuj},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {335--342},
	Title = {{High-probability Regret Bounds for Bandit Online Linear Optimization}}}

@inproceedings{park2002map,
	Author = {Park, James},
	Booktitle = {Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence},
	Pages = {388--396},
	Title = {{{\{}MAP{\}} Complexity Results and Approximation Methods}},
	Year = {2002}}

@inproceedings{kivinen2002online,
	Address = {Cambridge, MA},
	Author = {Kivinen, J and Smola, A J and Williamson, R C},
	Booktitle = {Advances in Neural Information Processing Systems 14},
	Editor = {Dietterich, T G and Becker, S and Ghahramani, Z},
	Publisher = {MIT Press},
	Title = {{Online Learning with Kernels}},
	Year = {2002}}

@article{hoeffding1963probability,
	Author = {Hoeffding, W},
	Journal = {Journal of the American Statistical Association},
	Pages = {13--30},
	Title = {{Probability inequalities for sums of bounded random variables}},
	Url = {https://www.csee.umbc.edu/{~}lomonaco/f08/643/hwk643/Hoeffding.pdf},
	Volume = {58},
	Year = {1963},
	Bdsk-Url-1 = {https://www.csee.umbc.edu/%7B~%7Dlomonaco/f08/643/hwk643/Hoeffding.pdf}}

@inproceedings{turian06advances,
	Address = {Morristown, NJ, USA},
	Author = {Turian, Joseph and Melamed, I Dan},
	Booktitle = {ACL '06: Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics},
	Doi = {http://dx.doi.org/10.3115/1220175.1220285},
	Pages = {873--880},
	Publisher = {Association for Computational Linguistics},
	Title = {{Advances in discriminative parsing}},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.3115/1220175.1220285}}

@article{megiddo74optimal,
	Abstract = {The concept of an optimal flow in a multiple source, multiple sink network is defined. It generalizes maximal flow in a single source, single sink network. An existence proof and an algorithm are given.},
	Author = {Megiddo, Nimrod},
	Journal = {Mathematical Programming},
	Number = {1},
	Pages = {97--107},
	Title = {{Optimal flows in networks with multiple sources and sinks}},
	Volume = {7},
	Year = {1974}}

@article{akoglu2015graph,
	Author = {Akoglu, Leman and Tong, Hanghang and Koutra, Danai},
	Doi = {10.1007/s10618-014-0365-y},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Akoglu, Tong, Koutra - 2015 - Graph based anomaly detection and description a survey.pdf:pdf},
	Issn = {1384-5810},
	Journal = {Data Mining and Knowledge Discovery},
	Month = {may},
	Number = {3},
	Pages = {626--688},
	Publisher = {Springer US},
	Title = {{Graph based anomaly detection and description: a survey}},
	Volume = {29},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10618-014-0365-y}}

@incollection{GP91,
	Author = {Gupta, S and Panchapakesan, S},
	Booktitle = {Handbook of Sequential Analysis},
	Editor = {Ghosh, B and Sen, P},
	Pages = {363--380},
	Title = {{On sequential ranking and selection procedures}},
	Year = {1991}}

@article{durrett1977weak,
	Author = {Durrett, Richard T. and Iglehart, Donald L. and Miller, Douglas R.},
	Journal = {The Annals of Probability},
	Number = {1},
	Pages = {117--129},
	Title = {{Weak convergence to Brownian meander and Brownian excursion}},
	Url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aop/1176995895},
	Volume = {5},
	Year = {1977},
	Bdsk-Url-1 = {https://projecteuclid.org/download/pdf%7B%5C_%7D1/euclid.aop/1176995895}}

@book{DeLu2001book,
	Author = {Devroye, L and Lugosi, G},
	Publisher = {Springer},
	Title = {{Combinatorial methods in density estimation}},
	Year = {2001}}

@book{kearfott1996rigorous,
	Author = {Kearfott, R Baker},
	Isbn = {9780792342380},
	Publisher = {Springer},
	Series = {Nonconvex Optimization and Its Applications},
	Title = {{Rigorous Global Search: Continuous Problems}},
	Url = {http://books.google.fr/books?id=GBVnnsN5yCYC},
	Year = {1996},
	Bdsk-Url-1 = {http://books.google.fr/books?id=GBVnnsN5yCYC}}

@article{fine2001efficient,
	Author = {Fine, S and Scheinberg, K},
	Journal = {Journal of Machine Learning Research},
	Pages = {243--264},
	Title = {{Efficient {\{}SVM{\}} training using low-rank kernel representations}},
	Volume = {2},
	Year = {2001}}

@article{agrawal2012thompsonarxiv,
	Abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of {\$}\backslashtilde{\{}O{\}}(d{\^{}}{\{}3/2{\}}\backslashsqrt{\{}T{\}}){\$} (or {\$}\backslashtilde{\{}O{\}}(d\backslashsqrt{\{}T \backslashlog(N){\}}){\$}), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of {\$}\backslashsqrt{\{}d{\}}{\$} (or {\$}\backslashsqrt{\{}\backslashlog(N){\}}{\$}) of the information-theoretic lower bound for this problem.},
	Archiveprefix = {arXiv},
	Arxivid = {1209.3352},
	Author = {Agrawal, Shipra and Goyal, Navin},
	Eprint = {1209.3352},
	Journal = {CoRR, abs/1209.3352, http://arxiv.org/abs/1209.3352},
	Month = {sep},
	Title = {{Thompson Sampling for Contextual Bandits with Linear Payoffs}},
	Url = {http://arxiv.org/abs/1209.3352},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1209.3352}}

@article{dunn1978conditional,
	Author = {Dunn, J C and Harshbarger, S},
	Journal = {Journal of Mathematical Analysis and Applications},
	Number = {2},
	Pages = {432--444},
	Publisher = {Elsevier},
	Title = {{Conditional gradient algorithms with open loop step size rules}},
	Volume = {62},
	Year = {1978}}

@article{seeger2008bayesian,
	Author = {Seeger, M W},
	Journal = {Journal of Machine Learning Research},
	Pages = {759--813},
	Publisher = {JMLR. org},
	Title = {{Bayesian inference and optimal design for the sparse linear model}},
	Volume = {9},
	Year = {2008}}

@inproceedings{chung1999dynamic,
	Author = {Chung, Eui-Young and Benini, Luca and de Micheli, Giovanni},
	Booktitle = {Proceedings of the 1999 IEEE / ACM International Conference on Computer-Aided Design},
	Pages = {274--279},
	Title = {{Dynamic Power Management Using Adaptive Learning Tree}},
	Year = {1999}}

@article{cunningham1984testing,
	Author = {Cunningham, W H},
	Journal = {Journal of Combinatorial Theory, Series B},
	Number = {2},
	Pages = {161--188},
	Publisher = {Elsevier},
	Title = {{Testing membership in matroid polyhedra}},
	Volume = {36},
	Year = {1984}}

@inproceedings{hoi2006learning,
	Address = {Washington, DC, USA},
	Annote = {comps{\_}distancX},
	Author = {Hoi, Steven C H and Liu, Wei and Lyu, Michael R and Ma, Wei-Ying},
	Booktitle = {CVPR '06: Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	Doi = {http://dx.doi.org/10.1109/CVPR.2006.167},
	Isbn = {0-7695-2597-0},
	Pages = {2072--2078},
	Publisher = {IEEE Computer Society},
	Title = {{Learning Distance Metrics with Contextual Constraints for Image Retrieval}},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CVPR.2006.167}}

@article{zhang1996nonparametric,
	Abstract = {Abstract Importance sampling is a widely used variance reduction simulation technique for the evaluation of high-dimensional integrals. A key step in the implementation of importance sampling is to choose a proper distribution function from which pseudorandom numbers are generated. Parametric sampling distributions, if available at all, are often inadequate for high-dimensional integrals over irregular regions. One possible remedy is to use a nonparametric method to estimate the unknown optimal sampling function. We show that the nonparametric approach yields integral estimates that converge faster than estimates obtained from parametric approaches. We also demonstrate that an adaptive method, which has been used successfully in parametric settings, does not yield better results than simple one-step methods in the nonparametric setting.},
	Author = {Zhang, Ping},
	Journal = {Journal of the American Statistical Association},
	Keywords = {Adaptive importance sampling,Integral evaluation,Kernel density estimation,Monte Carlo simulation,Variance reduction},
	Number = {435},
	Pages = {1245--1253},
	Title = {{Nonparametric importance sampling}},
	Volume = {91},
	Year = {1996}}

@article{matus1995extreme,
	Author = {Matus, F},
	Journal = {Discrete Mathematics},
	Number = {1},
	Pages = {177--192},
	Publisher = {Citeseer},
	Title = {{Extreme convex set functions with many nonnegative differences}},
	Volume = {135},
	Year = {1995}}

@article{gorur2011concave,
	Author = {G{\"{o}}r{\"{u}}r, Dilan and Teh, Yee Whye},
	Journal = {Journal of Computational and Graphical Statistics},
	Title = {{Concave-Convex adaptive rejection sampling}},
	Year = {2011}}

@book{crame1999mathematical,
	Author = {Cram{\'{e}}r, H},
	Isbn = {9780691005478},
	Publisher = {Princeton University Press},
	Series = {Princeton landmarks in mathematics and physics},
	Title = {{Mathematical methods of statistics}},
	Url = {http://books.google.com/books?id=CRTKKaJO0DYC},
	Year = {1999},
	Bdsk-Url-1 = {http://books.google.com/books?id=CRTKKaJO0DYC}}

@article{bala1998learning,
	Abstract = {When payoffs from different actions are unknown, agents use their own past experience as well as the experience of their neighbours to guide their decision making. In this paper, we develop a general framework to study the relationship between the structure of these neighbourhoods and the process of social learning. We show that, in a connected society, local learning ensures that all agents obtain the same payoffs in the long run. Thus, if actions have different payoffs, then all agents choose the same action, and social conformism obtains. We develop conditions on the distribution of prior beliefs, the structure of neighbourhoods and the informativeness of actions under which this action is optimal. In particular, we identify a property of neighbourhood structures-local independence-which greatly facilitates social learning. Simulations of the model generate spatial and temporal patterns of adoption that are consistent with empirical work.},
	Author = {Bala, Venkatesh and Goyal, Sanjeev},
	Journal = {Review of Economic Studies},
	Number = {3},
	Pages = {595--621},
	Title = {{Learning from neighbours}},
	Volume = {65},
	Year = {1998}}

@article{basu2017analysis,
	Archiveprefix = {arXiv},
	Arxivid = {1705.06808},
	Author = {Basu, Kinjal and Ghosh, Souvik},
	Eprint = {1705.06808},
	Journal = {arXiv preprint arXiv:1705.06808},
	Month = {may},
	Title = {{Analysis of Thompson sampling for Gaussian process optimization in the bandit setting}},
	Url = {http://arxiv.org/abs/1705.06808},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.06808}}

@inproceedings{blum1996on-line,
	Author = {Blum, Avrim},
	Booktitle = {Online Algorithms},
	Pages = {306--325},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {{On-line Algorithms in Machine Learning}},
	Volume = {1442},
	Year = {1996}}

@article{Lenstra1978,
	Abstract = {Precedence constraints between jobs that have to be respected in every feasible schedule generally increase the computational com- plexity of a scheduling problem. Occasionally, their introduction may turn a problem that is solvable within polynomial time into an NP-complete one, for which a good algorithm is highly unlikely to exist. We illustrate the use of these concepts by extending some typical NP-completeness results and simplifying their correctness proofs for scheduling problems involving precedence constraints.},
	Author = {Lenstra, J K and {Rinnooy Kan}, A H G},
	Doi = {10.1287/opre.26.1.22},
	Issn = {0030-364X},
	Journal = {Operations Research},
	Number = {1},
	Pages = {22--35},
	Title = {{Complexity of scheduling under precedence constraints}},
	Url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.26.1.22},
	Volume = {26},
	Year = {1978},
	Bdsk-Url-1 = {http://pubsonline.informs.org/doi/abs/10.1287/opre.26.1.22},
	Bdsk-Url-2 = {https://doi.org/10.1287/opre.26.1.22}}

@inproceedings{weinberger2007metric,
	Annote = {comps{\_}distance},
	Author = {Weinberger, K Q and Tesauro, G},
	Booktitle = {Proc. of the 11 thInternational Conference on Artificial Intelligence and Statistics},
	Title = {{Metric Learning for Kernel Regression}},
	Url = {http://www.stat.umn.edu/{~}aistat/proceedings/data/papers/077.pdf},
	Year = {2007},
	Bdsk-Url-1 = {http://www.stat.umn.edu/%7B~%7Daistat/proceedings/data/papers/077.pdf}}

@book{jordan1999learning,
	Address = {Cambridge, MA, USA},
	Editor = {Jordan, Michael I},
	Isbn = {0-262-60032-3},
	Publisher = {MIT Press},
	Title = {{Learning in graphical models}},
	Year = {1999}}

@article{arora2012deterministic,
	Author = {Arora, Raman and Dekel, Ofer and Tewari, Ambuj},
	Journal = {CoRR},
	Title = {{Deterministic {\{}MDP{\}}s with Adversarial Rewards and Bandit Feedback}},
	Volume = {abs/1210.4},
	Year = {2012}}

@article{varshamov1957estimate,
	Author = {Varshamov, Rom Rubenovich},
	Journal = {Doklady Akademii Nauk SSSR},
	Pages = {739--741},
	Title = {{Estimate of the number of signals in error correcting codes}},
	Volume = {117},
	Year = {1957}}

@inproceedings{blum2001learning,
	Address = {San Francisco, CA, USA},
	Author = {Blum, Avrim and Chawla, Shuchi},
	Booktitle = {ICML '01: Proceedings of the Eighteenth International Conference on Machine Learning},
	Isbn = {1-55860-778-1},
	Pages = {19--26},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Learning from Labeled and Unlabeled Data using Graph Mincuts}},
	Year = {2001}}

@article{mannor2004sample,
	Author = {Mannor, S and Tsitsiklis, J N},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {623--648},
	Title = {{The Sample Complexity of Exploration in the Multi-Armed Bandit Problem}},
	Volume = {5},
	Year = {2004}}

@inproceedings{kveton2005mcmc,
	Author = {Kveton, Branislav and Hauskrecht, Milos},
	Booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
	Pages = {1346--1351},
	Title = {{An {\{}MCMC{\}} Approach to Solving Hybrid Factored {\{}MDPs{\}}}},
	Year = {2005}}

@article{gnedenko1943distribution,
	Author = {Gnedenko, Boris},
	Journal = {The Annals of Mathematics},
	Number = {3},
	Pages = {423--453},
	Publisher = {JSTOR},
	Title = {{Sur la distribution limite du terme maximum d'une s{\'{e}}rie al{\'{e}}atoire}},
	Volume = {44},
	Year = {1943}}

@article{jewell1963markov-renewal,
	Author = {Jewell, William},
	Journal = {Operations Research},
	Number = {6},
	Pages = {938--948},
	Title = {{{\{}Markov{\}}-Renewal Programming. {\{}I{\}}: Formulation, Finite Return Models}},
	Volume = {11},
	Year = {1963}}

@inproceedings{dams13wireless,
	Author = {Dams, Johannes and Hoefer, Martin and Kesselheim, Thomas},
	Booktitle = {DISC},
	Editor = {Afek, Yehuda},
	Pages = {344--357},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {{Sleeping Experts in Wireless Networks.}},
	Volume = {8205},
	Year = {2013}}

@inproceedings{calandriello2017distributed,
	Abstract = {Most kernel-based methods, such as kernel regression, kernel PCA, ICA, or k-means clustering, do not scale to large datasets, because constructing and storing the kernel matrix Kn requires at least O(n2) time and space for n samples. Recent works (Alaoui 2014, Musco 2016) show that sampling points with replacement according to their ridge leverage scores (RLS) generates small dictionaries of relevant points with strong spectral approximation guarantees for Kn. The drawback of RLS-based methods is that computing exact RLS requires constructing and storing the whole kernel matrix. In this paper, we introduce SQUEAK, a new algorithm for kernel approximation based on RLS sampling that sequentially processes the dataset, storing a dictionary which creates accurate kernel matrix approximations with a number of points that only depends on the effective dimension deffgamma of the dataset. Moreover since all the RLS estimations are efficiently performed using only the small dictionary, SQUEAK never constructs the whole matrix kermatrixn, runs in linear time widetildeO(ndeffgamma3) w.r.t.n, and requires only a single pass over the dataset. We also propose a parallel and distributed version of SQUEAK achieving similar accuracy in as little as widetildeO(log(n)deffgamma3) time.},
	Author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Calandriello, Lazaric, Valko - 2017 - Distributed adaptive sampling for kernel matrix approximation.pdf:pdf},
	Title = {{Distributed adaptive sampling for kernel matrix approximation}},
	Year = {2017}}

@inproceedings{jegelka2011-fast-approx-sfm,
	Author = {Jegelka, S and Lin, H and Bilmes, J A},
	Booktitle = {Adv. NIPS},
	Title = {{Fast Approximate Submodular Minimization}},
	Year = {2011}}

@inproceedings{liu2011dynamic,
	Abstract = {We consider a large-scale cyber network with N components (e.g., paths, servers, subnets). Each component is either in a healthy state (0) or an abnormal state (1). Due to random intrusions, the state of each component transits from 0 to 1 over time according to certain stochastic process. At each time, a subset of K (K {\textless} N) components are checked and those observed in abnormal states are fixed. The objective is to design the optimal scheduling for intrusion detection such that the long-term network cost incurred by all abnormal components is minimized. We formulate the problem as a special class of Restless Multi-Armed Bandit (RMAB) process. A general RMAB suffers from the curse of dimensionality (PSPACE-hard) and numerical methods are often inapplicable. We show that, for this class of RMAB, Whittle index exists and can be obtained in closed form, leading to a low-complexity implementation of Whittle index policy with a strong performance. For homogeneous components, Whittle index policy is shown to have a simple structure that does not require any prior knowledge on the intrusion processes. Based on this structure, Whittle index policy is further shown to be optimal over a finite time horizon with an arbitrary length. Beyond intrusion detection, these results also find applications in queuing networks with finite-size buffers.},
	Archiveprefix = {arXiv},
	Arxivid = {1112.0101},
	Author = {Liu, Keqin and Zhao, Qing},
	Booktitle = {IEEE International Symposium on Information Theory Proceedings},
	Eprint = {1112.0101},
	Title = {{Dynamic Intrusion Detection in Resource-Constrained Cyber Networks}},
	Url = {http://arxiv.org/abs/1112.0101},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1112.0101}}

@inproceedings{maillard2011finite,
	Author = {Maillard, Odalric-Ambrym and Munos, R{\'{e}}mi and Stoltz, Gilles},
	Booktitle = {To appear in Proceedings of the 24th annual Conference On Learning Theory},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Series = {COLT '11},
	Title = {{Finite-Time Analysis of Multi-armed Bandits Problems with Kullback-Leibler Divergences}},
	Year = {2011}}

@book{vapnik1995nature,
	Address = {New York, NY, USA},
	Author = {Vapnik, Vladimir N},
	Isbn = {0-387-94559-8},
	Publisher = {Springer-Verlag New York, Inc.},
	Title = {{The nature of statistical learning theory}},
	Year = {1995}}

@inproceedings{shalev2007pegasos,
	Author = {Shalev-Shwartz, S and Singer, Y and Srebro, N},
	Booktitle = {Proc. ICML},
	Title = {{Pegasos: Primal estimated sub-gradient solver for svm}},
	Year = {2007}}

@inproceedings{farias2005exploration-exploitation,
	Author = {de Farias, Daniela Pucci and Megiddo, Nimrod},
	Booktitle = {Advances in Neural Information Processing Systems 17},
	Pages = {409--416},
	Title = {{Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments}},
	Year = {2005}}

@inproceedings{hauskrecht2004linear,
	Author = {Hauskrecht, Milos and Kveton, Branislav},
	Booktitle = {Proceedings of the 14th International Conference on Automated Planning and Scheduling},
	Pages = {306--314},
	Title = {{Heuristic Refinements of Approximate Linear Programming for Factored Continuous-State {\{}Markov{\}} Decision Processes}},
	Year = {2004}}

@inproceedings{blelloch2010hierarchical,
	Author = {Blelloch, Guy E and Koutis, Ioannis and Miller, Gary L and Tangwongsan, Kanat},
	Booktitle = {High Performance Computing, Networking, Storage and Analysis (SC), 2010 International Conference for},
	Organization = {IEEE},
	Pages = {1--12},
	Title = {{Hierarchical diagonal blocking and precision reduction applied to combinatorial multigrid}},
	Year = {2010}}

@book{soille,
	Author = {Soille, P},
	Publisher = {Springer},
	Title = {{Morphological Image Analysis: Principles and Applications}},
	Year = {2003}}

@inproceedings{sutton2008dyna-style,
	Author = {Sutton, Richard and Szepesvari, Csaba and Geramifard, Alborz and Bowling, Michael},
	Booktitle = {Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence},
	Pages = {528--536},
	Title = {{Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping}},
	Year = {2008}}

@inproceedings{Gupta2013,
	Abstract = {We study a general stochastic probing problem defined on a universe V, where each element e in V is "active" independently with probability p{\_}e. Elements have weights {\{}w{\_}e{\}} and the goal is to maximize the weight of a chosen subset S of active elements. However, we are given only the p{\_}e values-- to determine whether or not an element e is active, our algorithm must probe e. If element e is probed and happens to be active, then e must irrevocably be added to the chosen set S; if e is not active then it is not included in S. Moreover, the following conditions must hold in every random instantiation: (1) the set Q of probed elements satisfy an "outer" packing constraint, and (2) the set S of chosen elements satisfy an "inner" packing constraint. The kinds of packing constraints we consider are intersections of matroids and knapsacks. Our results provide a simple and unified view of results in stochastic matching and Bayesian mechanism design, and can also handle more general constraints. As an application, we obtain the first polynomial-time {\$}\backslashbackslashOmega(1/k){\$}-approximate "Sequential Posted Price Mechanism" under k-matroid intersection feasibility constraints.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1302.5913v1},
	Author = {Gupta, Anupam and Nagarajan, Viswanath},
	Booktitle = {Integer Programming and Combinatorial Optimization},
	Doi = {10.1007/978-3-642-36694-9_18},
	Eprint = {arXiv:1302.5913v1},
	Isbn = {9783642366932},
	Issn = {03029743},
	Pages = {205--216},
	Title = {{A stochastic probing problem with applications}},
	Volume = {7801},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-642-36694-9_18}}

@article{abernethy2009beating,
	Author = {Abernethy, Jacob Duncan and Rakhlin, A},
	Doi = {10.1109/ITA.2009.5044958},
	Isbn = {9781424439904},
	Journal = {2009 Information Theory and Applications Workshop},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {280--289},
	Publisher = {Ieee},
	Title = {{Beating the adaptive bandit with high probability}},
	Url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5044958},
	Year = {2009},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5044958},
	Bdsk-Url-2 = {https://doi.org/10.1109/ITA.2009.5044958}}

@inproceedings{bach2010structured,
	Author = {Bach, F},
	Booktitle = {Adv. NIPS},
	Title = {{Structured sparsity-inducing norms through submodular functions}},
	Year = {2010}}

@inproceedings{levine2012continuous,
	Author = {Levine, Sergey and Koltun, Vladlen},
	Booktitle = {ICML '12: Proceedings of the 29th International Conference on Machine Learning},
	Title = {{Continuous Inverse Optimal Control with Locally Optimal Examples}},
	Url = {http://graphics.stanford.edu/projects/cioc/cioc.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://graphics.stanford.edu/projects/cioc/cioc.pdf}}

@article{Graham1979,
	Abstract = {The theory of deterministic sequencing and scheduling has expanded rapidly during the past years. In this paper we survey the state of the art with respect to optimization and approximation algorithms and interpret these in terms of computational complexity theory. Special cases considered are single machine scheduling, identical, uniform and unrelated parallel machine scheduling, and open shop, flow shop and job shop scheduling. We indicate some problems for future research and include a selective bibliography. {\textcopyright}1979, North-Holland Publishing Company.},
	Author = {Graham, R L and Lawler, E L and Lenstra, J K and Kan, A H.G.Rinnooy},
	Doi = {10.1016/S0167-5060(08)70356-X},
	Isbn = {9780080867670},
	Issn = {01675060},
	Journal = {Annals of Discrete Mathematics},
	Number = {C},
	Pages = {287--326},
	Pmid = {384},
	Title = {{Optimization and approximation in deterministic sequencing and scheduling: A survey}},
	Volume = {5},
	Year = {1979},
	Bdsk-Url-1 = {https://doi.org/10.1016/S0167-5060(08)70356-X}}

@inproceedings{das2008anomaly,
	Address = {New York, NY, USA},
	Author = {Das, Kaustav and Schneider, Jeff and Neill, Daniel B},
	Booktitle = {Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
	Isbn = {978-1-60558-193-4},
	Keywords = {anomaly detection,machine learning,pattern detection},
	Pages = {169--176},
	Publisher = {ACM},
	Series = {KDD '08},
	Title = {{Anomaly pattern detection in categorical datasets}},
	Year = {2008}}

@misc{chang2001libsvm:,
	Annote = {Software available at http://www.csie.ntu.edu.tw/{\~{}}cjlin/libsvm},
	Author = {Chang, Chih-Chung and Lin, Chih-Jen},
	Title = {{{\{}LIBSVM{\}}: a library for support vector machines}},
	Year = {2001}}

@inproceedings{ST10,
	Author = {Sridharan, K and Tewari, A},
	Booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)},
	Title = {{Convex Games in Banach Spaces}},
	Year = {2010}}

@book{bertsekas1996neuro-dynamic,
	Address = {Belmont, MA},
	Author = {Bertsekas, Dimitri and Tsitsiklis, John},
	Publisher = {Athena Scientific},
	Title = {{Neuro-Dynamic Programming}},
	Year = {1996}}

@article{baxter2001infinite-horizon,
	Author = {Baxter, Jonathan and Bartlett, Peter and Weaver, Lex},
	Journal = {Journal of Artificial Intelligence Research},
	Pages = {319--350},
	Title = {{Infinite-Horizon Policy-Gradient Estimation}},
	Volume = {15},
	Year = {2001}}

@book{chung1997spectral,
	Author = {Chung, Fan},
	Keywords = {imported},
	Publisher = {American Mathematical Society},
	Title = {{Spectral Graph Theory}},
	Year = {1997}}

@article{koltchinskii2011nuclear,
	Author = {Koltchinskii, Vladimir and Lounici, Karim and Tsybakov, Alexandre B.},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Koltchinskii, Lounici, Tsybakov - 2011 - Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion.pdf:pdf},
	Journal = {The Annals of Statistics},
	Keywords = {Lasso,Matrix completion,low-rank matrix estimation,noncommutative Bernstein inequality,optimal rate of convergence,recovery of the rank,statistical learning},
	Number = {5},
	Pages = {2302--2329},
	Publisher = {Institute of Mathematical Statistics},
	Title = {{Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion}},
	Volume = {39},
	Year = {2011}}

@article{dean1989model,
	Author = {Dean, Thomas and Kanazawa, Keiji},
	Journal = {Computational Intelligence},
	Pages = {142--150},
	Title = {{A Model for Reasoning about Persistence and Causation}},
	Volume = {5},
	Year = {1989}}

@book{press1992numerical,
	Address = {Cambridge, MA},
	Author = {Press, William and Teukolsky, Saul and Vetterling, William and Flannery, Brian},
	Publisher = {Cambridge University Press},
	Title = {{Numerical Recipes in C}},
	Year = {1992}}

@article{WK08,
	Author = {Warmuth, M and Kuzmin, D},
	Journal = {Journal of Machine Learning Research},
	Pages = {2287--2320},
	Title = {{Randomized Online {\{}PCA{\}} Algorithms with Regret Bounds that are Logarithmic in the Dimension}},
	Volume = {9},
	Year = {2008}}

@article{coulom2007efficient,
	Abstract = {A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9x9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.},
	Author = {Coulom, R{\'{e}}mi},
	Journal = {Computers and games},
	Pages = {72--83},
	Title = {{Efficient selectivity and backup operators in Monte-Carlo tree search}},
	Volume = {4630},
	Year = {2007}}

@article{boucheron2012concentration,
	Abstract = {This note describes non-asymptotic variance and tail bounds for order statistics of samples of independent identically distributed random variables. Those bounds are checked to be asymptotically tight when the sampling distribution belongs to a maximum domain of attraction. If the sampling distribution has non-decreasing hazard rate (this includes the Gaussian distribution), we derive an exponential Efron-Stein inequality for order statistics: an inequality connecting the logarithmic moment generating function of centered order statistics with exponential moments of Efron-Stein (jackknife) estimates of variance. We use this general connection to derive variance and tail bounds for order statistics of Gaussian sample. Those bounds are not within the scope of the Tsirelson-Ibragimov-Sudakov Gaussian concentration inequality. Proofs are elementary and combine R$\backslash$'enyi's representation of order statistics and the so-called entropy approach to concentration inequalities popularized by M. Ledoux.},
	Archiveprefix = {arXiv},
	Arxivid = {1207.7209},
	Author = {Boucheron, St{\'{e}}phane and Thomas, Maud},
	Doi = {10.1214/ECP.v17-2210},
	Eprint = {1207.7209},
	Issn = {1083589X},
	Journal = {Electronic Communications in Probability},
	Keywords = {Concentration inequalities,Efron-Stein inequalities,Entropy method,Order statistics,Renyi's representation},
	Title = {{Concentration inequalities for order statistics}},
	Volume = {17},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1214/ECP.v17-2210}}

@book{schrijver2004combinatorial,
	Author = {Schrijver, A},
	Publisher = {Springer},
	Title = {{Combinatorial optimization: Polyhedra and efficiency}},
	Year = {2004}}

@article{haupt2006signal,
	Author = {Haupt, J and Nowak, R},
	Journal = {IEEE Transactions on Information Theory},
	Number = {9},
	Pages = {4036--4048},
	Publisher = {IEEE},
	Title = {{Signal reconstruction from noisy random projections}},
	Volume = {52},
	Year = {2006}}

@article{martino2011generalization,
	Author = {Martino, Luca and M{\'{i}}guez, Joaqu{\'{i}}n},
	Journal = {Statistics and Computing},
	Number = {4},
	Pages = {633--647},
	Title = {{A generalization of the adaptive rejection sampling algorithm}},
	Volume = {21},
	Year = {2011}}

@article{Auer2010,
	Abstract = {In the stochastic multi-armed bandit problem we consider a modification of the UCB algorithm of Auer et al. 4. For this modified algorithm we give an improved bound on the regret with respect to the optimal reward. While for the original UCB algorithm the regret in K-armed bandits after T trials is bounded by const K log(T)/Delta, where Delta measures the distance between a suboptimal arm and the optimal arm, for the modified UCB algorithm we show an upper bound on the regret of const K log (T/Delta 2) Delta.},
	Author = {Auer, Peter and Ortner, Ronald},
	Doi = {10.1007/s10998-010-3055-6},
	Journal = {Periodica Mathematica Hungarica},
	Keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
	Pages = {1--11},
	Title = {{UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem}},
	Volume = {61},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10998-010-3055-6}}

@inproceedings{boyan2001exact,
	Author = {Boyan, Justin and Littman, Michael},
	Booktitle = {Advances in Neural Information Processing Systems 13},
	Pages = {1026--1032},
	Title = {{Exact Solutions to Time-Dependent {\{}MDPs{\}}}},
	Year = {2001}}

@inproceedings{hauskrecht2004linear,
	Author = {Hauskrecht, Milos and Kveton, Branislav},
	Booktitle = {Advances in Neural Information Processing Systems 16},
	Pages = {895--902},
	Title = {{Linear Program Approximations for Factored Continuous-State {\{}Markov{\}} Decision Processes}},
	Year = {2004}}

@inproceedings{balluchi00automotiveengine,
	Author = {Balluchi, A and Benvenuti, L and {Di Benedetto}, M D and Pinello, C and Sangiovanni-Vincentelli, A L},
	Booktitle = {Proceedings of the IEEE},
	Pages = {888--912},
	Title = {{Automotive engine control and hybrid systems: challenges and opportunities}},
	Year = {2000}}

@misc{asuncion2007uci,
	Author = {Asuncion, A and Newman, D J},
	Institution = {University of California, Irvine, School of Information and Computer Sciences},
	Title = {{{\{}UCI{\}} Machine Learning Repository}},
	Url = {http://www.ics.uci.edu/{~}mlearn/{\%}7BMLR{\%}7Depository.html},
	Year = {2011}}

@inproceedings{ng2004inverted,
	Author = {Ng, Andrew and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
	Booktitle = {International Symposium on Experimental Robotics},
	Title = {{Inverted Autonomous Helicopter Flight via Reinforcement Learning}},
	Year = {2004}}

@inproceedings{hanawal2015cheap,
	Abstract = {We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a Omega($\backslash$sqrt(dT)) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d.},
	Author = {Hanawal, Manjesh and Saligrama, Venkatesh and Valko, Michal and Munos, R{\'{e}}mi},
	Booktitle = {International Conference on Machine Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Hanawal et al. - 2015 - Cheap bandits.pdf:pdf},
	Title = {{Cheap bandits}},
	Url = {http://proceedings.mlr.press/v37/hanawal15.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v37/hanawal15.pdf}}

@inproceedings{kveton2007adaptive,
	Author = {Kveton, Branislav and Gandhi, Prashant and Theocharous, Georgios and Mannor, Shie and Rosario, Barbara and Shah, Nilesh},
	Booktitle = {Proceedings of the 22nd National Conference on Artificial Intelligence},
	Pages = {1795--1800},
	Title = {{Adaptive Timeout Policies for Fast Fine-Grained Power Management}},
	Year = {2007}}

@article{jaynes57maxent,
	Author = {Jaynes, E T},
	Journal = {Physical Review},
	Number = {4},
	Pages = {620--630},
	Publisher = {American Physical Society},
	Title = {{Information Theory and Statistical Mechanics}},
	Volume = {106},
	Year = {1957}}

@techreport{toshev2010submodular,
	Annote = {Written Preliminary Examination},
	Author = {Toshev, A},
	Institution = {University of Pennsylvania},
	Title = {{Submodular Function Minimization}},
	Year = {2010}}

@article{gilbert1952comparison,
	Abstract = {Two channels are considered; a discrete channel which can transmit sequences of binary digits, and a continuous channel which can transmit band limited signals. The performance of a large number of simple signalling alphabets is computed and it is concluded that one cannot signal at rates near the channel capacity without using very complicated, alphabets.},
	Author = {Gilbert, Edgar Nelson},
	Journal = {Bell System Technical Journal},
	Number = {3},
	Pages = {504--522},
	Title = {{A comparison of signalling alphabets}},
	Volume = {31},
	Year = {1952}}

@inproceedings{zhang1995reinforcement,
	Author = {Zhang, Wei and Dietterich, Thomas},
	Booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence},
	Pages = {1114--1120},
	Title = {{A Reinforcement Learning Approach to Job-Shop Scheduling}},
	Year = {1995}}

@article{tsybakov1998pointwise,
	Abstract = {The problem of nonparametric function estimation in the Gaussian white noise model is considered. It is assumed that the unknown function belongs to one of the Sobolev classes, with an unknown regularity parameter. Asymptotically exact adaptive estimators of functions are proposed on the scale of Sobolev classes, with respect to pointwise and sup-norm risks. It is shown that, unlike the case of L2-risk, a loss of efficiency under adaptation is inevitable here. Bounds on the value of the loss of efficiency are obtained.},
	Author = {Tsybakov, A. B.},
	Journal = {Annals of Statistics},
	Keywords = {Adaptive nonparametric estimation,Exact constants,Gaussian white noise,Loss of efficiency under adaptation,Minimax risk,Sobolev class},
	Number = {6},
	Pages = {2420--2469},
	Title = {{Pointwise and sup-norm sharp adaptive estimation of functions on the Sobolev classes}},
	Volume = {26},
	Year = {1998}}

@book{CSV09,
	Author = {Conn, A and Scheinberg, K and Vicente, L},
	Publisher = {Society for Industrial and Applied Mathematics (SIAM)},
	Title = {{Introduction to Derivative-Free Optimization}},
	Year = {2009}}

@inproceedings{neu12o-mdp-full,
	Annote = {From Duplicate 1 (Online {\{}M{\}}arkov Decision Processes under Bandit Feedback - Neu, G; Gy{\"{o}}rgy, A; Szepesv{\'{a}}ri, $\backslash$textCs; Antos, A)

Accepted for publication},
	Author = {Neu, G and Gy{\"{o}}rgy, A and Szepesv{\'{a}}ri, $\backslash$relax Cs. $\backslash$textCs and Antos, A},
	Booktitle = {IEEE Transactions on Automatic Control},
	Pages = {1804--1812},
	Title = {{Online {\{}M{\}}arkov Decision Processes under Bandit Feedback}},
	Year = {2013}}

@article{gaiffas2011sharp,
	Abstract = {We observe (Xi,Yi)i=1n where the Yi 's are real valued outputs and the Xi's are m ? T matrices. We observe a new entry X and we want to predict the output Y associated with it. We focus on the high-dimensional setting, where m T ? n. This includes the matrix completion problem with noise, as well as other problems. We consider linear prediction procedures based on different penalizations, involving a mixture of several norms: the nuclear norm, the Frobenius norm and the ?1-norm. For these procedures, we prove sharp oracle inequalities, using a statistical learning theory point of view. A surprising fact in our results is that the rates of convergence do not depend on m and T directly. The analysis is conducted without the usually considered incoherency condition on the unknown matrix or restricted isometry condition on the sampling operator. Moreover, our results are the first to give for this problem an analysis of penalization (such as nuclear norm penalization) as a regularization algorithm: our oracle inequalities prove that these procedures have a prediction accuracy close to the deterministic oracle one, given that the reguralization parameters are well-chosen. ? 2011 IEEE.},
	Author = {Ga{\"{i}}ffas, St{\'{e}}phane and Lecu{\'{e}}, Guillaume},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ga{\"{i}}ffas, Lecu{\'{e}} - 2011 - Sharp oracle inequalities for high-dimensional matrix prediction.pdf:pdf},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Empirical process theory,Schatten norms,empirical risk minimization,high-dimensional matrix,matrix completion,nuclear norm,oracle inequalities,sparsity},
	Number = {10},
	Pages = {6942--6957},
	Title = {{Sharp oracle inequalities for high-dimensional matrix prediction}},
	Volume = {57},
	Year = {2011}}

@article{johnson74approximation,
	Author = {Johnson, David},
	Journal = {Journal of Computer and System Sciences},
	Number = {3},
	Pages = {256--278},
	Title = {{Approximation Algorithms for Combinatorial Problems}},
	Volume = {9},
	Year = {1974}}

@inproceedings{delalleau2005efficient,
	Author = {Delalleau, Olivier and Bengio, Yoshua and Roux, Nicolas Le},
	Booktitle = {AISTAT},
	Pages = {96--103},
	Title = {{Efficient Non-Parametric Function Induction in Semi-Supervised Learning}},
	Year = {2005}}

@article{ziebart2012probabilistic,
	Abstract = {Numerous interaction techniques have been developed that make "virtual" pointing at targets in graphical user interfaces easier than analogous physical pointing tasks by invoking target-based interface modifications. These pointing facilitation techniques crucially depend on methods for estimating the relevance of potential targets. Unfortunately, many of the simple methods employed to date are inaccurate in common settings with many selectable targets in close proximity. In this paper, we bring recent advances in statistical machine learning to bear on this underlying target relevance estimation problem. By framing past target-driven pointing trajectories as approximate solutions to well-studied control problems, we learn the probabilistic dynamics of pointing trajectories that enable more accurate predictions of intended targets.},
	Author = {Ziebart, Brian D and Dey, Anind K and Bagnell, J Andrew},
	Doi = {10.1145/2166966.2166968},
	Isbn = {9781450310482},
	Journal = {Proceedings of the 2012 ACM international conference on Intelligent User Interfaces IUI 12},
	Pages = {1},
	Publisher = {ACM Press},
	Series = {IUI '12},
	Title = {{Probabilistic Pointing Target Prediction via Inverse Optimal Control}},
	Url = {http://dl.acm.org/citation.cfm?doid=2166966.2166968},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?doid=2166966.2166968},
	Bdsk-Url-2 = {https://doi.org/10.1145/2166966.2166968}}

@article{ahmed2011maximizing,
	Author = {Ahmed, S and Atamt{\"{u}}rk, A},
	Journal = {Mathematical Programming: Series A and B},
	Number = {1-2},
	Pages = {149--169},
	Publisher = {Springer-Verlag New York, Inc.},
	Title = {{Maximizing a class of submodular utility functions}},
	Volume = {128},
	Year = {2011}}

@article{wong2005what,
	Address = {Cambridge, MA, USA},
	Author = {Wong, Weng-Keen and Moore, Andrew and Cooper, Gregory and Wagner, Michael},
	Issn = {1533-7928},
	Journal = {J. Mach. Learn. Res.},
	Pages = {1961--1998},
	Publisher = {MIT Press},
	Title = {{What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks}},
	Volume = {6},
	Year = {2005}}

@inproceedings{Kveton2015,
	Abstract = {A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we close the problem of computationally and sample efficient learning in stochastic combinatorial semi-bandits. In particular, we analyze a UCB-like algorithm for solving the problem, which is known to be computationally efficient; and prove {\$}O(K L (1 / \backslashDelta) \backslashlog n){\$} and {\$}O(\backslashsqrt{\{}K L n \backslashlog n{\}}){\$} upper bounds on its {\$}n{\$}-step regret, where {\$}L{\$} is the number of ground items, {\$}K{\$} is the maximum number of chosen items, and {\$}\backslashDelta{\$} is the gap between the expected returns of the optimal and best suboptimal solutions. The gap-dependent bound is tight up to a constant factor and the gap-free bound is tight up to a polylogarithmic factor.},
	Archiveprefix = {arXiv},
	Arxivid = {1410.0949},
	Author = {Kveton, Branislav and Wen, Zheng and Ashkan, Azin and Szepesvari, Csaba},
	Booktitle = {Proceedings of AISTATS},
	Eprint = {1410.0949},
	Issn = {15337928},
	Title = {{Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits}},
	Url = {http://arxiv.org/abs/1410.0949},
	Volume = {38},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1410.0949}}

@article{hauskrecht2012outlier,
	Abstract = {We develop and evaluate a data-driven approach for detecting unusual (anomalous) patient-management decisions using past patient cases stored in electronic health records (EHRs). Our hypothesis is that a patient-management decision that is unusual with respect to past patient care may be due to an error and that it is worthwhile to generate an alert if such a decision is encountered. We evaluate this hypothesis using data obtained from EHRs of 4486 post-cardiac surgical patients and a subset of 222 alerts generated from the data. We base the evaluation on the opinions of a panel of experts. The results of the study support our hypothesis that the outlier-based alerting can lead to promising true alert rates. We observed true alert rates that ranged from 25{\%} to 66{\%} for a variety of patient-management actions, with 66{\%} corresponding to the strongest outliers.},
	Author = {Hauskrecht, Milos and Batal, Iyad and Valko, Michal and Visweswaran, Shyam and Cooper, Gregory F and Clermont, Gilles},
	Doi = {10.1016/j.jbi.2012.08.004},
	Issn = {1532-0464},
	Journal = {Journal of Biomedical Informatics},
	Keywords = {Clinical alerting,Conditional outlier detection,Machine learning,Medical errors},
	Month = {feb},
	Number = {1},
	Pages = {47--55},
	Title = {{Outlier detection for patient monitoring and alerting}},
	Url = {http://www.sciencedirect.com/science/article/pii/S1532046412001281},
	Volume = {46},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1532046412001281},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jbi.2012.08.004}}

@inproceedings{Kavukcuoglu2009,
	Author = {Kavukcuoglu, K and Ranzato, M A and Fergus, R and Le-Cun, Y},
	Booktitle = {Proc. CVPR},
	Title = {{Learning invariant features through topographic filter maps}},
	Year = {2009}}

@article{fisher1936use,
	Author = {Fisher, R A},
	Journal = {Annals of Eugenics},
	Keywords = {classic classification linear-classification linea},
	Number = {7},
	Pages = {179--188},
	Title = {{The Use of Multiple Measurements in Taxonomic Problems}},
	Volume = {7},
	Year = {1936}}

@article{cunningham1985minimum,
	Author = {Cunningham, W H},
	Journal = {Networks},
	Number = {2},
	Pages = {205--215},
	Publisher = {Wiley Online Library},
	Title = {{Minimum cuts, modular functions, and matroid polyhedra}},
	Volume = {15},
	Year = {1985}}

@inproceedings{menzies2006bayesian,
	Annote = {comps{\_}anX},
	Author = {Menzies, T and Allen, D and Orrego, A},
	Booktitle = {Proceedings of the Machine Learning Algorithms for Surveillance and Event Detection Workshop},
	Title = {{Bayesian Anomaly Detection}},
	Url = {http://menzies.us/pdf/06bad.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://menzies.us/pdf/06bad.pdf}}

@inproceedings{kearns1999sparse,
	Abstract = {A critical issue for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or infinite state spaces, traditional planning and reinforcement learning algorithms may be inapplicable, since their running time typically grows linearly with the state space size in the worst case. In this paper we present a new algorithm that, given only a generative model (a natural and common type of simulator) for an arbitrary MDP, performs on-line, near-optimal planning with a per-state running time that has no dependence on the number of states. The running time is exponential in the horizon time (which depends only on the discount factor $\gamma$ and the desired degree of approximation to the optimal policy). Our algorithm thus provides a different complexity trade-off than classical algorithms such as value iteration---rather than scaling linearly in both horizon time and state space size, our running time trades an exponential dependence on the former in exchange for no dependence on the latter. Our algorithm is based on the idea of sparse sampling. We prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an MDP. Practical implementations of the algorithm are discussed, and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable MDPs.},
	Author = {Kearns, Michael and Mansour, Yishay and Ng, Andrew Y.},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Title = {{A sparse sampling algorithm for near-optimal planning in large Markov decision processes}},
	Year = {1999}}

@article{barabasi1999emergence,
	Abstract = {Systems as diverse as genetic networks or the world wide web are best described as networks with complex topology. A common property of many large networks is that the vertex connectivities follow a scale-free power-law distribution. This feature is found to be a consequence of the two generic mechanisms that networks expand continuously by the addition of new vertices, and new vertices attach preferentially to already well connected sites. A model based on these two ingredients reproduces the observed stationary scale-free distributions, indicating that the development of large networks is governed by robust self-organizing phenomena that go beyond the particulars of the individual systems.},
	Author = {Barab{\'{a}}si, Albert-L{\'{a}}szl{\'{o}} and Albert, R{\'{e}}ka},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Barab{\'{a}}si, Albert - 1999 - Emergence of scaling in random networks.pdf:pdf},
	Journal = {Science},
	Keywords = {complex networks,network,networks},
	Mendeley-Tags = {complex networks,network,networks},
	Pages = {11},
	Title = {{Emergence of scaling in random networks}},
	Volume = {286},
	Year = {1999}}

@inproceedings{saluja2014graph,
	Author = {Saluja, Avneesh and Hassan, Hany and Toutanova, Kristina and Quirk, Chris},
	Booktitle = {Proceedings of ACL'14},
	Title = {{Graph-based semi-supervised learning of translation models from monolingual data}},
	Year = {2014}}

@techreport{dymetman2012os,
	Abstract = {Most current sampling algorithms for high-dimensional distributions are based on MCMC techniques and are approximate in the sense that they are valid only asymptotically. Rejection sampling, on the other hand, produces valid samples, but is unrealistically slow in high-dimension spaces. The OS* algorithm that we propose is a unified approach to exact optimization and sampling, based on incremental refinements of a functional upper bound, which combines ideas of adaptive rejection sampling and of A* optimization search. We show that the choice of the refinement can be done in a way that ensures tractability in high-dimension spaces, and we present first experiments in two different settings: inference in high-order HMMs and in large discrete graphical models.},
	Archiveprefix = {arXiv},
	Arxivid = {1207.0742},
	Author = {Dymetman, Marc and Bouchard, Guillaume and Carter, Simon},
	Eprint = {1207.0742},
	Institution = {http://arxiv.org/abs/1207.0742},
	Title = {{The OS* algorithm: A joint approach to exact optimization and sampling}},
	Year = {2012}}

@inproceedings{Babaioff09truthful,
	Author = {Babaioff, Moshe and Sharma, Yogeshwer and Slivkins, Aleksandrs},
	Booktitle = {ACM-EC},
	Pages = {79--88},
	Title = {{Characterizing truthful multi-armed bandit mechanisms}},
	Year = {2009}}

@article{markou2003novelty,
	Address = {Amsterdam, The Netherlands, The Netherlands},
	Annote = {comps{\_}ano},
	Author = {Markou, Markos and Singh, Sameer},
	Doi = {http://dx.doi.org/10.1016/j.sigpro.2003.07.019},
	Issn = {0165-1684},
	Journal = {Signal Process.},
	Number = {12},
	Pages = {2481--2497},
	Publisher = {Elsevier North-Holland, Inc.},
	Title = {{Novelty detection: a review, part 1: statistical approaches}},
	Volume = {83},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.sigpro.2003.07.019}}

@article{fawcett1997adaptive,
	Address = {Hingham, MA, USA},
	Annote = {comps{\_}ano},
	Author = {Fawcett, Tom and Provost, Foster},
	Doi = {http://dx.doi.org/10.1023/A:1009700419189},
	Issn = {1384-5810},
	Journal = {Data Min. Knowl. Discov.},
	Number = {3},
	Pages = {291--316},
	Publisher = {Kluwer Academic Publishers},
	Title = {{Adaptive Fraud Detection}},
	Volume = {1},
	Year = {1997},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1009700419189}}

@article{Al-Mharmah1996a,
	Author = {Al-Mharmah, Hisham and Calvin, James M.},
	Journal = {Journal of Global Optimization},
	Month = {jan},
	Number = {1},
	Pages = {81--90},
	Title = {{Optimal random non-adaptive algorithm for global optimization of Brownian motion}},
	Url = {https://link.springer.com/article/10.1007/BF00229303},
	Volume = {8},
	Year = {1996},
	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/BF00229303}}

@article{gyorgy06adaptive,
	Address = {Oxford, UK},
	Author = {Gy{\"{o}}rgy, Andr{\'{a}}s and Ottucs{\'{a}}k, $\backslash$textGy{\"{o}}rgy},
	Doi = {10.1093/comjnl/bxh168},
	Issn = {0010-4620},
	Journal = {Computer Journal},
	Keywords = {adaptive routing,machine learning,network routing},
	Number = {2},
	Pages = {180--189},
	Publisher = {Oxford University Press},
	Title = {{Adaptive Routing Using Expert Advice}},
	Url = {http://dx.doi.org/10.1093/comjnl/bxh168},
	Volume = {49},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1093/comjnl/bxh168}}

@article{duane1987hybrid,
	Author = {Duane, Simon and Kennedy, A D and Pendleton, Brian and Roweth, Duncan},
	Journal = {Physics Letters B},
	Number = {2},
	Pages = {216--222},
	Title = {{Hybrid {\{}Monte Carlo{\}}}},
	Volume = {195},
	Year = {1987}}

@inproceedings{auerlogarithmic,
	Author = {Auer, Peter and Ortner, Ronald},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {49--56},
	Title = {{Logarithmic online regret bounds for undiscounted reinforcement learning}}}

@book{chapelle2006semi-supervised,
	Address = {Cambridge, MA},
	Editor = {Chapelle, O and Sch{\"{o}}lkopf, B and Zien, A},
	Publisher = {MIT Press},
	Title = {{Semi-Supervised Learning}},
	Url = {http://www.kyb.tuebingen.mpg.de/ssl-book},
	Year = {2006},
	Bdsk-Url-1 = {http://www.kyb.tuebingen.mpg.de/ssl-book}}

@article{farias2003linear,
	Author = {de Farias, Daniela Pucci and {Van Roy}, Benjamin},
	Journal = {Operations Research},
	Number = {6},
	Pages = {850--856},
	Title = {{The Linear Programming Approach to Approximate Dynamic Programming}},
	Volume = {51},
	Year = {2003}}

@book{SNW11,
	Editor = {Sra, S and Nowozin, S and Wright, S},
	Publisher = {MIT Press},
	Title = {{Optimization for Machine Learning}},
	Year = {2011}}

@incollection{fergus2009semi-supervised,
	Author = {Fergus, Rob and Weiss, Yair and Torralba, Antonio},
	Booktitle = {Advances in Neural Information Processing Systems 22},
	Editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C K I and Culotta, A},
	Pages = {522--530},
	Publisher = {NIPS Foundation (http://books.nips.cc)},
	Title = {{Semi-Supervised Learning in Gigantic Image Collections}},
	Year = {2009}}

@inproceedings{munos1999variable,
	Author = {Munos, Remi and Moore, Andrew},
	Booktitle = {Proceedings of the 16th International Joint Conference on Artificial Intelligence},
	Pages = {1348--1355},
	Title = {{Variable Resolution Discretization for High-Accuracy Solutions of Optimal Control Problems}},
	Year = {1999}}

@book{HL01,
	Author = {Hiriart-Urruty, J.-B. and Lemar{\'{e}}chal, C},
	Publisher = {Springer},
	Title = {{Fundamentals of Convex Analysis}},
	Year = {2001}}

@article{hauskrecht2000value-function,
	Author = {Hauskrecht, Milos},
	Journal = {Journal of Artificial Intelligence Research},
	Pages = {33--94},
	Title = {{Value-Function Approximations for Partially Observable {\{}Markov{\}} Decision Processes}},
	Volume = {13},
	Year = {2000}}

@article{hall1984best,
	Author = {Hall, Peter and Welsh, Alan H.},
	Doi = {10.1214/aos/1176346723},
	Journal = {The Annals of Statistics},
	Number = {3},
	Pages = {1079--1084},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {{Best Attainable Rates of Convergence for Estimates of Parameters of Regular Variation}},
	Volume = {12},
	Year = {1984},
	Bdsk-Url-1 = {https://doi.org/10.1214/aos/1176346723}}

@inproceedings{bubeck2011lipschitz,
	Author = {Bubeck, S{\'{e}}bastien and Stoltz, Gilles and Yu, Jia Yuan},
	Booktitle = {Algorithmic Learning Theory},
	Title = {{Lipschitz Bandits without the Lipschitz Constant}},
	Url = {https://arxiv.org/pdf/1105.5041.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1105.5041.pdf}}

@book{mccullagh1989generalized,
	Address = {London},
	Author = {Mccullagh, P and Nelder, J A},
	Edition = {2nd},
	Keywords = {asymptotics,glm,logit,probit,social{\_}science{\_}statistics},
	Publisher = {Chapman and Hall},
	Title = {{Generalized Linear Models}},
	Year = {1989}}

@article{Wright2009,
	Author = {Wright, S J and Nowak, R D and Figueiredo, M A T},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {7},
	Pages = {2479--2493},
	Title = {{Sparse reconstruction by separable approximation}},
	Volume = {57},
	Year = {2009}}

@incollection{he2008nearest-neighbor-based,
	Address = {Cambridge, MA},
	Annote = {comps{\_}ano},
	Author = {He, Jingrui and Carbonell, Jaime},
	Booktitle = {Advances in Neural Information Processing Systems 20},
	Editor = {Platt, J C and Koller, D and Singer, Y and Roweis, S},
	Pages = {633--640},
	Publisher = {MIT Press},
	Title = {{Nearest-Neighbor-Based Active Learning for Rare Category Detection}},
	Url = {http://books.nips.cc/papers/files/nips20/NIPS2007{\_}0051.pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips20/NIPS2007%7B%5C_%7D0051.pdf}}

@article{spielman2011graph,
	Author = {Spielman, Daniel A and Srivastava, Nikhil},
	Journal = {Journal on Computing},
	Number = {6},
	Title = {{Graph sparsification by effective resistances}},
	Volume = {40},
	Year = {2011}}

@article{Bubeck2013a,
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1205.3181v1},
	Author = {Bubeck, S. and Wang, T. and Viswanathan, N.},
	Eprint = {arXiv:1205.3181v1},
	Journal = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
	Title = {{Multiple identifications in multi-armed bandits}},
	Volume = {28},
	Year = {2013}}

@article{Jenatton2010b,
	Author = {Jenatton, R and Mairal, J and Obozinski, G and Bach, F},
	Journal = {Journal Machine Learning Research},
	Pages = {2297--2334},
	Title = {{Proximal Methods for Hierarchical Sparse Coding}},
	Volume = {12},
	Year = {2011}}

@misc{doyle2000random,
	Abstract = {A popular account of the connection between random walks and electric
networks.},
	Annote = {* Fundamental matrix of the absorbing chain * Probabilistic interpretation of current and voltage},
	Author = {Doyle, Peter G and Snell, Laurie J},
	Keywords = {resistance},
	Month = {jan},
	Title = {{Random Walks and Electric Networks}},
	Url = {http://arxiv.org/abs/math/0001057},
	Year = {2000},
	Bdsk-Url-1 = {http://arxiv.org/abs/math/0001057}}

@inproceedings{zinkevich2003online,
	Author = {Zinkevich, Martin},
	Booktitle = {Proceedings of the 20th International Conference on Machine Learning},
	Pages = {928--936},
	Title = {{Online Convex Programming and Generalized Infinitesimal Gradient Ascent}},
	Year = {2003}}

@incollection{ZiNe13,
	Author = {Zimin, A and Neu, G},
	Booktitle = {Advances in Neural Information Processing Systems 26},
	Editor = {Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and Weinberger, K Q},
	Pages = {1583--1591},
	Title = {{Online learning in episodic {\{}M{\}}arkovian decision processes by relative entropy policy search}},
	Year = {2013}}

@inproceedings{ICML2011Nagano_506,
	Author = {Nagano, K and Kawahara, Y and Aihara, K},
	Booktitle = {Proc. ICML},
	Title = {{Size-constrained Submodular Minimization through Minimum Norm Base}},
	Year = {2011}}

@article{manevitz2002one-class,
	Author = {Manevitz, Larry M and Yousef, Malik},
	Issn = {1532-4435},
	Journal = {J. Mach. Learn. Res.},
	Pages = {139--154},
	Publisher = {JMLR.org},
	Title = {{One-class svms for document classification}},
	Volume = {2},
	Year = {2002}}

@article{Heckerman1995,
	Abstract = {This article discusses an optimal troubleshooting plan. An optimal troubling plan is a sequence of observations and repairs that minimizes expected costs. The classic way to compute the expected cost of a plan is to use a decision tree with Bayesian networks. A decision tree represents the possible unfolding of events in temporal order. The representation contains two types of nodes called decision nodes and chance nodes. A decision node represents a decision which is an irrevocable allocation of resource. Branches of a decision node correspond to the mutually exclusive and collectively exhaustive set of alternatives available to the decision-maker. A chance node represents an uncertain variable. Branches of a chance node correspond to the mutually exclusive and collectively exhaustive possible states of the variable. Associated with each chance-node branch, is the decision maker's probability that the variable will be in the corresponding state. Each path through the tree reflects a possible outcome for the decision-maker. Associated with each path is the decision-maker's preference for that outcome.},
	Author = {Heckerman, David and Breese, John S and Rommelse, Koos},
	Doi = {10.1145/203330.203341},
	Issn = {00010782},
	Journal = {Communications of the ACM},
	Number = {3},
	Pages = {49--57},
	Title = {{Decision-theoretic troubleshooting}},
	Url = {http://portal.acm.org/citation.cfm?doid=203330.203341},
	Volume = {38},
	Year = {1995},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?doid=203330.203341},
	Bdsk-Url-2 = {https://doi.org/10.1145/203330.203341}}

@inproceedings{bubeck2015entropic,
	Abstract = {We prove that the Cram$\backslash$'er transform of the uniform measure on a convex body in {\$}\backslashmathbb{\{}R{\}}{\^{}}n{\$} is a {\$}(1+o(1)) n{\$}-self-concordant barrier, improving a seminal result of Nesterov and Nemirovski. This gives the first explicit construction of a universal barrier for convex bodies with optimal self-concordance parameter. The proof is based on basic geometry of log-concave distributions, and elementary duality in exponential families.},
	Author = {Bubeck, S{\'{e}}bastien and Eldan, Ronen},
	Booktitle = {Conference on Learning Theory},
	Title = {{The entropic barrier: A simple and optimal universal self-concordant barrier}},
	Year = {2015}}

@article{gelly2012grand,
	Author = {Gelly, Sylvain and Kocsis, Levente and Schoenauer, Marc and Sebag, Mich{\`{e}}le and Silver, David and Szepesv{\'{a}}ri, Csaba and Teytaud, Olivier},
	Journal = {Communications of the ACM},
	Month = {mar},
	Number = {3},
	Pages = {106--113},
	Publisher = {ACM},
	Title = {{The grand challenge of computer Go: Monte Carlo tree search and extensions}},
	Volume = {55},
	Year = {2012}}

@inproceedings{doya95novel,
	Author = {Doya, K and Sejnowski, T J},
	Pages = {101--108},
	Title = {{A Novel Reinforcement Model of Birdsong Vocalization Learning}},
	Url = {citeseer.ist.psu.edu/doya95novel.html},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/doya95novel.html}}

@inproceedings{carpentier11active,
	Author = {Carpentier, Alexandra and Lazaric, Alessandro and Ghavamzadeh, Mohammad and Munos, R{\'{e}}mi and Auer, Peter},
	Booktitle = {Algorithmic Learning Theory},
	Pages = {189--203},
	Title = {{Upper-Confidence-Bound algorithms for active learning in multi-armed bandits}},
	Year = {2011}}

@incollection{mccormick2006submodular,
	Author = {{S. Thomas McCormick}},
	Booktitle = {Handbook on Discrete Optimization},
	Chapter = {Submodular},
	Editor = {Nemhauser, G. and Aardal, K. and Weismantel, R.},
	Pages = {321--391},
	Publisher = {Elsevier},
	Title = {{Submodular function minimization}},
	Year = {2006}}

@article{zhou2003probabilistic,
	Author = {Zhou, Shaohua and Kruger, Volker and Chellappa, Rama},
	Journal = {Computer Vision and Image Understanding},
	Number = {1-2},
	Pages = {214--245},
	Title = {{Probabilistic Recognition of Human Faces from Video}},
	Volume = {91},
	Year = {2003}}

@inproceedings{carpentier2015simple,
	Abstract = {We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter {\$}\backslashbeta{\$} characterizing the distribution of the near-optimal arms. We prove that depending on {\$}\backslashbeta{\$}, our algorithm is minimax optimal either up to a multiplicative constant or up to a {\$}\backslashlog(n){\$} factor. We also provide extensions to several important cases: when {\$}\backslashbeta{\$} is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.},
	Author = {Carpentier, Alexandra and Valko, Michal},
	Booktitle = {International Conference on Machine Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Carpentier, Valko - 2015 - Simple regret for infinitely many armed bandits.pdf:pdf},
	Title = {{Simple regret for infinitely many armed bandits}},
	Year = {2015}}

@inproceedings{aggarwal2003framework,
	Author = {Aggarwal, Charu C and Han, Jiawei and Wang, Jianyong and Yu, Philip S},
	Booktitle = {Proceedings of the 29th international conference on Very large data bases - Volume 29},
	Isbn = {0-12-722442-4},
	Pages = {81--92},
	Publisher = {VLDB Endowment},
	Series = {VLDB '2003},
	Title = {{A framework for clustering evolving data streams}},
	Url = {http://portal.acm.org/citation.cfm?id=1315451.1315460},
	Year = {2003},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?id=1315451.1315460}}

@inproceedings{Kveton2010a,
	Abstract = {This paper proposes an algorithm for real-time learning without explicit feedback. The algorithm combines the ideas of semi-supervised learning on graphs and online learning. In particular, it iteratively builds a graphical representation of its world and updates it with observed examples. Labeled examples constitute the initial bias of the algorithm and are provided offline, and a stream of unlabeled examples is collected online to update this bias. We motivate the algorithm, discuss how to implement it efficiently, prove a regret bound on the quality of its solutions, and apply it to the problem of real-time face recognition. Our recognizer runs in real time, and achieves superior precision and recall on 3 challenging video datasets. {\textcopyright} 2010 IEEE.},
	Author = {Kveton, B. and Philipose, M. and Valko, M. and Huang, L.},
	Booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
	Doi = {10.1109/CVPRW.2010.5543877},
	Isbn = {9781424470297},
	Title = {{Online semi-supervised perception: Real-time learning without explicit feedback}},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPRW.2010.5543877}}

@incollection{neal2003density,
	Abstract = {I introduce a family of prior distributions over multivariate distributions, based on the use of a " Dirichlet diffusion tree " to generate exchangeable data sets. These priors can be viewed as generalizations of Dirichlet processes and of Dirichlet process mixtures, but unlike simple mixtures, they can capture the hierarchical structure present in many distributions, by means of the latent diffusion tree underlying the data. This latent tree also provides a hierarchical clustering of the data, which, unlike ad hoc clustering methods, comes with probabilistic indications of uncertainty. The relevance of each variable to the clustering can also be determined. Although Dirichlet diffusion trees are defined in terms of a continuous-time process, posterior inference involves only finite-dimensional quantities, allowing computation to be performed by reasonably efficient Markov chain Monte Carlo methods. The methods are demonstrated on problems of modeling a two-dimensional density and of clustering gene expression data. 1. INTRODUCTION Unknown distributions are encountered when estimating the density of observed data and when modeling the distribution of random effects or other latent variables. Exploratory data analysis can also be viewed in terms of finding features of the data, such as clusters, that are useful in modeling its distribution. A Bayesian model involving an unknown distribution requires a prior distribution over distributions. For such a model to be useful in practice, the prior must be an adequate approximation to our actual prior beliefs about the unknown distribution, and it must be possible to compute the predictive distribution for new data with reasonable efficiency. The Dirichlet process (Ferguson 1973) is a simple and computationally tractable prior for an unknown distribution. However, it produces distributions that are discrete with probability one, making it unsuitable for density modeling. This can be avoided by convolving the distribution with some continuous kernel, or more generally, by using a Dirichlet process to define a mixture distribution with infinitely many components, of some simple parametric form (Antoniak 1973; Ferguson 1983). Such Dirichlet process mixture models are not always ideal, however, because they use a prior distribution in which the parameters of one mixture component are independent of the parameters of other components. For many problems, we would expect instead that the components will be hierarchically organized, in ways analogous to the hierarchical grouping of organisms belonging to various species. Even if no obvious hierarchy is present, modeling a complex distribution by a mixture of simple distributions will require that each mode of the distribution be modeled using many of these simple mixture components, which will have similar parameters, and therefore form clusters themselves. Since Dirichlet process mixture models don't capture this hierarchical structure, inference using these models will be inefficient},
	Author = {Neal, Radford M.},
	Booktitle = {Bayesian Statistics},
	Pages = {619--629},
	Title = {{Density modeling and clustering using Dirichlet diffusion trees}},
	Volume = {7},
	Year = {2003}}

@article{horst1999dc,
	Author = {Horst, R and Thoai, N V},
	Journal = {Journal of Optimization Theory and Applications},
	Number = {1},
	Pages = {1--43},
	Publisher = {Springer},
	Title = {{DC programming: overview}},
	Volume = {103},
	Year = {1999}}

@inproceedings{erraqabi2017trading,
	Abstract = {In multi-armed bandits, the most common objective is the maximization of the cumulative reward. Alternative settings include active exploration, where a learner tries to gain accurate estimates of the rewards of all arms. While these objectives are contrasting, in many scenarios it is desirable to trade off rewards and errors. For instance, in educational games the designer wants to gather generalizable knowledge about the behavior of the students and teaching strategies (small estimation errors) but, at the same time, the system needs to avoid giving a bad experience to the players, who may leave the system permanently (large reward). In this paper, we formalize this tradeoff and introduce the ForcingBalance algorithm whose performance is provably close to the best possible tradeoff strategy. Finally, we demonstrate on real-world educational data that ForcingBalance returns useful information about the arms without compromising the overall reward.},
	Author = {Erraqabi, Akram and Lazaric, Alessandro and Valko, Michal and Brunskill, Emma and Liu, Yun-en},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	File = {:Users/miki/Dropbox/research/bandits/educational/2016-AISTATS-discovery-bandit/erraqabi2017trading.pdf:pdf},
	Title = {{Trading off rewards and errors in multi-armed bandits}},
	Year = {2017}}

@article{ramachandran2007bayesian,
	Abstract = {Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.},
	Author = {Ramachandran, Deepak and Amir, Eyal},
	Editor = {Shawe-Taylor, J and Zemel, R S and Bartlett, P and Pereira, F C N and Weinberger, K Q},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ramachandran, Amir - 2007 - Bayesian Inverse Reinforcement Learning.pdf:pdf},
	Journal = {Learning},
	Keywords = {markov decision processes,reinforcement learning},
	Pages = {2586--2591},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Bayesian Inverse Reinforcement Learning}},
	Url = {http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf},
	Volume = {51},
	Year = {2007},
	Bdsk-Url-1 = {http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf}}

@inproceedings{2004advances,
	Booktitle = {NIPS},
	Title = {{Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada]}},
	Year = {2004}}

@article{edmonds71matroids,
	Author = {Edmonds, Jack},
	Journal = {Mathematical Programming},
	Number = {1},
	Pages = {127--136},
	Title = {{Matroids and the Greedy Algorithm}},
	Volume = {1},
	Year = {1971}}

@inproceedings{badanidiyuru2013bandits,
	Abstract = {Multi-armed bandit problems are the predominant theoretical model of exploration-exploitation tradeoffs in learning, and they have countless applications ranging from medical trials, to communication networks, to Web search and advertising. In many of these application domains the learner may be constrained by one or more supply (or budget) limits, in addition to the customary limitation on the time horizon. The literature lacks a general model encompassing these sorts of problems. We introduce such a model, called "bandits with knapsacks", that combines aspects of stochastic integer programming with online learning. A distinctive feature of our problem, in comparison to the existing regret-minimization literature, is that the optimal policy for a given latent distribution may significantly outperform the policy that plays the optimal fixed arm. Consequently, achieving sub linear regret in the bandits-with-knapsacks problem is significantly more challenging than in conventional bandit problems. We present two algorithms whose reward is close to the information-theoretic optimum: one is based on a novel "balanced exploration" paradigm, while the other is a primal-dual algorithm that uses multiplicative updates. Further, we prove that the regret achieved by both algorithms is optimal up to polylogarithmic factors. We illustrate the generality of the problem by presenting applications in a number of different domains including electronic commerce, routing, and scheduling. As one example of a concrete application, we consider the problem of dynamic posted pricing with limited supply and obtain the first algorithm whose regret, with respect to the optimal dynamic policy, is sub linear in the supply.},
	Archiveprefix = {arXiv},
	Arxivid = {1305.2545},
	Author = {Badanidiyuru, Ashwinkumar and Kleinberg, Robert and Slivkins, Aleksandrs},
	Booktitle = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
	Doi = {10.1109/FOCS.2013.30},
	Eprint = {1305.2545},
	Isbn = {9780769551357},
	Issn = {02725428},
	Keywords = {Dynamic ad allocation,Dynamic pricing,Dynamic procurement,Exploratio},
	Pages = {207--216},
	Title = {{Bandits with knapsacks}},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/FOCS.2013.30}}

@inproceedings{IgelHusken00:iRprop,
	Author = {Igel, Christian and H{\"{u}}sken, Michael},
	Booktitle = {Proceedings of the Second International ICSC Symposium on Neural Computation (NC 2000)},
	Pages = {115--121},
	Publisher = {ICSC Academic Press},
	Title = {{Improving the {\{}R{\}}prop Learning Algorithm}},
	Url = {citeseer.ist.psu.edu/igel00improving.html},
	Year = {2000},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/igel00improving.html}}

@book{hastie2001elements,
	Abstract = {During the past decade there has been an explosion in computation
and information technology. With it has come vast amounts of data
in a variety of fields such as medicine, biology, finance, and marketing.
The challenge of understanding these data has led to the development
of new tools in the field of statistics, and spawned new areas such
as data mining, machine learning, and bioinformatics. Many of these
tools have common underpinnings but are often expressed with different
terminology. This book descibes theimprtant ideas in these areas
ina common conceptual framework. While the approach is statistical,
the emphasis is on concepts rather than mathematics. Many examples
are given, with a liberal use of color graphics. It should be a vluable
resource for statisticians and anyone interested in data mining in
science or industry. The book's coverage is broad, from supervised
learing (prediction) to unsupervised learning. The many topics include
neural networks, support vector machines, classification trees and
boosting-the first comprehensive treatment of this topic in any book.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors
of statistics at Stanford University. They are prominent researchers
in this area: Hastie and Tibshirani developed generalized additive
models and wrote a popular book of that title. Hastie wrote much
of the statistical modeling software in S-PLUS and invented principal
curves and surfaces. Tibshirani proposed the Lasso and is co-author
of the very successful An Introduction to the Bootstrap. Friedman
is the co-inventor of many data-mining tools including CART, MARS,
and projection pursuit.},
	Author = {Hastie, T and Tibshirani, R and Friedman, J H},
	Howpublished = {Hardcover},
	Isbn = {0387952845},
	Keywords = {machine-learning,statistic},
	Month = {aug},
	Publisher = {Springer},
	Title = {{The Elements of Statistical Learning}},
	Year = {2001}}

@inproceedings{guestrin2002multiagent,
	Author = {Guestrin, Carlos and Koller, Daphne and Parr, Ronald},
	Booktitle = {Advances in Neural Information Processing Systems 14},
	Pages = {1523--1530},
	Title = {{Multiagent Planning with Factored {\{}MDPs{\}}}},
	Year = {2002}}

@article{neill2010multivariate,
	Abstract = {Abstract We present the multivariate Bayesian scan statistic (MBSS), a general framework for event detection and characterization in$\backslash$nmultivariate spatial time series data. MBSS integrates prior information and observations from multiple data streams in a$\backslash$nprincipled Bayesian framework, computing the posterior probability of each type of event in each space-time region. MBSS learns$\backslash$na multivariate Gamma-Poisson model from historical data, and models the effects of each event type on each stream using expert$\backslash$nknowledge or labeled training examples. We evaluate MBSS on various disease surveillance tasks, detecting and characterizing$\backslash$noutbreaks injected into three streams of Pennsylvania medication sales data. We demonstrate that MBSS can be used both as$\backslash$na ``general'' event detector, with high detection power across a variety of event types, and a ``specific'' detector that incorporates$\backslash$nprior knowledge of an event's effects to achieve much higher detection power. MBSS has many other advantages over previous$\backslash$nevent detection approaches, including faster computation and easy interpretation and visualization of results, and allows$\backslash$nfaster and more accurate event detection by integrating information from the multiple streams. Most importantly, MBSS can$\backslash$nmodel and differentiate between multiple event types, thus distinguishing between events requiring urgent responses and other,$\backslash$nless relevant patterns in the data.},
	Author = {Neill, Daniel B. and Cooper, Gregory F.},
	Doi = {10.1007/s10994-009-5144-4},
	Issn = {0885-6125},
	Journal = {Machine Learning},
	Pages = {261--282},
	Title = {{A multivariate Bayesian scan statistic for early event detection and characterization}},
	Volume = {79},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10994-009-5144-4}}

@article{meila2001random,
	Abstract = {We present a new view of clustering and segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This view shows that spectral methods for clustering and segmentation have a probabilistic foundation. We prove that the Normalized Cut method arises naturally from our framework and we provide a complete characterization of the cases when the Normalized Cut algorithm is exact. Then we discuss other spectral segmentation and clustering methods showing that several of them are essentially the same as NCut.},
	Author = {Meila, M. and Meila, M. and Shi, J. and Shi, J.},
	Journal = {International Conference on Artificial Intelligence and Statistics},
	Title = {{A random walks view of spectral segmentation}},
	Year = {2001}}

@inproceedings{narasimhan2004pac,
	Author = {Narasimhan, M and Bilmes, J},
	Booktitle = {Proc. UAI},
	Title = {{{\{}PAC{\}}-learning bounded tree-width graphical models}},
	Year = {2004}}

@techreport{vershynin_note_2009,
	Author = {Vershynin, Roman},
	Title = {{A note on sums of independent random matrices after Ahlswede-Winter}},
	Url = {http://www.umich.edu/{~}romanv/teaching/reading-group/ahlswede-winter.pdf},
	Year = {2009},
	Bdsk-Url-1 = {http://www.umich.edu/%7B~%7Dromanv/teaching/reading-group/ahlswede-winter.pdf}}

@article{bubeck2012regret,
	Abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the Thirties, exploration-exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this survey, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
	Archiveprefix = {arXiv},
	Arxivid = {1204.5721},
	Author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicol{\`{o}}},
	Eprint = {1204.5721},
	Journal = {Foundations and Trends in Machine Learning},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {1--122},
	Title = {{Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems}},
	Url = {http://arxiv.org/abs/1204.5721},
	Volume = {5},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1204.5721}}

@inproceedings{valko2010online,
	Author = {Valko, Michal and Kveton, Branislav and Huang, Ling and Ting, Daniel},
	Booktitle = {Uncertainty in Artificial Intelligence},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Title = {{Online semi-supervised learning on quantized graphs}},
	Url = {http://researchers.lille.inria.fr/{~}valko/hp/serve.php?what=publications/valko2010online.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://researchers.lille.inria.fr/%7B~%7Dvalko/hp/serve.php?what=publications/valko2010online.pdf}}

@inproceedings{yang2006efficient,
	Annote = {comps{\_}distancX},
	Author = {Yang, Liu and Jin, Rong and Sukthankar, Rahul and Liu, Yi},
	Booktitle = {Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, July 16-20, 2006, Boston, Massachusetts, USA},
	Title = {{An Efficient Algorithm for Local Distance Metric Learning}},
	Url = {http://www.cse.msu.edu/{~}yangliu1/aaai2006-distance-v7.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://www.cse.msu.edu/%7B~%7Dyangliu1/aaai2006-distance-v7.pdf}}

@inproceedings{maddison2014a,
	Author = {Maddison, Chris J and Tarlow, Daniel and Minka, Tom},
	Booktitle = {Neural Information Processing Systems},
	Title = {{A* sampling}},
	Year = {2014}}

@book{GKKW02,
	Author = {Gy{\"{o}}rfi, L and Kohler, M and Krzyzak, A and Walk, H},
	Publisher = {Springer},
	Title = {{A Distribution-Free Theory of Nonparametric Regression}},
	Year = {2001}}

@inproceedings{geman02dynamic,
	Address = {Morristown, NJ, USA},
	Author = {Geman, Stuart and Johnson, Mark},
	Booktitle = {ACL '02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
	Doi = {http://dx.doi.org/10.3115/1073083.1073130},
	Pages = {279--286},
	Publisher = {Association for Computational Linguistics},
	Title = {{Dynamic programming for parsing and estimation of stochastic unification-based grammars}},
	Year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.3115/1073083.1073130}}

@inproceedings{eskin2000detecting,
	Address = {Seattle},
	Annote = {comps{\_}ano},
	Author = {Eskin, Eleazar},
	Booktitle = {Proc. 17th International Conf. on Machine Learning},
	Month = {apr},
	Pages = {255--262},
	Publisher = {Morgan Kaufmann, San Francisco, {\{}CA{\}}},
	Title = {{Anomaly Detection over Noisy Data using Learned Probability Distributions}},
	Url = {http://citeseer.ist.psu.edu/eskin00anomaly.html http://citeseer.comp.nus.edu.sg/542907.html},
	Year = {2000},
	Bdsk-Url-1 = {http://citeseer.ist.psu.edu/eskin00anomaly.html%20http://citeseer.comp.nus.edu.sg/542907.html}}

@incollection{bengio2004out-of-sample,
	Address = {Cambridge, MA},
	Author = {Bengio, Yoshua and Paiement, Jean-Fran{\c{c}}ois and Vincent, Pascal and Delalleau, Olivier and {Le Roux}, Nicolas and Ouimet, Marie},
	Booktitle = {Advances in Neural Information Processing Systems 16},
	Editor = {Thrun, Sebastian and Saul, Lawrence and Sch{\"{o}}lkopf, Bernhard},
	Keywords = {Isomap,Nystrom formula,dimensionality reduction,eigenfunctions learning,kernel PCA,locally linear embedding,spectral methods},
	Publisher = {MIT Press},
	Title = {{Out-of-Sample Extensions for {\{}LLE{\}}, Isomap, {\{}MDS{\}}, Eigenmaps, and Spectral Clustering}},
	Year = {2004}}

@incollection{joshi97treeadjoining,
	Address = {Berlin, New York},
	Author = {Joshi, Aravind K and Schabes, Yves},
	Booktitle = {Handbook of Formal Languages},
	Pages = {69--124},
	Publisher = {Springer},
	Title = {{Tree-Adjoining Grammars}},
	Url = {citeseer.ist.psu.edu/joshi97treeadjoining.html},
	Volume = {3},
	Year = {1997},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/joshi97treeadjoining.html}}

@inproceedings{KaSriTe08,
	Author = {Kakade, S M and Sridharan, K and Tewari, A},
	Pages = {793--800},
	Title = {{On the complexity of linear prediction: Risk bounds, margin bounds, and regularization}}}

@inproceedings{Venkataraman-2004-Mislabeled,
	Author = {Venkataraman, S and Fradkin, D Metxas D and Kulikowski., C},
	Booktitle = {16th IEEE International Conference on Tools with Artificial Intelligence},
	Pages = {356--361},
	Title = {{Distinguishing Mislabeled Data from Correctly Labeled Data in Classifier Design.}},
	Year = {2004}}

@inproceedings{azar2014online,
	Abstract = {In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel any-time {\$}\backslashmathcal{\{}X{\}}{\$}-armed bandit algorithm, and derive regret bounds matching the performance of existing state-of-the-art in terms of dependency on number of steps and smoothness factor. The main advantage of HCT is that it handles the challenging case of correlated rewards, whereas existing methods require that the reward-generating process of each arm is an identically and independent distributed (iid) random process. HCT also improves on the state-of-the-art in terms of its memory requirement as well as requiring a weaker smoothness assumption on the mean-reward function in compare to the previous anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.},
	Author = {Azar, Mohammad Gheshlaghi and Lazaric, Alessandro and Brunskill, Emma},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Online stochastic optimization under correlated bandit feedback}},
	Url = {http://proceedings.mlr.press/v32/azar14.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v32/azar14.pdf}}

@inproceedings{hendrickson1995multilevel,
	Author = {Hendrickson, B and Leland, R},
	Booktitle = {Proceedings of Supercomputing},
	Title = {{A multilevel algorithm for partitioning graphs}},
	Year = {1995}}

@incollection{oki2012glpk,
	Abstract = {The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library.},
	Author = {Oki, Eiji},
	Booktitle = {Linear Programming and Algorithms for Communication Networks - A Practical Guide to Network Design, Control, and Management},
	Title = {{GNU Linear Programming Kit, Version 4.61}},
	Url = {http://www.gnu.org/software/glpk/},
	Year = {2012},
	Bdsk-Url-1 = {http://www.gnu.org/software/glpk/}}

@inproceedings{koutis2010approaching,
	Author = {Koutis, Ioannis and Miller, Gary L. and Peng, Richard},
	Booktitle = {2010 IEEE 51st Annual Symposium on Foundations of Computer Science},
	Doi = {10.1109/FOCS.2010.29},
	Isbn = {978-1-4244-8525-3},
	Issn = {0272-5428},
	Language = {English},
	Month = {oct},
	Pages = {235--244},
	Publisher = {IEEE},
	Title = {{Approaching Optimality for Solving SDD Linear Systems}},
	Url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5671167},
	Year = {2010},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5671167},
	Bdsk-Url-2 = {https://doi.org/10.1109/FOCS.2010.29}}

@article{domingos1997optimality,
	Author = {Domingos, Pedro and Pazzani, Michael J},
	Journal = {Machine Learning},
	Number = {2-3},
	Pages = {103--130},
	Title = {{On the Optimality of the Simple Bayesian Classifier under Zero-One Loss}},
	Url = {citeseer.ist.psu.edu/article/domingos97optimality.html},
	Volume = {29},
	Year = {1997},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/article/domingos97optimality.html}}

@article{pazzani2007content,
	Abstract = {This chapter discusses content-based recommendation systems, i.e., systems that recommend an item to a user based upon a description of the item and a profile of the users interests. Content-based recommendation systems may be used in a variety of domains ranging from recommending web pages, news articles, restaurants, television programs, and items for sale. Although the details of various systems differ, content-based recommendation systems share in common a means for describing the items that may be recommended, a means for creating a profile of the user that describes the types of items the user likes, and a means of comparing items to the user profile to determine what to re commend. The profile is often created and updated automatically in response to feedback on the desirability of items that have been presented to the user.},
	Author = {Pazzani, Michael J and Billsus, Daniel},
	Journal = {The adaptive web},
	Title = {{Content-Based Recommendation Systems}},
	Year = {2007}}

@article{hefter2016optimal,
	Author = {Hefter, Mario and Herzwurm, Andr{\'{e}}},
	Journal = {Communications in Mathematical Sciences},
	Keywords = {Adaptive algorithm,Cox-Ingersoll-Ross process,Reflected Brownian motion,Strong approximation,n-th minimal error},
	Number = {8},
	Pages = {2121--2141},
	Title = {{Optimal strong approximation of the one-dimensional squared Bessel process}},
	Url = {https://arxiv.org/pdf/1601.01455.pdf},
	Volume = {15},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1601.01455.pdf}}

@article{luxburg2007tutorial,
	Author = {von Luxburg, Ulrike},
	Journal = {Statistics and Computing},
	Number = {4},
	Pages = {395--416},
	Title = {{A tutorial on spectral clustering}},
	Url = {http://www.kyb.mpg.de/fileadmin/user{\_}upload/files/publications/attachments/Luxburg07{\_}tutorial{\_}4488{\%}5B0{\%}5D.pdf},
	Volume = {17},
	Year = {2007}}

@inproceedings{Jenatton2011,
	Author = {Jenatton, R and Gramfort, A and Michel, V and Obozinski, G and Bach, F and Thirion, B},
	Booktitle = {International Workshop on Pattern Recognition in Neuroimaging (PRNI)},
	Title = {{Multi-scale Mining of f{\{}MRI{\}} Data with Hierarchical Structured Sparsity}},
	Year = {2011}}

@inproceedings{das2008algorithms,
	Author = {Das, A and Kempe, D},
	Booktitle = {Proceedings of the 40th annual ACM symposium on Theory of computing},
	Organization = {ACM},
	Title = {{Algorithms for subset selection in linear regression}},
	Year = {2008}}

@article{jaksch2010near,
	Address = {Cambridge, MA, USA},
	Author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	Issn = {1532-4435},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {aug},
	Pages = {1563--1600},
	Publisher = {MIT Press},
	Title = {{Near-optimal Regret Bounds for Reinforcement Learning}},
	Volume = {99},
	Year = {2010}}

@inproceedings{Steinberger2009,
	Author = {Steinberger, R and Pouliquen, B and {Van der Goot}, E},
	Booktitle = {Information Access in a Multilingual World-Proceedings of the SIGIR 2009 Workshop (SIGIR-CLIR'2009)},
	Pages = {1--8},
	Title = {{An Introduction to the {\{}Europe Media Monitor{\}} Family of Applications}},
	Year = {2009}}

@article{minoux1978accelerated,
	Author = {Minoux, M},
	Journal = {Optimization Techniques},
	Pages = {234--243},
	Publisher = {Springer},
	Title = {{Accelerated greedy algorithms for maximizing submodular set functions}},
	Year = {1978}}

@article{ando1979concavity,
	Author = {Ando, T},
	Journal = {Linear Algebra and its Applications},
	Pages = {203--241},
	Publisher = {Elsevier},
	Title = {{Concavity of certain maps on positive definite matrices and applications to {\{}H{\}}adamard products}},
	Volume = {26},
	Year = {1979}}

@article{schweitzer1985generalized,
	Author = {Schweitzer, Paul and Seidmann, Abraham},
	Journal = {Journal of Mathematical Analysis and Applications},
	Pages = {568--582},
	Title = {{Generalized Polynomial Approximations in {\{}Markovian{\}} Decision Processes}},
	Volume = {110},
	Year = {1985}}

@article{abernethy2008efficient,
	Abstract = {We introduce an efficient algorithm for the problem of online linear optimization in the bandit setting which achieves the optimal O (T regret. The setting is a natural generalization of the non-stochastic multi-armed bandit problem, and the existence of an efficient optimal algorithm has been posed as an open problem in a number of recent papers. We show how the difficulties encountered by previous approaches are overcome by the use of a self-concordant potential function. Our approach presents a novel connection between online learning and interior point methods.},
	Author = {Abernethy, Jacob Duncan and Hazan, Elad and Rakhlin, Alexander},
	Doi = {10.1080/09544820500115717},
	Institution = {EECS Department, University of California, Berkeley},
	Issn = {09544828},
	Journal = {Online},
	Number = {3},
	Pages = {540--543},
	Publisher = {Citeseer},
	Title = {{An Efficient Algorithm for Bandit Linear Optimization}},
	Url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1080/09544820500115717{\&}magic=crossref},
	Volume = {1},
	Year = {2008},
	Bdsk-Url-1 = {http://www.informaworld.com/openurl?genre=article%7B%5C&%7Ddoi=10.1080/09544820500115717%7B%5C&%7Dmagic=crossref},
	Bdsk-Url-2 = {https://doi.org/10.1080/09544820500115717}}

@article{AgHeTe88:Switching,
	Author = {Agrawal, R and Hedge, M V and Teneketzis, D},
	Journal = {IEEE Transactions on Automatic Control},
	Number = {10},
	Pages = {899--906},
	Title = {{Asymptotically Efficient Adaptive Allocation Rules for the Multiarmed Bandit Problem with Switching Cost}},
	Volume = {33},
	Year = {1988}}

@inproceedings{valko2008distance,
	Abstract = {Anomaly detection methods can be very useful in identifying unusual or interesting patterns in data. A recently proposed conditional anomaly detection framework extends anomaly detection to the problem of identifying anomalous patterns on a subset of attributes in the data. The anomaly always depends (is conditioned) on the value of remaining attributes. The work presented in this paper focuses on instance-based methods for detecting conditional anomalies. The methods depend heavily on the distance metric that lets us identify examples in the dataset that are most critical for detecting the anomaly. To optimize the performance of the anomaly detection methods we explore and study metric learning methods. We evaluate the quality of our methods on the Pneumonia PORT dataset by detecting unusual admission decisions for patients with the community-acquired pneumonia. The results of our metric learning methods show an improved detection performance over standard distance metrics, which is very promising for building automated anomaly detection systems for variety of intelligent monitoring applications.},
	Annote = {From Duplicate 1 ( Distance Metric Learning for Conditional Anomaly Detection - Valko, Michal; Hauskrecht, Milos )

From Duplicate 2 ( Distance Metric Learning for Conditional Anomaly Detection - Valko, Michal; Hauskrecht, Milos )

comps{\_}distances



From Duplicate 2 ( Distance Metric Learning for Conditional Anomaly Detection - Valko, Michal; Hauskrecht, Milos )

comps{\_}distances},
	Author = {Valko, Michal and Hauskrecht, Milos},
	Booktitle = {Twenty-First International Florida Artificial Intelligence Research Society Conference},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Publisher = {AAAI Press},
	Title = {{Distance metric learning for conditional anomaly detection}},
	Year = {2008}}

@article{orlin2009faster,
	Author = {Orlin, James B},
	Doi = {10.1007/s10107-007-0189-2},
	Isbn = {1010700701892},
	Issn = {00255610},
	Journal = {Mathematical Programming},
	Number = {2},
	Pages = {237--251},
	Publisher = {Springer},
	Title = {{A faster strongly polynomial time algorithm for submodular function minimization}},
	Url = {http://www.springerlink.com/index/10.1007/s10107-007-0189-2},
	Volume = {118},
	Year = {2009},
	Bdsk-Url-1 = {http://www.springerlink.com/index/10.1007/s10107-007-0189-2},
	Bdsk-Url-2 = {https://doi.org/10.1007/s10107-007-0189-2}}

@article{sethuraman1994constructive,
	Annote = {c{\_}omps{\_}models},
	Author = {Sethuraman, J},
	Journal = {Statistica Sinica},
	Pages = {639--650},
	Title = {{A constructive definition of Dirichlet priors}},
	Url = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A4n216.pdf},
	Volume = {4},
	Year = {1994},
	Bdsk-Url-1 = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A4n216.pdf}}

@inproceedings{mohri2014optimal,
	Author = {Mohri, Mehryar and Munoz, Andres},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Optimal regret minimization in posted-price auctions with strategic buyers}},
	Url = {https://papers.nips.cc/paper/5438-optimal-regret-minimization-in-posted-price-auctions-with-strategic-buyers.pdf},
	Year = {2014},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/5438-optimal-regret-minimization-in-posted-price-auctions-with-strategic-buyers.pdf}}

@inproceedings{grudic2000localizing,
	Author = {Grudic, Gregory and Ungar, Lyle},
	Booktitle = {Proceedings of 17th International Conference on Machine Learning},
	Pages = {343--350},
	Title = {{Localizing Policy Gradient Estimates to Action Transitions}},
	Year = {2000}}

@article{Vov99,
	Author = {Vovk, V},
	Journal = {Machine Learning},
	Number = {3},
	Pages = {247--282},
	Title = {{Derandomizing stochastic prediction strategies}},
	Volume = {35},
	Year = {1999}}

@techreport{gelly2006modifications,
	Abstract = {Algorithm UCB1 for multi-armed bandit problem has already been extended to Algorithm UCT (Upper bound Confidence for Tree) which works for minimax tree search. We have developed a Monte-Carlo Go program, MoGo, which is the first computer Go program using UCT. We explain our modification of UCT for Go application and also the intelligent random simulation with patterns which has improved significantly the performance of MoGo. UCT combined with pruning techniques for large Go board is discussed, as well as parallelization of UCT. MoGo is now a top level Go program on {\$}9\backslashtimes9{\$} and {\$}13\backslashtimes13{\$} Go boards.},
	Author = {Gelly, Sylvain and Yizao, Wang and Munos, R{\'{e}}mi and Teytaud, Olivier},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Gelly et al. - 2006 - Modification of UCT with Patterns in Monte-Carlo Go.pdf:pdf},
	Publisher = {Inria},
	Title = {{Modification of UCT with Patterns in Monte-Carlo Go}},
	Url = {https://hal.inria.fr/inria-00117266/},
	Year = {2006},
	Bdsk-Url-1 = {https://hal.inria.fr/inria-00117266/}}

@article{neu09parsers,
	Author = {Neu, G and Szepesv{\'{a}}ri, $\backslash$textCs.},
	Journal = {Machine Learning Journal},
	Number = {2},
	Pages = {303--337},
	Title = {{Training parsers by inverse reinforcement learning}},
	Volume = {77},
	Year = {2009}}

@book{jannach2010recommender,
	Author = {Jannach, Dietmar and Zanker, Markus and Felfernig, Alexander and Friedrich, Gerhard},
	Publisher = {Cambridge University Press},
	Title = {{Recommender systems: An introduction}},
	Url = {www.scholat.com/teamwork/teamworkdownloadscholar.html?id=542{\&}teamId=316},
	Year = {2010},
	Bdsk-Url-1 = {www.scholat.com/teamwork/teamworkdownloadscholar.html?id=542%7B%5C&%7DteamId=316}}

@article{smith1956various,
	Author = {Smith, Wayne E},
	Journal = {Naval Research Logistics},
	Number = {1-2},
	Pages = {59--66},
	Publisher = {Wiley Online Library},
	Title = {{Various optimizers for single-stage production}},
	Url = {https://pdfs.semanticscholar.org/f02e/1823cc1f80b129125ceb94af5f62f862b791.pdf},
	Volume = {3},
	Year = {1956},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/f02e/1823cc1f80b129125ceb94af5f62f862b791.pdf}}

@article{yuille2003concave,
	Author = {Yuille, A L and Rangarajan, A},
	Journal = {Neural Computation},
	Number = {4},
	Pages = {915--936},
	Publisher = {MIT Press},
	Title = {{The concave-convex procedure}},
	Volume = {15},
	Year = {2003}}

@inproceedings{wong2003bayesian,
	Annote = {comps{\_}ano},
	Author = {Wong, Weng Keen and Moore, Andrew and Cooper, Gregory and Wagner, Michael},
	Booktitle = {Proceedings of the 20th International Conference on Machine Learning (ICML-2003)},
	Title = {{Bayesian Network Anomaly Pattern Detection for Disease Outbreaks}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.1245},
	Year = {2003},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.1245}}

@inproceedings{ashkan14diversified,
	Author = {Ashkan, Azin and Kveton, Branislav and Berkovsky, Shlomo and Wen, Zheng},
	Booktitle = {Conference on Recommender Systems},
	Title = {{Diversified utility maximization for recommendations}},
	Year = {2014}}

@article{Tremblay2017a,
	Abstract = {We present a new random sampling strategy for k-bandlimited signals defined on graphs, based on determinantal point processes (DPP). For small graphs, ie, in cases where the spectrum of the graph is accessible, we exhibit a DPP sampling scheme that enables perfect recovery of bandlimited signals. For large graphs, ie, in cases where the graph's spectrum is not accessible, we investigate, both theoretically and empirically, a sub-optimal but much faster DPP based on loop-erased random walks on the graph. Preliminary experiments show promising results especially in cases where the number of measurements should stay as small as possible and for graphs that have a strong community structure. Our sampling scheme is efficient and can be applied to graphs with up to {\$}10{\^{}}6{\$} nodes.},
	Archiveprefix = {arXiv},
	Arxivid = {1703.01594},
	Author = {Tremblay, Nicolas and Amblard, Pierre-Olivier and Barthelm{\'{e}}, Simon},
	Doi = {10.23919/EUSIPCO.2017.8081494},
	Eprint = {1703.01594},
	Isbn = {9780992862671},
	Title = {{Graph sampling with determinantal processes}},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/EUSIPCO.2017.8081494}}

@techreport{golding1996idleness,
	Author = {Golding, Richard and Bosch, Peter and Wilkes, John},
	Institution = {Hewlett-Packard Laboratories},
	Number = {HPL-96-140},
	Title = {{Idleness Is Not Sloth}},
	Year = {1996}}

@article{cortez2009,
	Author = {Cortez, P and Cerdeira, A and Almeida, F and Matos, T and Reis, J},
	Journal = {Decision Support Systems},
	Pages = {547--553},
	Publisher = {Elsevier},
	Title = {{Modeling wine preferences by data mining from physicochemical properties}},
	Volume = {47},
	Year = {2009}}

@inproceedings{mary2015bandits,
	Abstract = {This paper addresses the on-line recommendation problem facing new users and new items; we assume that no information is available neither about users, nor about the items. The only source of information is a set of ratings given by users to some items. By on-line, we mean that the set of users, and the set of items, and the set of ratings is evolving along time and that at any moment, the recommendation system has to select items to recommend based on the currently available information, that is basically the sequence of past events. We also mean that each user comes with her preferences which may evolve along short and longer scales of time; so we have to continuously update their preferences. When the set of ratings is the only available source of information , the traditional approach is matrix factorization. In a decision making under uncertainty setting, actions should be selected to balance exploration with exploitation; this is best modeled as a bandit problem. Matrix factors provide a latent representation of users and items. These representations may then be used as contextual information by the bandit algorithm to select items. This last point is exactly the originality of this paper: the combination of matrix factorization and bandit algorithms to solve the on-line recommendation problem. Our work is driven by considering the recommendation problem as a feedback controlled loop. This leads to interactions between the representation learning, and the recommendation policy.},
	Author = {Mary, J{\'{e}}r{\'{e}}mie and Gaudel, Romaric and Preux, Philippe},
	Booktitle = {First International Workshop on Machine Learning, Optimization, and Big Data},
	Keywords = {Collaborative Filtering,Matrix Factorization,Multi-Armed Bandtis,Recommender Systems,contextual Bandits,sequential Recommender Systems},
	Title = {{Bandits and recommender systems}},
	Year = {2015}}

@inproceedings{Kim2009,
	Author = {Kim, S and Xing, E P},
	Booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
	Title = {{Tree-Guided Group {\{}L{\}}asso for Multi-Task Regression with Structured Sparsity}},
	Year = {2010}}

@article{heckerman1995learning,
	Author = {Heckerman, D and Geiger, D and Chickering, D M},
	Journal = {Machine Learning},
	Number = {3},
	Pages = {197--243},
	Publisher = {Springer},
	Title = {{Learning {\{}B{\}}ayesian networks: The combination of knowledge and statistical data}},
	Volume = {20},
	Year = {1995}}

@article{walsh2010integrating,
	Abstract = {Recent advancements in model-based reinforcement learn- ing have shown that the dynamics of many structured do- mains (e.g. DBNs) can be learned with tractable sample com- plexity, despite their exponentially large state spaces. Un- fortunately, these algorithms all require access to a planner that computes a near optimal policy, and while many tra- ditional MDP algorithms make this guarantee, their com- putation time grows with the number of states. We show how to replace these over-matched planners with a class of sample-based plannerswhose computation time is indepen- dent of the number of stateswithout sacrificing the sample- efficiency guarantees of the overall learning algorithms. To do so, we define sufficient criteria for a sample-based planner to be used in such a learning system and analyze two popu- lar sample-based approaches from the literature. We also in- troduce our own sample-based planner, which combines the strategies fromthese algorithms and stillmeets the criteria for integration into our learning system. In doing so, we define the first complete RL solution for compactly represented (ex- ponentially sized) state spaces with efficiently learnable dy- namics that is both sample efficient and whose computation time does not grow rapidly with the number of states.},
	Author = {Walsh, Thomas J and Goschin, Sergiu and Littman, Michael L},
	Journal = {AAAI Conference on Artificial Intelligence},
	Title = {{Integrating sample-based planning and model-based reinforcement learning}},
	Year = {2010}}

@article{amari,
	Author = {Amari, S},
	Journal = {Neural Computation},
	Number = {2},
	Pages = {251--276},
	Title = {{Natural Gradient Works Efficiently in Learning}},
	Volume = {10},
	Year = {1998}}

@book{AGZ10,
	Author = {Anderson, G and Guionnet, A and Zeitouni, O},
	Publisher = {Cambridge University Press},
	Title = {{An Introduction to Random Matrices}},
	Year = {2010}}

@article{metropolis1949monte,
	Abstract = {Abstract We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences. Abstract We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.},
	Author = {Metropolis, Nicholas and Ulam, S.},
	Journal = {Journal of the American Statistical Association},
	Number = {247},
	Pages = {335--341},
	Title = {{The Monte Carlo method}},
	Volume = {44},
	Year = {1949}}

@inproceedings{el-yaniv_stable_2006,
	Author = {El-Yaniv, Ran and Pechyony, Dmitry},
	Booktitle = {Proceedings of COLT},
	Title = {{Stable transductive learning}},
	Year = {2006}}

@book{GP,
	Author = {Rasmussen, Carl Edward and Williams, Christopher},
	Publisher = {MIT Press},
	Title = {{Gaussian Processes for Machine Learning}},
	Year = {2006}}

@inproceedings{silver2010monte-carlo,
	Abstract = {This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte- Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan e ectively in signi cantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 X 10 battleship and partially observable PacMan, with approximately 10{\^{}}18 and 10{\^{}}56 states respectively. Our Monte- Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the rst general purpose planner to achieve high performance in such large and unfactored POMDPs.},
	Author = {Silver, David and Veness, Joel},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Monte-Carlo planning in large POMDPs}},
	Year = {2010}}

@misc{urlhttp://mplab.ucsd.edumplab,
	Author = {{\$}\backslash{\$}urlhttp://mplab.ucsd.edu},
	Title = {{MPLab GENKI Database}}}

@inproceedings{toby,
	Author = {Hocking, T and Joulin, A and Bach, F and Vert, J.-P.},
	Booktitle = {Proc. ICML},
	Title = {{Clusterpath: an Algorithm for Clustering using Convex Fusion Penalties}},
	Year = {2011}}

@article{Pol67,
	Author = {Polyak, B},
	Journal = {Soviet Math. Doklady},
	Pages = {33--36},
	Title = {{A general method for solving extremal problems}},
	Volume = {174},
	Year = {1967}}

@inproceedings{Gleich2015robustifying,
	Author = {Gleich, David F and Mahoney, Michael W},
	Booktitle = {SIGKDD},
	Pages = {359--368},
	Title = {{Using Local Spectral Methods to Robustify Graph-Based Learning Algorithms}},
	Year = {2015}}

@article{cardoso2003dependence,
	Author = {Cardoso, J F},
	Journal = {The Journal of Machine Learning Research},
	Pages = {1177--1203},
	Publisher = {JMLR. org},
	Title = {{Dependence, correlation and Gaussianity in independent component analysis}},
	Volume = {4},
	Year = {2003}}

@inproceedings{karnin2013almost,
	Author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Almost optimal exploration in multi-armed bandits}},
	Year = {2013}}

@phdthesis{guestrin2003planning,
	Author = {Guestrin, Carlos},
	School = {Stanford University},
	Title = {{Planning Under Uncertainty in Complex Structured Environments}},
	Year = {2003}}

@inproceedings{valko2005evolutionary,
	Abstract = {This paper presents an application of the biologically realistic JASTAP
neural network model to classification tasks. The JASTAP neural network
model is presented as an alternative to the basic multi-layer perceptron
model. An evolutionary procedure previously applied to the simultaneous
solution of feature selection and neural network training on standard
multi-layer perceptrons is extended with JASTAP model. Preliminary
results on IRIS standard data set give evidence that this extension
allows the use of smaller neural networks that can handle noisier
data without any degradation in classification accuracy.},
	Author = {Valko, Michal and Marques, Nuno Cavalheiro and Castelani, Marco},
	Booktitle = {Proceedings of 2005 Portuguese Conference on Artificial Intelligence},
	Editor = {Et al., Bento},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Pages = {24--32},
	Publisher = {IEEE},
	Title = {{Evolutionary Feature Selection for Spiking Neural Network Pattern Classifiers}},
	Year = {2005}}

@book{neuts1981matrix-geometric,
	Address = {Baltimore, MD},
	Author = {Neuts, Marcel},
	Publisher = {Johns Hopkins University Press},
	Title = {{Matrix-Geometric Solutions in Stochastic Models: An Algorithmic Approach}},
	Year = {1981}}

@book{Spa03,
	Author = {Spall, J},
	Publisher = {Wiley Interscience},
	Title = {{Introduction to stochastic search and optimization. Estimation, simulation, and control}},
	Year = {2003}}

@inproceedings{SS07,
	Author = {Shalev-Shwartz, S and Singer, Y},
	Booktitle = {Advances in Neural Information Processing Systems (NIPS)},
	Title = {{Convex Repeated Games and Fenchel Duality}},
	Year = {2007}}

@article{korostelev1999asymptotic,
	Author = {Korostelev, Alexander and Nussbaum, Michael},
	Journal = {Bernoulli},
	Number = {6},
	Pages = {1099--1118},
	Title = {{The asymptotic minimax constant for sup-norm loss in nonparametric density estimation}},
	Volume = {5},
	Year = {1999}}

@inproceedings{hkl,
	Author = {Bach, F},
	Booktitle = {Adv. NIPS},
	Title = {{Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning}},
	Year = {2008}}

@inproceedings{mnih2008,
	Author = {Mnih, V and Szepesv{\'{a}}ri, $\backslash$textCs and Audibert, J.-Y.},
	Booktitle = {ICML},
	Pages = {672--679},
	Title = {{Empirical {\{}B{\}}ernstein stopping}},
	Year = {2008}}

@article{whitney35abstract,
	Author = {Whitney, Hassler},
	Journal = {American Journal of Mathematics},
	Number = {3},
	Pages = {509--533},
	Title = {{On the abstract properties of linear dependence}},
	Volume = {57},
	Year = {1935}}

@article{koutis2011combinatorial,
	Abstract = {Several algorithms for problems including image segmentation, gradient inpainting and total variation are based on solving symmetric diagonally dominant (SDD) linear systems. These algorithms generally produce results of high quality. However, existing solvers are not always efficient, and in many cases they operate only on restricted topologies. The unavailability of reliably efficient solvers has arguably hindered the adoptability of approaches and algorithms based on SDD systems, especially in applications involving very large systems. A central claim of this paper is that SDD-based approaches can now be considered practical and reliable. To support our claim we present Combinatorial Multigrid (CMG), the first reliably efficient SDD solver that tackles problems in general and arbitrary weighted topologies. The solver borrows the structure and operators of multigrid algorithms, but embeds into them powerful and algebraically sound combinatorial preconditioners, based on novel tools from support graph theory. In order to present the derivation of CMG, we review and exemplify key notions of support graph theory that can also guide the future development of specialized solvers. We validate our claims on very large systems derived from imaging applications. Finally, we outline two new reductions of non-linear filtering problems to SDD systems and review the integration of SDD systems into selected algorithms. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
	Author = {Koutis, Ioannis and Miller, Gary L. and Tolliver, David},
	Journal = {Computer Vision and Image Understanding},
	Number = {12},
	Pages = {1638--1646},
	Title = {{Combinatorial preconditioners and multilevel solvers for problems in computer vision and image processing}},
	Url = {http://www.cs.cmu.edu/{~}./jkoutis/papers/cviu{\_}preprint.pdf},
	Volume = {115},
	Year = {2011},
	Bdsk-Url-1 = {http://www.cs.cmu.edu/%7B~%7D./jkoutis/papers/cviu%7B%5C_%7Dpreprint.pdf}}

@inproceedings{calandriello2017efficient,
	Abstract = {Online kernel learning (OKL) is a flexible framework to approach prediction problems, since the large approximation space provided by reproducing kernel Hilbert spaces can contain an accurate function for the problem. Nonetheless, optimizing over this space is computationally expensive. Not only first order methods accumulate O( sqrt T ) more loss than the optimal function, but the curse of kernelization results in a O(t) per step complexity. Second-order methods get closer to the optimum much faster, suffering only O( log(T)) regret, but second-order updates are even more expensive, with a O(t 2) per-step cost. Existing approximate OKL methods try to reduce this complexity either by limiting the Support Vectors (SV) introduced in the predictor, or by avoiding the kernelization process altogether using embedding. Nonetheless, as long as the size of the approximation space or the number of SV does not grow over time, an adversary can always exploit the approximation process. In this paper, we propose PROS-N-KONS, a method that combines Nystrom sketching to project the input point in a small, accurate embedded space, and performs efficient second-order updates in this space. The embedded space is continuously updated to guarantee that the embedding remains accurate, and we show that the per-step cost only grows with the effective dimension of the problem and not with T . Moreover, the second-order updated allows us to achieve the logarithmic regret. We empirically compare our algorithm on recent large-scales benchmarks and show it performs favorably.},
	Author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Efficient second-order online kernel learning with adaptive embedding}},
	Year = {2017}}

@article{KW01,
	Author = {Kivinen, J and Warmuth, M},
	Journal = {Machine Learning},
	Pages = {301--329},
	Title = {{Relative loss bounds for multidimensional regression problems}},
	Volume = {45},
	Year = {2001}}

@inproceedings{grabner2008semi-supervised,
	Author = {Grabner, Helmut and Leistner, Christian and Bischof, Horst},
	Booktitle = {Proceedings of the 10th European Conference on Computer Vision},
	Pages = {234--247},
	Title = {{Semi-Supervised On-Line Boosting for Robust Tracking}},
	Year = {2008}}

@article{best1990active,
	Author = {Best, M J and Chakravarti, N},
	Journal = {Mathematical Programming},
	Number = {1},
	Pages = {425--439},
	Publisher = {Springer},
	Title = {{Active set algorithms for isotonic regression; a unifying framework}},
	Volume = {47},
	Year = {1990}}

@incollection{taskar2004max-margin,
	Address = {Cambridge, MA},
	Author = {Taskar, Ben and Guestrin, Carlos and Koller, Daphne},
	Booktitle = {Advances in Neural Information Processing Systems 16},
	Editor = {Thrun, Sebastian and Saul, Lawrence and Sch{\"{o}}lkopf, Bernhard},
	Keywords = {Markov models,PAC bounds,graphical models,kernel methods,large margin methods,machine learning,quadratic programming,statistical learning theory,structured data},
	Publisher = {MIT Press},
	Title = {{Max-Margin Markov Networks}},
	Year = {2004}}

@inproceedings{negahban2008joint,
	Author = {Negahban, S and Wainwright, M J},
	Booktitle = {Adv. NIPS},
	Title = {{Joint support recovery under high-dimensional scaling: Benefits and perils of $\backslash$ell{\_}1-$\backslash$ell{\_}$\backslash$infty-regularization}},
	Year = {2008}}

@article{crammer2002algorithmic,
	Address = {Cambridge, MA, USA},
	Author = {Crammer, Koby and Singer, Yoram},
	Issn = {1533-7928},
	Journal = {J. Mach. Learn. Res.},
	Pages = {265--292},
	Publisher = {MIT Press},
	Title = {{On the algorithmic implementation of multiclass kernel-based vector machines}},
	Volume = {2},
	Year = {2002}}

@inproceedings{yu2005blockwise,
	Author = {Yu, Kai and Yu, Shipeng},
	Booktitle = {Proc. of the 22nd ICML Workshop on Learning},
	Title = {{Blockwise supervised inference on large graphs}},
	Year = {2005}}

@inproceedings{bennett1999semi-supervised,
	Author = {Bennett, Kristin and Demiriz, Ayhan},
	Booktitle = {Advances in Neural Information Processing Systems 11},
	Pages = {368--374},
	Title = {{Semi-Supervised Support Vector Machines}},
	Year = {1999}}

@techreport{trick1993linear,
	Author = {Trick, Michael and Zin, Stanley},
	Institution = {Carnegie Mellon University},
	Title = {{A Linear Programming Approach to Solving Stochastic Dynamic Programs}},
	Year = {1993}}

@article{Becker2009,
	Author = {Becker, S and Bobin, J and Candes, E},
	Journal = {SIAM J. on Imaging Sciences},
	Number = {1},
	Pages = {1--39},
	Title = {{NESTA: A Fast and Accurate First-order Method for Sparse Recovery}},
	Volume = {4},
	Year = {2011}}

@inproceedings{erraqabi2017trading,
	Abstract = {In multi-armed bandits, the most common objective is the maximization of the cumulative reward. Alternative settings include active exploration, where a learner tries to gain accurate estimates of the rewards of all arms. While these objectives are contrasting, in many scenarios it is desirable to trade off rewards and errors. For instance, in educational games the designer wants to gather generalizable knowledge about the behavior of the students and teaching strategies (small estimation errors) but, at the same time, the system needs to avoid giving a bad experience to the players, who may leave the system permanently (large reward). In this paper, we formalize this tradeoff and introduce the ForcingBalance algorithm whose performance is provably close to the best possible tradeoff strategy. Finally, we demonstrate on real-world educational data that ForcingBalance returns useful information about the arms without compromising the overall reward.},
	Author = {Erraqabi, Akram and Lazaric, Alessandro and Valko, Michal and Brunskill, Emma and Liu, Yun-en},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	File = {:Users/miki/Dropbox/research/bandits/educational/2016-AISTATS-discovery-bandit/erraqabi2017trading.pdf:pdf},
	Title = {{Trading off rewards and errors in multi-armed bandits}},
	Year = {2017}}

@inproceedings{taskar05learning,
	Author = {Taskar, Ben and Chatalbashev, Vassil and Koller, Daphne and Guestrin, Carlos},
	Pages = {896--903},
	Title = {{Learning structured prediction models: a large margin approach}}}

@article{harisson1978,
	Author = {Harrison, D and Rubinfeld, D L},
	Journal = {J. Environ. Economics {\&} Management},
	Pages = {81--102},
	Title = {{Hedonic prices and the demand for clean air}},
	Volume = {5},
	Year = {1978}}

@inproceedings{Schmidt2010,
	Author = {Schmidt, M and Murphy, K},
	Booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
	Title = {{Convex Structure Learning in Log-Linear Models: Beyond Pairwise Potentials}},
	Year = {2010}}

@article{Ambuhl2009,
	Abstract = {In this paper we study the singlemachine precedence constrained schedul- ing problem of minimizing the sum of weighted completion time. Specifically, we settle an open problem first raised by Chudak and Hochbaum and whose answer was subsequently conjectured by Correa and Schulz. As shown by Correa and Schulz, the proof of this conjecture implies that the addressed scheduling problem is a special case of the vertex cover problem. This means that previous results for the scheduling problem can be explained, and in some cases improved, by means of vertex cover theory. For example, the conjecture implies the existence of a polynomial time algo- rithm for the special case of two-dimensional partial orders. This considerably ex- tends Lawler's result from 1978 for series-parallel orders.},
	Author = {Amb{\"{u}}hl, Christoph and Mastrolilli, Monaldo},
	Doi = {10.1007/s00453-008-9251-6},
	Isbn = {3540388753},
	Issn = {01784617},
	Journal = {Algorithmica},
	Keywords = {Algorithms,Scheduling,Vertex cover},
	Number = {4},
	Pages = {488--503},
	Title = {{Single machine precedence constrained scheduling is a vertex cover problem}},
	Volume = {53},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00453-008-9251-6}}

@article{even2006action,
	Author = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
	Journal = {Journal of Machine Learning Research},
	Pages = {1079--1105},
	Publisher = {JMLR. org},
	Title = {{Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems}},
	Volume = {7},
	Year = {2006}}

@inproceedings{jamali2010matrix,
	Author = {Jamali, Mohsen and Ester, Martin},
	Booktitle = {Conference on Recommender systems},
	Title = {{A matrix factorization technique with trust propagation for recommendation in social networks}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.691{\&}rep=rep1{\&}type=pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.691%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{li2005lazy,
	Author = {Li, Lihong and Littman, Michael},
	Booktitle = {Proceedings of the 20th National Conference on Artificial Intelligence},
	Pages = {1175--1180},
	Title = {{Lazy Approximation for Solving Continuous Finite-Horizon {\{}MDPs{\}}}},
	Year = {2005}}

@inproceedings{kleinbergregret,
	Author = {Kleinberg, Robert D and Niculescu-Mizil, Alexandru and Sharma, Yogeshwer},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {425--436},
	Title = {{Regret Bounds for Sleeping Experts and Bandits}}}

@inproceedings{koller1999computing,
	Author = {Koller, Daphne and Parr, Ronald},
	Booktitle = {Proceedings of the 16th International Joint Conference on Artificial Intelligence},
	Pages = {1332--1339},
	Title = {{Computing Factored Value Functions for Policies in Structured {\{}MDPs{\}}}},
	Year = {1999}}

@inproceedings{mahalanobis1936generalized,
	Author = {Mahalanobis, P},
	Booktitle = {Proc. National Inst. Sci. (India)},
	Pages = {49--55},
	Series = {12},
	Title = {{On the generalized distance in statistics}},
	Year = {1936}}

@article{chow1991optimal,
	Author = {Chow, Chee-Seng and Tsitsiklis, John},
	Journal = {IEEE Transactions on Automatic Control},
	Number = {8},
	Pages = {898--914},
	Title = {{An Optimal One-Way Multigrid Algorithm for Discrete-Time Stochastic Control}},
	Volume = {36},
	Year = {1991}}

@inproceedings{zolghadr2013online,
	Author = {Zolghadr, Navid and Bartok, Gabor and Greiner, Russell and Gy{\"{o}}rgy, Andr{\'{a}}s and Szepesvari, Csaba},
	Booktitle = {Advances in Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Zolghadr et al. - 2013 - Online Learning with Costly Features and Labels.pdf:pdf},
	Pages = {1241--1249},
	Title = {{Online Learning with Costly Features and Labels}},
	Url = {http://papers.nips.cc/paper/5149-online-learning-with-costly-features-and-labels},
	Year = {2013},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5149-online-learning-with-costly-features-and-labels}}

@article{hazel2000multivariate,
	Annote = {comps{\_}anX},
	Author = {Hazel, G G},
	Doi = {10.1109/36.843012},
	Issn = {0196-2892},
	Journal = {Geoscience and Remote Sensing, IEEE Transactions on},
	Keywords = {Markov processes,anomaly detection,first-order isotropic texture model,geophysical measurement technique,geophysical signal processing,geophysical techniques,image processing,image segmentation,image texture,joint spatial-spectral modeling,land surface,multidimensional signal processing,multispectral imagery,multispectral scene segmentation,multivariate Gaussian MRF,multivariate method,receiver operating characteristic,remote sensing,terrain mapping,terrain mappingGaussian Markov random field textu,vector observations},
	Month = {may},
	Number = {3},
	Pages = {1199--1211},
	Title = {{Multivariate Gaussian MRF for multispectral scene segmentation and anomaly detection}},
	Volume = {38},
	Year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1109/36.843012}}

@inproceedings{hoey1999spudd:,
	Author = {Hoey, Jesse and St-Aubin, Robert and Hu, Alan and Boutilier, Craig},
	Booktitle = {Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence},
	Pages = {279--288},
	Title = {{{\{}SPUDD{\}}: Stochastic Planning using Decision Diagrams}},
	Year = {1999}}

@book{duda73pattern,
	Author = {Duda, Richard and Hart, Peter},
	Publisher = {Wiley},
	Title = {{Pattern Classification and Scene Analysis}},
	Year = {1973}}

@incollection{hauskrecht2006fundamentals,
	Abstract = {Finding reliable, meaningful patterns in data with high numbers of attributes can be extremely difficult. Feature selection helps us to decide what attributes or combination of attributes are most important for finding these patterns. In this chapter, we study feature selection methods for building classification models from high-throughput genomic (microarray) and proteomic (mass spectrometry) data sets. Thousands of feature candidates must be analyzed, compared and combined in such data sets. We describe the basics of four different approaches used for feature selection and illustrate their effects on an MS cancer proteomic data set. The closing discussion provides assistance in performing an analysis in high-dimensional genomic and proteomic data.},
	Author = {Hauskrecht, Milos and Pelikan, Richard and Valko, Michal and Lyons-Weiler, James},
	Booktitle = {Fundamentals of Data Mining in Genomics and Proteomics},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Publisher = {Springer},
	Title = {{Feature selection and dimensionality reduction in genomics and proteomics}},
	Year = {2006}}

@article{NJLS09,
	Author = {Nemirovski, A and Juditsky, A and Lan, G and Shapiro, A},
	Journal = {SIAM Journal on Optimization},
	Pages = {1574--1609},
	Title = {{Robust stochastic approximation approach to stochastic programming}},
	Volume = {19},
	Year = {2009}}

@inproceedings{chambolle2005total,
	Author = {Chambolle, A},
	Booktitle = {Energy Minimization Methods in Computer Vision and Pattern Recognition},
	Organization = {Springer},
	Pages = {136--152},
	Title = {{Total variation minimization and a class of binary {\{}MRF{\}} models}},
	Year = {2005}}

@article{GLS01,
	Author = {Grove, A and Littlestone, N and Schuurmans, D},
	Journal = {Machine Learning},
	Pages = {173--210},
	Title = {{General convergence results for linear discriminant updates}},
	Volume = {43},
	Year = {2001}}

@inproceedings{papadimitriou2003cross-outlier,
	Author = {Papadimitriou, Spiros and Faloutsos, Christos},
	Booktitle = {Advances in Spatial and Temporal Databases, 8th International Symposium, SSTD 2003, Santorini Island, Greece, July 24-27, 2003, Proceedings},
	Editor = {Hadzilacos, Thanasis and Manolopoulos, Yannis and Roddick, John F and Theodoridis, Yannis},
	Pages = {199--213},
	Title = {{Cross-Outlier Detection}},
	Volume = {2750},
	Year = {2003}}

@article{mitra2002density-based,
	Author = {Mitra, P and Murthy, C A and Pal, S K},
	Journal = {IEEE Transactions on PAMI},
	Number = {6},
	Pages = {1--14},
	Title = {{Density-based multiscale data condensation}},
	Volume = {24},
	Year = {2002}}

@article{wainwright2008graphical,
	Author = {Wainwright, M J and Jordan, M I},
	Journal = {Foundations and Trends{\{}$\backslash$textregistered{\}} in Machine Learning},
	Number = {1-2},
	Pages = {1--305},
	Publisher = {Now Publishers Inc.},
	Title = {{Graphical models, exponential families, and variational inference}},
	Volume = {1},
	Year = {2008}}

@techreport{cohen_online_2016,
	Abstract = {Finding a small spectral approximation for a tall {\$}n \backslashtimes d{\$} matrix {\$}A{\$} is a fundamental numerical primitive. For a number of reasons, one often seeks an approximation whose rows are sampled from those of {\$}A{\$}. Row sampling improves interpretability, saves space when {\$}A{\$} is sparse, and preserves row structure, which is especially important, for example, when {\$}A{\$} represents a graph. However, correctly sampling rows from {\$}A{\$} can be costly when the matrix is large and cannot be stored and processed in memory. Hence, a number of recent publications focus on row sampling in the streaming setting, using little more space than what is required to store the outputted approximation [KL13, KLM+14]. Inspired by a growing body of work on online algorithms for machine learning and data analysis, we extend this work to a more restrictive online setting: we read rows of {\$}A{\$} one by one and immediately decide whether each row should be kept in the spectral approximation or discarded, without ever retracting these decisions. We present an extremely simple algorithm that approximates {\$}A{\$} up to multiplicative error {\$}\backslashepsilon{\$} and additive error {\$}\backslashdelta{\$} using {\$}O(d \backslashlog d \backslashlog(\backslashepsilon||A||{\_}2/\backslashdelta)/\backslashepsilon{\^{}}2){\$} online samples, with memory overhead proportional to the cost of storing the spectral approximation. We also present an algorithm that uses {\$}O(d{\^{}}2{\$}) memory but only requires {\$}O(d\backslashlog(\backslashepsilon||A||{\_}2/\backslashdelta)/\backslashepsilon{\^{}}2){\$} samples, which we show is optimal. Our methods are clean and intuitive, allow for lower memory usage than prior work, and expose new theoretical properties of leverage score based matrix approximation.},
	Annote = {arXiv: 1604.05448},
	Archiveprefix = {arXiv},
	Arxivid = {1604.05448},
	Author = {Cohen, Michael B and Musco, Cameron and Pachocki, Jakub},
	Eprint = {1604.05448},
	Title = {{Online row sampling}},
	Url = {http://arxiv.org/abs/1604.05448},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1604.05448}}

@book{cover-thomas-1991,
	Author = {Cover, T and Thomas, J A},
	Publisher = {Wiley},
	Title = {{Elements of Information Theory}},
	Year = {1991}}

@inproceedings{marecki2007fast,
	Author = {Marecki, Janusz and Koenig, Sven and Tambe, Milind},
	Booktitle = {Proceedings of the 20th International Joint Conference on Artificial Intelligence},
	Title = {{A Fast Analytical Algorithm for Solving {\{}Markov{\}} Decision Processes with Continuous Resources}},
	Year = {2007}}

@article{astrom1965optimal,
	Author = {Astrom, Karl},
	Journal = {Journal of Mathematical Analysis and Applications},
	Number = {1},
	Pages = {174--205},
	Title = {{Optimal Control of {\{}Markov{\}} Processes with Incomplete State Information}},
	Volume = {10},
	Year = {1965}}

@inproceedings{hwang1997predictive,
	Author = {Hwang, Chi-Hong and Wu, Allen},
	Booktitle = {Proceedings of the 1997 IEEE / ACM International Conference on Computer-Aided Design},
	Pages = {28--32},
	Title = {{A Predictive System Shutdown Method for Energy Saving of Event-Driven Computation}},
	Year = {1997}}

@article{freund1997decision,
	Author = {Freund, Y and Schapire, R E},
	Journal = {Journal of Computer and System Sciences},
	Pages = {119--139},
	Title = {{A decision-theoretic generalization of on-line learning and an application to boosting}},
	Volume = {55},
	Year = {1997}}

@inproceedings{charpiatexhaustive,
	Author = {Charpiat, G},
	Booktitle = {Proc. CVPR},
	Title = {{Exhaustive Family of Energies Minimizable Exactly by a Graph Cut}},
	Year = {2011}}

@article{Ambuhl2011,
	Abstract = {We consider the single-machine scheduling problem to minimize the weighted sum of completion times under precedence constraints. In a series of recent papers, it was established that this scheduling problem is a special case of minimum weighted vertex cover. In this paper, we show that the vertex cover graph associated with the scheduling problem is exactly the graph of incomparable pairs defined in the dimension theory of partial orders. Exploiting this relationship allows us to present a framework for obtaining (2 --- 2/f)-approximation algorithms, provided that the set of precedence constraints has fractional dimension of at most f. Our approach yields the best-known approximation ratios for all previously considered special classes of precedence constraints, and it provides the first results for bounded degree and orders of interval dimension 2. On the negative side, we show that the addressed problem remains NP-hard even when restricted to the special case of interval orders. Furthermore, we prove that the general problem, if a fixed cost present in all feasible schedules is ignored, becomes as hard to approximate as vertex cover. We conclude by giving the first inapproximability result for this problem, showing under a widely believed assumption that it does not admit a polynomial-time approximation scheme.},
	Author = {Amb{\"{u}}hl, Christoph and Mastrolilli, Monaldo and Mutsanas, Nikolaus and Svensson, Ola},
	Doi = {10.2307/41412330},
	Isbn = {0364765X},
	Issn = {0364765X},
	Journal = {Mathematics of Operations Research},
	Number = {4},
	Pages = {653--669},
	Title = {{On the approximability of single-machine scheduling with precedence constraints}},
	Url = {http://www.jstor.org/stable/41412330},
	Volume = {36},
	Year = {2011},
	Bdsk-Url-1 = {http://www.jstor.org/stable/41412330},
	Bdsk-Url-2 = {https://doi.org/10.2307/41412330}}

@inproceedings{jamieson2014lilUCB,
	Author = {Jamieson, Kevin and Malloy, Matthew and Nowak, Robert and Bubeck, S{\'{e}}bastien},
	Booktitle = {Conference on Learning Theory},
	Title = {{lil'UCB: An Optimal Exploration Algorithm for Multi-Armed Bandits}},
	Year = {2014}}

@article{candes2006near-optimal,
	Author = {Cand{\`{e}}s, Emmanuel J. and Tao, Terence},
	Journal = {IEEE Transactions on Information Theory},
	Number = {12},
	Pages = {5406--5425},
	Title = {{Near-optimal signal recovery from random projections: universal encoding strategies?}},
	Volume = {52},
	Year = {2006}}

@article{hannan1957approximation,
	Author = {Hannan, James},
	Journal = {Contributions to the theory of games},
	Pages = {97--139},
	Title = {{Approximation to Bayes risk in repeated play}},
	Volume = {3},
	Year = {1957}}

@techreport{gelly2006modifications,
	Author = {Gelly, Sylvain and Yizao, Wang and Munos, R{\'{e}}mi and Teytaud, Olivier},
	Institution = {Inria},
	Title = {{Modification of UCT with patterns in Monte-Carlo Go}},
	Url = {https://hal.inria.fr/inria-00117266},
	Year = {2006},
	Bdsk-Url-1 = {https://hal.inria.fr/inria-00117266}}

@inproceedings{locatelli2018adaptive,
	Abstract = {We present the first adaptive strategy for active learning in the setting of classification with smooth decision boundary. The problem of adaptivity (to unknown distributional parameters) has remained opened since the seminal work of Castro and Nowak (2007), which first established (active learning) rates for this setting. While some recent advances on this problem establish adaptive rates in the case of univariate data, adaptivity in the more practical setting of multivariate data has so far remained elusive. Combining insights from various recent works, we show that, for the multivariate case, a careful reduction to univariate-adaptive strategies yield near-optimal rates without prior knowledge of distributional parameters.},
	Archiveprefix = {arXiv},
	Arxivid = {1711.09294},
	Author = {Locatelli, Andrea and Carpentier, Alexandra and Kpotufe, Samory},
	Booktitle = {Algorithmic Learning Theory},
	Eprint = {1711.09294},
	Title = {{An Adaptive Strategy for Active Learning with Smooth Decision Boundary}}}

@inproceedings{valko2008learning,
	Abstract = {Multiple technologies that measure expression levels of protein mixtures in the human body offer a potential for detection and understanding the disease. The recent increase of these technologies prompts researchers to evaluate the individual and combined utility of data generated by the technologies. In this work, we study two data sources to measure the expression of protein mixtures in the human body: whole-sample MS profiling and multiplexed protein arrays. We investigate the individual and combined utility of these technologies by learning and testing a variety of classification models on the data from a pancreatic cancer study. We show that for the combination of these two (heterogeneous) datasets, classification models that work well on one of them individually fail on the combination of the two datasets. We study and propose a class of model fusion methods that acknowledge the differences and try to reap most of the benefits from their combination.},
	Author = {Valko, Michal and Pelikan, Richard and Hauskrecht, Milos},
	Booktitle = {AMIA Summit on Translational Bioinformatics},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Month = {mar},
	Title = {{Learning predictive models for multiple heterogeneous proteomic datasources}},
	Year = {2008}}

@article{chatterjee2015matrix,
	Abstract = {Consider the problem of estimating the entries of a large matrix, when the observed entries are noisy versions of a small random fraction of the original entries. This problem has received widespread attention in recent times, especially after the pioneering works of Emmanuel Candes and collaborators. Typically, it is assumed that the underlying matrix has low rank. This paper introduces a simple estimation procedure, called Universal Singular Value Thresholding (USVT), that works for any matrix that has `a little bit of structure'. In particular, the matrix need not be of low rank. The procedure is very simple and fast, works under minimal assumptions, and is applicable for very large matrices. Surprisingly, this simple estimator achieves the minimax error rate up to a constant factor. The method is applied to give simple solutions to difficult questions in low rank matrix estimation, blockmodels, distance matrix completion, latent space models, positive definite matrix completion, problems related to graph limits, and generalized Bradley-Terry models for pairwise comparison.},
	Author = {Chatterjee, Sourav},
	Journal = {Annals of Statistics},
	Keywords = {Covariance matrix,Distance matrix,Graphons,Latent space model,Low rank matrices,Matrix completion,Matrix estimation,Singular value decomposition,Sochastic blockmodel},
	Number = {1},
	Pages = {177--214},
	Title = {{Matrix estimation by universal singular value thresholding}},
	Volume = {43},
	Year = {2015}}

@article{tesauro1992practical,
	Author = {Tesauro, Gerald},
	Journal = {Machine Learning},
	Number = {3-4},
	Pages = {257--277},
	Title = {{Practical Issues in Temporal Difference Learning}},
	Volume = {8},
	Year = {1992}}

@article{kveton2006solvinga,
	Author = {Kveton, Branislav and Hauskrecht, Milos and Guestrin, Carlos},
	Journal = {Journal of Artificial Intelligence Research},
	Pages = {153--201},
	Title = {{Solving Factored {\{}MDPs{\}} with Hybrid State and Action Variables}},
	Volume = {27},
	Year = {2006}}

@article{lanczos1950iteration,
	Abstract = {The present investigation designs a systematic method for finding the latent roots and the principal axes of a matrix, without reducing the order of the matrix. It is characterized by a wide field of applicability and great accuracy, since the accumulation of rounding errors is avoided, through the process of "minimized iterations". Moreover, the method leads to a well convergent successive approximation procedure by which the solution of integral equations of the Fredholm type and the solution of the eigenvalue problem of linear differential and integral operators may be accomplished.},
	Author = {Lanczos, C.},
	Doi = {10.6028/jres.045.026},
	Issn = {0091-0635},
	Journal = {Journal of Research of the National Bureau of Standards},
	Number = {4},
	Pages = {255},
	Title = {{An iteration method for the solution of the eigenvalue problem of linear differential and integral operators}},
	Url = {http://nvlpubs.nist.gov/nistpubs/jres/045/jresv45n4p255{\_}A1b.pdf},
	Volume = {45},
	Year = {1950},
	Bdsk-Url-1 = {http://nvlpubs.nist.gov/nistpubs/jres/045/jresv45n4p255%7B%5C_%7DA1b.pdf},
	Bdsk-Url-2 = {https://doi.org/10.6028/jres.045.026}}

@article{marbach2001simulation-based,
	Author = {Marbach, Peter and Tsitsiklis, John},
	Journal = {IEEE Transactions on Automatic Control},
	Number = {2},
	Pages = {191--209},
	Title = {{Simulation-Based Optimization of {\{}Markov{\}} Reward Processes}},
	Volume = {46},
	Year = {2001}}

@article{badanidiyuru2014resourceful,
	Abstract = {We study contextual bandits with ancillary constraints on resources, which are common in real-world applications such as choosing ads or dynamic pricing of items. We design the first algorithm for solving these problems, and prove a regret guarantee with near-optimal statistical properties.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1402.6779v2},
	Author = {Badanidiyuru, A and Langford, John and Slivkins, Aleksandrs},
	Eprint = {arXiv:1402.6779v2},
	Journal = {arXiv preprint arXiv:1402.6779},
	Pages = {1--22},
	Title = {{Resourceful Contextual Bandits}},
	Url = {http://arxiv.org/abs/1402.6779},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1402.6779}}

@article{pickland1975statistical,
	Abstract = {A method is presented for making statistical inferences about the upper tail of a distribution function. It is useful for estimating the probabilities of future extremely large observations. The method is applicable if the underlying distribution function satisfies a condition which holds for all common continuous distribution functions.},
	Author = {Pickands, James III},
	Journal = {The Annals of Statistics},
	Pages = {119--131},
	Title = {{Statistical Inference Using Extreme Order Statistics}},
	Volume = {3},
	Year = {1975}}

@article{chopra1989spanning,
	Author = {Chopra, S},
	Journal = {Operations Research Letters},
	Number = {1},
	Pages = {25--29},
	Publisher = {Elsevier},
	Title = {{On the spanning tree polyhedron}},
	Volume = {8},
	Year = {1989}}

@inproceedings{carvalho06voting,
	Address = {New York, NY, USA},
	Author = {Carvalho, Vitor R and Cohen, William W},
	Booktitle = {KDD '06},
	Pages = {548--553},
	Publisher = {ACM},
	Title = {{Single-pass online learning: performance, voting schemes and online feature selection}},
	Year = {2006}}

@misc{graclus,
	Author = {Graclus},
	Publisher = {University of Texas},
	Title = {{Graclus}},
	Url = {http://www.cs.utexas.edu/users/dml/Software/graclus.html},
	Year = {2013},
	Bdsk-Url-1 = {http://www.cs.utexas.edu/users/dml/Software/graclus.html}}

@book{fujishige2005submodular,
	Author = {Fujishige, Satoru},
	Series = {Annals of discrete mathematics},
	Title = {{Submodular functions and optimization}},
	Year = {2005}}

@article{KV05,
	Author = {Kalai, A and Vempala, S},
	Journal = {Journal of Computer and System Sciences},
	Pages = {291--307},
	Title = {{Efficient algorithms for online decision problems}},
	Volume = {71},
	Year = {2005}}

@inproceedings{drineas2005nystr$o$m,
	Author = {Drineas, P and Mahoney, M W},
	Booktitle = {Proceedings of COLT, 2005},
	Title = {{On the {\{}N{\}}ystr{\{}$\backslash$ddot{\{}$\backslash$text{\{}o{\}}{\}}{\}}m method for approximating a {\{}G{\}}ram matrix for improved kernel-based learning}},
	Year = {2005}}

@inproceedings{ng2000algorithms,
	Abstract = {This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behaviour. IRL may be useful for apprenticeship learning to acquire skilled behaviour, and for ascertaining the reward function being optimized by a natural system. We rst characterize the set of all reward functions for which a given policy is optimal. We then derive three algorithms for IRL. The rst two deal with the case where the entire policy is known; we handle tabulated reward functions on a nite state space and linear functional approximation of the reward function over a potentially in- nite state space. The third algorithm deals with the more realistic case in which the policy is known only through a nite set of observed trajectories. In all cases, a key issue is degeneracythe existence of a large set of reward functions for which the observed policy is optimal. To remove...},
	Author = {Ng, Andrew and Russell, Stuart},
	Booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	Doi = {10.2460/ajvr.67.2.323},
	Editor = {{De Sousa}, Jorge Pinho},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Ng, Russell - 2000 - Algorithms for inverse reinforcement learning.pdf:pdf},
	Issn = {00029645},
	Pages = {663--670},
	Pmid = {16454640},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Algorithms for inverse reinforcement learning}},
	Url = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
	Year = {2000},
	Bdsk-Url-1 = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
	Bdsk-Url-2 = {https://doi.org/10.2460/ajvr.67.2.323}}

@inproceedings{AYSze11,
	Author = {Abbasi-Yadkori, Yasin and Szepesv{\'{a}}ri, $\backslash$textCsaba},
	Title = {{Regret Bounds for the Adaptive Control of Linear Quadratic Systems}},
	Url = {http://webdocs.cs.ualberta.ca/{~}abbasiya/LQR.pdf},
	Bdsk-Url-1 = {http://webdocs.cs.ualberta.ca/%7B~%7Dabbasiya/LQR.pdf}}

@inproceedings{gopalan2013thompson,
	Abstract = {We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms' rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets.},
	Author = {Gopalan, Aditya and Mannor, Shie and Mansour, Yishay},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Thompson sampling for complex bandit problems}},
	Year = {2014}}

@article{lagoudakis2003least-squares,
	Author = {Lagoudakis, Michail and Parr, Ronald},
	Journal = {Journal of Machine Learning Research},
	Pages = {1107--1149},
	Title = {{Least-Squares Policy Iteration}},
	Volume = {4},
	Year = {2003}}

@book{dubitzky2007fundamentals,
	Abstract = {Finding reliable, meaningful patterns in data with high numbers of attributes can be extremely difficult. Feature selection helps us to decide what attributes or combination of attributes are most important for finding these patterns. In this chapter, we study feature selection methods for building classification models from high-throughput genomic (microarray) and proteomic (mass spectrometry) data sets. Thousands of feature candidates must be analyzed, compared and combined in such data sets. We describe the basics of four different approaches used for feature selection and illustrate their effects on an MS cancer proteomic data set. The closing discussion provides assistance in performing an analysis in high-dimensional genomic and proteomic data.},
	Author = {Dubitzky, W and Granzow, M and Berrar, Dp},
	Booktitle = {Vasa},
	Pages = {149--172},
	Title = {{Fundamentals of data mining in genomics and proteomics}},
	Year = {2007}}

@article{freund99perceptron,
	Address = {Hingham, MA, USA},
	Author = {Freund, Yoav and Schapire, Robert E},
	Doi = {http://dx.doi.org/10.1023/A:1007662407062},
	Issn = {0885-6125},
	Journal = {Machine Learning},
	Number = {3},
	Pages = {277--296},
	Publisher = {Kluwer Academic Publishers},
	Title = {{Large Margin Classification Using the Perceptron Algorithm}},
	Volume = {37},
	Year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1007662407062}}

@inproceedings{boularias2011relative,
	Abstract = {We consider the problem of imitation learn- ing where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generaliz- ing the demonstration, based on the assump- tion that the expert is optimally acting in a Markov Decision Process (MDP). Most of the past work on IRL requires that a (near)- optimal policy can be computed for differ- ent reward functions. However, this require- ment can hardly be satisfied in systems with a large, or continuous, state space. In this pa- per, we propose a model-free IRL algorithm, where the relative entropy between the em- pirical distribution of the state-action trajec- tories under a uniform policy and their distri- bution under the learned policy is minimized by stochastic gradient descent. We compare this new approach to well-known IRL algo- rithms using approximate MDP models. Em- pirical results on simulated car racing, grid- world and ball-in-a-cup problems show that our approach is able to learn good policies from a small number of demonstrations.},
	Author = {Boularias, Abdeslam and Kober, Jens and Peters, Jan},
	Booktitle = {Proceedings of the 14th International Con- ference on Artificial Intelligence and Statistics},
	Pages = {182--189},
	Title = {{Relative Entropy Inverse Reinforcement Learning}},
	Volume = {15},
	Year = {2011}}

@article{blei2005variational,
	Annote = {c{\_}omps{\_}models},
	Author = {Blei, David M and Jordan, Michael I},
	Journal = {Bayesian Analysis},
	Pages = {2006},
	Title = {{Variational inference for Dirichlet process mixtures}},
	Url = {http://stat-www.berkeley.edu/tech-reports/674.pdf},
	Volume = {1},
	Year = {2005},
	Bdsk-Url-1 = {http://stat-www.berkeley.edu/tech-reports/674.pdf}}

@article{candes2009exact,
	Abstract = {We consider a problem of considerable practical interest: the recovery of a data matrix from a sampling of its entries. Suppose that we observe m entries selected uniformly at random from a matrix M. Can we complete the matrix and recover the entries that we have not seen? We show that one can perfectly recover most low-rank matrices from what appears to be an incomplete set of entries. We prove that if the number m of sampled entries obeys m {\textgreater}= C n{\^{}}{\{}1.2{\}} r log n for some positive numerical constant C, then with very high probability, most n by n matrices of rank r can be perfectly recovered by solving a simple convex optimization program. This program finds the matrix with minimum nuclear norm that fits the data. The condition above assumes that the rank is not too large. However, if one replaces the 1.2 exponent with 1.25, then the result holds for all values of the rank. Similar results hold for arbitrary rectangular matrices as well. Our results are connected with the recent literature on compressed sensing, and show that objects other than signals and images can be perfectly reconstructed from very limited information.},
	Author = {Cand{\`{e}}s, Emmanuel J. and Recht, Benjamin},
	Journal = {Foundations of Computational Mathematics},
	Keywords = {Compressed sensing,Convex optimization,Decoupling,Duality in optimization,Low-rank matrices,Matrix completion,Noncommutative Khintchine inequality,Nuclear norm minimization,Random matrices},
	Number = {6},
	Pages = {717--772},
	Title = {{Exact matrix completion via convex optimization}},
	Volume = {9},
	Year = {2009}}

@inproceedings{globerson07exponentiated,
	Author = {Globerson, Amir and Koo, Terry Y and Carreras, Xavier and Collins, Michael},
	Doi = {http://doi.acm.org/10.1145/1273496.1273535},
	Pages = {305--312},
	Title = {{Exponentiated gradient algorithms for log-linear structured prediction}},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1273496.1273535}}

@inproceedings{neu2007apprenticeship,
	Abstract = {In this paper we propose a novel gradient algorithm to learn a policy from an expert's observed behavior assuming that the expert behaves optimally with respect to some unknown reward function of a Markovian Decision Problem. The algorithm's aim is to find a reward function such that the resulting optimal policy matches well the expert's observed behavior. The main difficulty is that the mapping from the parameters to policies is both nonsmooth and highly redundant. Resorting to subdifferentials solves the first difficulty, while the second one is overcome by computing natural gradients. We tested th eproposed method in two artificial domains and found it to be more reliable and efficient than some previous methods.},
	Author = {Neu, Gergely and Szepesv{\'{a}}ri, C},
	Booktitle = {Proceedings of the Twenty-Third Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-07)},
	Isbn = {0-9749039-3-00-9749039-3-0},
	Pages = {295----302},
	Title = {{Apprenticeship learning using inverse reinforcement learning and gradient methods}},
	Url = {http://arxiv.org/abs/1206.5264},
	Year = {2007},
	Bdsk-Url-1 = {http://arxiv.org/abs/1206.5264}}

@article{Zhao2009,
	Author = {Zhao, P and Rocha, G and Yu, B},
	Journal = {Annals of Statistics},
	Number = {6A},
	Pages = {3468--3497},
	Title = {{The composite absolute penalties family for grouped and hierarchical variable selection}},
	Volume = {37},
	Year = {2009}}

@inproceedings{jin2001mining,
	Address = {New York, NY, USA},
	Author = {Jin, Wen and Tung, Anthony K H and Han, Jiawei},
	Booktitle = {KDD '01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
	Doi = {http://doi.acm.org/10.1145/502512.502554},
	Isbn = {1-58113-391-X},
	Pages = {293--298},
	Publisher = {ACM Press},
	Title = {{Mining top-n local outliers in large databases}},
	Year = {2001},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/502512.502554}}

@article{nagano2007strongly,
	Author = {Nagano, K},
	Journal = {Discrete Optimization},
	Number = {3-4},
	Pages = {349--359},
	Publisher = {Elsevier},
	Title = {{A strongly polynomial algorithm for line search in submodular polyhedra}},
	Volume = {4},
	Year = {2007}}

@inproceedings{konda2000actor-critic,
	Author = {Konda, Vijay and Tsitsiklis, John},
	Booktitle = {Advances in Neural Information Processing Systems 12},
	Pages = {1008--1014},
	Title = {{Actor-Critic Algorithms}},
	Year = {2000}}

@inproceedings{moore1991variable,
	Author = {Moore, Andrew},
	Booktitle = {Proceedings of the 8th International Conference on Machine Learning},
	Title = {{Variable Resolution Dynamic Programming: Efficiently Learning Action Maps in Multivariate Real-Valued State-Spaces}},
	Year = {1991}}

@article{browne2012survey,
	Author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
	Journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	Number = {1},
	Pages = {1--43},
	Title = {{A survey of Monte Carlo tree search methods}},
	Volume = {4},
	Year = {2012}}

@inproceedings{valko2013finite,
	Abstract = {We tackle the problem of online reward maximisation over a large finite set of actions described by their contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume that we have access to the similarities between actions' contexts and that the expected reward is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCB algorithm, and give a cumulative regret bound through a frequentist analysis. For contextual bandits, the related algorithm GP-UCB turns out to be a special case of our algorithm, and our finite-time analysis improves the regret bound of GP-UCB for the agnostic case, both in the terms of the kernel-dependent quantity and the RKHS norm of the reward function. Moreover, for the linear kernel, our regret bound matches the lower bound for contextual linear bandits.},
	Author = {Valko, Michal and Korda, Nathan and Munos, R{\'{e}}mi and Flaounas, Ilias and Cristianini, Nelo},
	Booktitle = {Uncertainty in Artificial Intelligence},
	Title = {{Finite-time analysis of kernelised contextual bandits}},
	Url = {https://hal.inria.fr/hal-00826946/document},
	Year = {2013},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-00826946/document}}

@misc{movielens,
	Author = {Lam, Shyong and Herlocker, Jon},
	Howpublished = {http://www.grouplens.org/node/12},
	Title = {{MovieLens 1M Dataset}},
	Year = {2012}}

@unpublished{jordan2003introduction,
	Annote = {A textbook for a probabilistic graphical models class
comps{\_}models},
	Author = {Jordan, Michael I},
	Keywords = {bibtex-import},
	Title = {{An Introduction to Probabilistic Graphical Models}},
	Year = {2003}}

@inproceedings{talwalkar2008large-scale,
	Author = {Talwalkar, Ameet and Kumar, Sanjiv and Rowley, Henry A},
	Booktitle = {Computer Vision and Pattern Recognition (CVPR)},
	Title = {{Large-Scale Manifold Learning}},
	Year = {2008}}

@techreport{kolmogorov2010minimizing,
	Author = {Kolmogorov, V},
	Institution = {Arxiv},
	Number = {1006.1990},
	Title = {{Minimizing a sum of submodular functions}},
	Year = {2010}}

@book{LT91,
	Author = {Ledoux, M and Talagrand, M},
	Publisher = {Springer},
	Title = {{Probability in Banach Spaces}},
	Year = {1991}}

@article{hochbaum1995strongly,
	Author = {Hochbaum, D S and Hong, S P},
	Journal = {Mathematical Programming},
	Number = {1},
	Pages = {269--309},
	Publisher = {Springer},
	Title = {{About strongly polynomial time algorithms for quadratic optimization over submodular constraints}},
	Volume = {69},
	Year = {1995}}

@book{VW95,
	Author = {van der Vaart, A and Wellner, J},
	Publisher = {Springer},
	Title = {{Weak Convergence and Empirical Processes}},
	Year = {1995}}

@techreport{williams1993tight,
	Author = {Williams, Ronald and {Baird III}, Leemon},
	Institution = {Northeastern University},
	Number = {NU-CCS-93-14},
	Title = {{Tight Performance Bounds on Greedy Policies Based on Imperfect Value Functions}},
	Year = {1993}}

@article{tran2014efficient,
	Author = {Tran-Thanh, Long and Stein, Sebastian and Rogers, Alex and Jennings, Nicholas R},
	Journal = {Artificial Intelligence},
	Pages = {89--111},
	Publisher = {Elsevier},
	Title = {{Efficient crowdsourcing of unknown experts using bounded multi-armed bandits}},
	Url = {http://www.orchid.ac.uk/eprints/195/1/mab{\_}crowdsourcing{\_}AIJ.pdf},
	Volume = {214},
	Year = {2014},
	Bdsk-Url-1 = {http://www.orchid.ac.uk/eprints/195/1/mab%7B%5C_%7Dcrowdsourcing%7B%5C_%7DAIJ.pdf}}

@inproceedings{liu2010large,
	Author = {Liu, W and He, Junfeng and Chang, Shih-Fu},
	Booktitle = {ICML},
	Title = {{Large Graph Construction for Scalable Semi-Supervised Learning}},
	Year = {2010}}

@article{Sanchez-2003-Mislabeled,
	Author = {Sanchez, J S and Barandela, R and Marques, A I and Alejo, R and J., Badenas.},
	Journal = {Pattern Recognition Letteres 24},
	Pages = {1015--1022},
	Title = {{Analysis of New Techniques to Obtain Quality Training Sets.}},
	Year = {2003}}

@article{variant2011shamir,
	Author = {Shamir, Ohad},
	Journal = {CoRR},
	Title = {{A Variant of Azuma's Inequality for Martingales with Subgaussian Tails}},
	Volume = {abs/1110.2},
	Year = {2011}}

@inproceedings{kveton2010online,
	Abstract = {This paper proposes an algorithm for real-time learning without explicit
feedback. The algorithm combines the ideas of semi-supervised learning
on graphs and online learning. In particular, it iteratively builds
a graphical representation of its world and updates it with observed
examples. Labeled examples constitute the initial bias of the algorithm
and are provided offline, and a stream of unlabeled examples is collected
online to update this bias. We motivate the algorithm, discuss how
to implement it efficiently, prove a regret bound on the quality
of its solutions, and apply it to the problem of real-time face recognition.
Our recognizer runs in real time, and achieves superior precision
and recall on 3 challenging video datasets.},
	Address = {San Francisco, CA},
	Author = {Kveton, Branislav and Valko, Michal and Phillipose, Matthai and Huang, Ling},
	Booktitle = {The Fourth IEEE Online Learning for Computer Vision Workshop in The Twenty--Third IEEE Conference on Computer Vision and Pattern Recognition},
	Keywords = {misovalko},
	Mendeley-Tags = {misovalko},
	Title = {{Online Semi-Supervised Perception: Real-Time Learning without Explicit Feedback}},
	Year = {2010}}

@article{Martinet1978,
	Author = {Martinet, B},
	Journal = {ESAIM: Mathematical Modelling and Numerical Analysis - Mod{\'{e}}lisation Math{\'{e}}matique et Analyse Num{\'{e}}rique},
	Number = {2},
	Pages = {153--171},
	Publisher = {EDP Sciences},
	Title = {{Perturbation des m{\'{e}}thodes d'optimisation. Applications}},
	Url = {http://eudml.org/doc/193317},
	Volume = {12},
	Year = {1978},
	Bdsk-Url-1 = {http://eudml.org/doc/193317}}

@inproceedings{goldberg2008online,
	Author = {Goldberg, Andrew and Li, Ming and Zhu, Xiaojin},
	Booktitle = {Proceeding of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
	Title = {{Online Manifold Regularization: A New Learning Setting and Empirical Study}},
	Year = {2008}}

@inproceedings{boyan1995generalization,
	Author = {Boyan, Justin and Moore, Andrew},
	Booktitle = {Advances in Neural Information Processing Systems 7},
	Pages = {369--376},
	Title = {{Generalization in Reinforcement Learning: Safely Approximating the Value Function}},
	Year = {1995}}

@article{gine2010adaptive,
	Author = {Gin{\'{e}}, Evarist and Nickl, Richard},
	Journal = {Bernoulli},
	Number = {4},
	Pages = {1137--1163},
	Title = {{Adaptive estimation of a distribution function and its density in sup-norm loss by wavelet and spline projections}},
	Volume = {16},
	Year = {2010}}

@article{HW09,
	Author = {Helmbold, D P and Warmuth, M},
	Journal = {Journal of Machine Learning Research},
	Pages = {1705--1736},
	Title = {{Learning Permutations with Exponential Weights}},
	Volume = {10},
	Year = {2009}}

@inproceedings{LV06,
	Author = {Lovasz, L and Vempala, S},
	Booktitle = {Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)},
	Pages = {57--68},
	Title = {{Fast algorithms for logconcave functions: sampling, rounding, integration and optimization}},
	Year = {2006}}

@inproceedings{kuniyoshi94visionbased,
	Author = {Kuniyoshi, Y and Riekki, J and Ishii, M and Rougeaux, S and Kita, N and Sakane, S and Kakikura, M},
	Booktitle = {IEEE/RSJ IROS},
	Pages = {925--931},
	Title = {{Vision-based behaviors for multi-robot cooperation}},
	Url = {citeseer.ist.psu.edu/kuniyoshi94visionbased.html},
	Year = {1994},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/kuniyoshi94visionbased.html}}

@article{isaac2009overrides,
	Abstract = {BACKGROUND: Electronic prescribing systems with decision support may
improve patient safety in ambulatory care by offering drug allergy
and drug interaction alerts. However, preliminary studies show that
clinicians override most of these alerts. METHODS: We performed a
retrospective analysis of 233 537 medication safety alerts generated
by 2872 clinicians in Massachusetts, New Jersey, and Pennsylvania
who used a common electronic prescribing system from January 1, 2006,
through September 30, 2006. We used multivariate techniques to examine
factors associated with alert acceptance. RESULTS: A total of 6.6{\%}
of electronic prescription attempts generated alerts. Clinicians
accepted 9.2{\%} of drug interaction alerts and 23.0{\%} of allergy alerts.
High-severity interactions accounted for most alerts (61.6{\%}); clinicians
accepted high-severity alerts slightly more often than moderate-
or low-severity interaction alerts (10.4{\%}, 7.3{\%}, and 7.1{\%}, respectively;
P {\textless} .001). Clinicians accepted 2.2{\%} to 43.1{\%} of high-severity interaction
alerts, depending on the classes of interacting medications. In multivariable
analyses, we found no difference in alert acceptance among clinicians
of different specialties (P = .16). Clinicians were less likely to
accept a drug interaction alert if the patient had previously received
the alerted medication (odds ratio, 0.03; 95{\%} confidence interval,
0.03-0.03). CONCLUSION: Clinicians override most medication alerts,
suggesting that current medication safety alerts may be inadequate
to protect patient safety.},
	Author = {Isaac, Thomas and Weissman, Joel S and Davis, Roger B and Massagli, Michael and Cyrulik, Adrienne and Sands, Daniel Z and Weingart, Saul N},
	Doi = {10.1001/archinternmed.2008.551},
	Institution = {Division of General Medicine and Primary Care, Beth Israel Deaconess Medical Center, 330 Brookline Ave., Boston, MA 02215, USA. tisaac@bidmc.harvard.edu},
	Journal = {Arch Intern Med},
	Keywords = {80 and over; Ambulatory Care; Drug Hypersensitivi,Adolescent; Adult; Adverse Drug Reaction Reporting,Computer-Assisted; Female; Humans; Male; Medical,prevention /{\&}/ control; Drug Interactions; Drug T,prevention /{\&}/ control; Medicine,statistics /{\&}/ numerical data; Middle Aged; Physi,statistics /{\&}/ numerical data; Retrospective Stud},
	Month = {feb},
	Number = {3},
	Pages = {305--311},
	Pmid = {19204222},
	Title = {{Overrides of medication alerts in ambulatory care.}},
	Url = {http://dx.doi.org/10.1001/archinternmed.2008.551},
	Volume = {169},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1001/archinternmed.2008.551}}

@inproceedings{augustin,
	Author = {Lef{\`{e}}vre, A and Bach, F and F{\'{e}}votte, C},
	Booktitle = {Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
	Title = {{Itakura-{\{}S{\}}aito Nonnegative Matrix Factorization With Group Sparsity}},
	Year = {2011}}

@inproceedings{russell1998learning,
	Abstract = {This talk proposes a very simple 'baseline architecture' for a learning agent that can handle stochastic, partially observable environments. The architecture uses reinforcement learning together with a method for representing temporal processes as graphical models. I will discuss methods for learning the parameters and structure of such representations from sensory inputs, and for computing posterior probabilities. Some open problems remain before we can try out the complete agent; more arise when we consider scaling up. A second theme of the talk will be whether reinforcement learning can provide a good model of animal and human learning. To answer this question, we must do inverse reinforcement learning: given the observed behaviour, what reward signal, if any, is being optimized? This seems to be a very interesting problem for the COLT, UAI, and ML communities, and has been addressed in econometrics under the heading of structural estimation of Markov decision processes.},
	Author = {Russell, Stuart},
	Booktitle = {Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT)},
	Doi = {10.1145/279943.279964},
	Isbn = {1581130570},
	Keywords = {Decision theory,Graphic methods,Inverse problems,Inverse reinforcement learning,Learning agents,Learning systems,Markov processes,Mathematical models,Optimization,Probability,Problem solving,Reinforcement learning},
	Pages = {101--103},
	Title = {{Learning agents for uncertain environments (extended abstract)}},
	Url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0031640746{\&}partnerID=40},
	Year = {1998},
	Bdsk-Url-1 = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0031640746%7B%5C&%7DpartnerID=40},
	Bdsk-Url-2 = {https://doi.org/10.1145/279943.279964}}

@article{chen2009similarity,
	Abstract = {This paper reviews and extends the field of similarity-based classification, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classification problems. Specifically, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning.},
	Author = {Chen, Yihua and Garcia, Eric K and Gupta, Maya R and Rahimi, Ali and Cazzanti, Luca},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2009 - Similarity-based Classification Concepts and Algorithms.pdf:pdf},
	Issn = {15324435},
	Journal = {Journal of Machine Learning Research},
	Number = {206},
	Pages = {747--776},
	Publisher = {JMLR. org},
	Title = {{Similarity-based Classification: Concepts and Algorithms}},
	Url = {http://jmlr.csail.mit.edu/papers/volume10/chen09a/chen09a.pdf},
	Volume = {10},
	Year = {2009},
	Bdsk-Url-1 = {http://jmlr.csail.mit.edu/papers/volume10/chen09a/chen09a.pdf}}

@article{kumar2012sampling,
	Author = {Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},
	Journal = {J. Mach. Learn. Res.},
	Number = {1},
	Pages = {981--1006},
	Title = {{Sampling Methods for the Nystr{\"{o}}m Method}},
	Volume = {13},
	Year = {2012}}

@article{cornuejols1977location,
	Author = {Cornuejols, G and Fisher, M L and Nemhauser, G L},
	Journal = {Management Science},
	Number = {8},
	Pages = {789--810},
	Publisher = {JSTOR},
	Title = {{Location of bank accounts to optimize float: An analytic study of exact and approximate algorithms}},
	Volume = {23},
	Year = {1977}}

@inproceedings{campbell2000linear,
	Annote = {comps{\_}ano},
	Author = {Campbell, Colin and Bennett, Kristin P},
	Booktitle = {Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS) 2000},
	Pages = {395--401},
	Title = {{A Linear Programming Approach to Novelty Detection}},
	Url = {http://books.nips.cc/papers/files/nips13/CampbellBennett.pdf},
	Year = {2000},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips13/CampbellBennett.pdf}}

@article{guez2012efficient,
	Abstract = {Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayesoptimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.},
	Author = {Guez, Arthur and Silver, David and Dayan, Peter},
	Journal = {Neural Information Processing Systems},
	Title = {{Efficient Bayes-adaptive reinforcement learning using sample-based search}},
	Year = {2012}}

@inproceedings{besbes2014optimal,
	Abstract = {In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward "variation" and the minimal achievable regret. Our analysis draws some connections between two rather disparate strands of literature: the adversarial and the stochastic MAB frameworks.},
	Archiveprefix = {arXiv},
	Arxivid = {1405.3316},
	Author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1405.3316},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Besbes, Gur, Zeevi - 2014 - Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with Non-stationary Rewards.pdf:pdf},
	Month = {may},
	Title = {{Stochastic multi-armed bandit problem with non-stationary rewards}},
	Url = {http://arxiv.org/abs/1405.3316},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1405.3316}}

@article{spielman2007spectral,
	Abstract = {Spectral partitioning methods use the Fiedler vector-the eigenvector of the second-smallest eigenvalue of the Laplacian matrix-to find a small separator of a graph. These methods are important components of many scientific numerical algorithms and have been demonstrated by experiment to work extremely well. In this paper, we show that spectral partitioning methods work well on bounded-degree planar graphs and finite element meshes-the classes of graphs to which they are usually applied. While naive spectral bisection does not necessarily work, we prove that spectral partitioning techniques can be used to produce separators whose ratio of vertices removed to edges cut is O (sqrt(n)) for bounded-degree planar graphs and two-dimensional meshes and O(n1/d) for well-shaped d-dimensional meshes. The heart of our analysis is an upper bound on the second-smallest eigenvalues of the Laplacian matrices of these graphs: we prove a bound of O(1/n) for bounded-degree planar graphs and O(1/n2/d) for well-shaped d-dimensional meshes. ?? 2006 Elsevier Inc. All rights reserved.},
	Author = {Spielman, Daniel A. and Teng, Shang H.},
	Journal = {Linear Algebra and Its Applications},
	Keywords = {Eigenvalue problems,Graph embedding,Graph partitioning,Spectral analysis,Spectral methods},
	Pages = {284--305},
	Title = {{Spectral partitioning works: Planar graphs and finite element meshes}},
	Volume = {421},
	Year = {2007}}

@phdthesis{valko2016bandits,
	Abstract = {We investigate the structural properties of certain sequential decision-making problems with limited feedback (bandits) in order to bring the known algorithmic solutions closer to a practical use. In the first part, we put a special emphasis on structures that can be represented as graphs on actions, in the second part we study the large action spaces that can be of exponential size in the number of base actions or even infinite. We show how to take advantage of structures over the actions and (provably) learn faster.},
	Author = {Valko, Michal},
	School = {{\'{E}}cole normale sup{\'{e}}rieure de Cachan},
	Title = {{Bandits on graphs and structures}},
	Type = {habilitation},
	Url = {https://hal.inria.fr/tel-01359757/document},
	Year = {2016},
	Bdsk-Url-1 = {https://hal.inria.fr/tel-01359757/document}}

@article{jegelka2011online,
	Abstract = {Key processes during recruitment of Quercus petraea and Corylus avellana were investigated in abandoned calcareous grasslands and adjacent scrub using the following methods: (1) observation of hoarding animals during the main period of ripening of acorns and hazel nuts, (2) exposition of seeds on the soil surface and in 56 cm depth to test differences in predation and germination, and (3) mapping of seedlings in the grasslands. European jays (Garrulus glandarius) and mice were the main disperser of seeds. Jays preferred acorns, whereas the rodents were less selective, but probably more important for dispersal of nuts. The maximum dispersal distance was about 1020 m for mice and was estimated several hundred metres for jays. Mice collected hoards of several seeds in about 2 cm depth in the soil, whereas jays stored single seeds. Seed predation and probably hoarding by mice were highest under scrub and in unmown grassland, while jays preferred mown sites for hoarding. However, hiding of seeds in the soil reduced losses in all sites. Predation of nuts was slightly less intensive than that of acorns. Seeds of Corylus were more sensitive to desiccation than Quercus, but in both species germination was lower for seeds exposed on the soil surface and in drier sites. Quercus and Corylus were the most abundant woody species in the fallow grasslands, probably due to the effective multi-staged dispersal by jays and mice, whereas wind-dispersed and fleshy-fruited species were less common; the latter restricted to margins of adjacent scrubland. The study provides several examples for discordance in suitability of patches for seeds and seedlings due to different habitat requirements of successive developmental stages. This result emphasizes the need for studies in the multiple stages during recruitment of vertebrate-dispersed plants.},
	Author = {Jegelka, Stefanie and Bilmes, Jeff},
	Editor = {Getoor, Lise and Scheffer, Tobias},
	Isbn = {9781450306195},
	Journal = {Strategies},
	Number = {1},
	Pages = {345--352},
	Publisher = {ACM},
	Series = {ICML '11},
	Title = {{Online Submodular Minimization for Combinatorial Structures}},
	Url = {http://ssli.ee.washington.edu/{~}bilmes/mypubs/jegelka2011-online-submodular-min.extended.pdf},
	Volume = {125},
	Year = {2011},
	Bdsk-Url-1 = {http://ssli.ee.washington.edu/%7B~%7Dbilmes/mypubs/jegelka2011-online-submodular-min.extended.pdf}}

@techreport{cohen2015ridge,
	Abstract = {Often used as importance sampling probabilities, leverage scores have become indispensable in randomized algorithms for linear algebra, optimization, graph theory, and machine learning. A major body of work seeks to adapt these scores to low-rank approximation problems. However, existing "low-rank leverage scores" can be difficult to compute, often work for just a single application, and are sensitive to matrix perturbations. We show how to avoid these issues by exploiting connections between low-rank approximation and regularization. Specifically, we employ ridge leverage scores, which are simply standard leverage scores computed with respect to an {\$}\backslashell{\_}2{\$} regularized input. Importance sampling by these scores gives the first unified solution to two of the most important low-rank sampling problems: {\$}(1+\backslashepsilon){\$} error column subset selection and {\$}(1+\backslashepsilon){\$} error projection-cost preservation. Moreover, ridge leverage scores satisfy a key monotonicity property that does not hold for any prior low-rank leverage scores. Their resulting robustness leads to two sought-after results in randomized linear algebra. 1) We give the first input-sparsity time low-rank approximation algorithm based on iterative column sampling, resolving an open question posed in [LMP13], [CLM+15], and [AM15]. 2) We give the first single-pass streaming column subset selection algorithm whose real-number space complexity has no dependence on stream length.},
	Archiveprefix = {arXiv},
	Arxivid = {1511.07263},
	Author = {Cohen, Michael B. and Musco, Cameron and Musco, Christopher},
	Eprint = {1511.07263},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Cohen, Musco, Musco - 2015 - Ridge Leverage Scores for Low-Rank Approximation.pdf:pdf},
	Month = {nov},
	Title = {{Ridge leverage scores for low-rank approximation}},
	Url = {http://arxiv.org/abs/1511.07263},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1511.07263}}

@article{cooper1992bayesian,
	Abstract = {This paper presents a Bayesian method for constructing probabilistic
networks from databases. In particular, we focus on constructing
Bayesian belief networks. Potential applications include computer-assisted
hypothesis testing, automated scientific discovery, and automated
construction of probabilistic expert systems. We extend the basic
method to handle missing data and hidden (latent) variables. We show
how to perform probabilistic inference by averaging over the inferences
of multiple belief networks. Results are presented of a preliminary
evaluation of an algorithm for constructing a belief network from
a database of cases. Finally, we relate the methods in this paper
to previous work, and we discuss open problems.},
	Author = {Cooper, G F and Herskovits, E},
	Issn = {0885-6125},
	Journal = {Machine Learning},
	Keywords = {algorithm,bayesian,induction,learning,network,structure},
	Month = {oct},
	Number = {4},
	Pages = {309--347},
	Title = {{A Bayesian Method for the Induction of Probabilistic Networks from Data}},
	Url = {http://www.ingentaconnect.com/content/klu/mach/1992/00000009/00000004/00422779},
	Volume = {09},
	Year = {1992},
	Bdsk-Url-1 = {http://www.ingentaconnect.com/content/klu/mach/1992/00000009/00000004/00422779}}

@incollection{rakhlin12rr,
	Author = {Rakhlin, Sasha and Shamir, Ohad and Sridharan, Karthik},
	Booktitle = {Advances in Neural Information Processing Systems 25},
	Pages = {2150--2158},
	Title = {{Relax and Randomize : From Value to Algorithms}},
	Year = {2012}}

@inproceedings{BM11,
	Author = {Bach, F and Moulines, E},
	Booktitle = {Advances in Neural Information Processing Systems (NIPS)},
	Title = {{Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning}},
	Year = {2011}}

@inproceedings{lafferty01conditional,
	Address = {San Francisco, CA, USA},
	Annote = {From Duplicate 1 (Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data - Lafferty, John D; McCallum, Andrew; Pereira, Fernando C N)

comps{\_}models},
	Author = {Lafferty, John D and McCallum, Andrew and Pereira, Fernando C N},
	Booktitle = {ICML '01: Proceedings of the Eighteenth International Conference on Machine Learning},
	Isbn = {1-55860-778-1},
	Pages = {282--289},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data}},
	Url = {http://www.cis.upenn.edu/{~}pereira/papers/crf.pdf citeseer.ist.psu.edu/lafferty01conditional.html},
	Year = {2001},
	Bdsk-Url-1 = {http://www.cis.upenn.edu/%7B~%7Dpereira/papers/crf.pdf%20citeseer.ist.psu.edu/lafferty01conditional.html}}

@article{Flajolet2015,
	Abstract = {Optimal regret bounds for Multi-Armed Bandit problems are now well documented. They can be classified into two categories based on the growth rate with respect to the time horizon {\$}T{\$}: (i) small, distribution-dependent, bounds of order of magnitude {\$}\backslashbackslashln(T){\$} and (ii) robust, distribution-free, bounds of order of magnitude {\$}\backslashbackslashsqrt{\{}\backslash{\{}{\}}T{\{}\backslash{\}}{\}}{\$}. The Bandits with Knapsacks model, an extension to the framework allowing to model resource consumption, lacks this clear-cut distinction. While several algorithms have been shown to achieve asymptotically optimal distribution-free bounds on regret, there has been little progress toward the development of small distribution-dependent regret bounds. We partially bridge the gap by designing a general-purpose algorithm with distribution-dependent regret bounds that are logarithmic in the initial endowments of resources in several important cases that cover many practical applications, including dynamic pricing with limited supply, bid optimization in online advertisement auctions, and dynamic procurement.},
	Archiveprefix = {arXiv},
	Arxivid = {1510.01800},
	Author = {Flajolet, Arthur and Jaillet, Patrick},
	Eprint = {1510.01800},
	Journal = {arXiv preprint},
	Title = {{Logarithmic regret bounds for Bandits with Knapsacks}},
	Url = {http://arxiv.org/abs/1510.01800},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1510.01800}}

@inproceedings{cooper1988method,
	Author = {Cooper, Gregory},
	Booktitle = {Proceedings of the Workshop on Uncertainty in Artificial Intelligence},
	Pages = {55--63},
	Title = {{A Method for Using Belief Networks as Influence Diagrams}},
	Year = {1988}}

@incollection{JN11a,
	Author = {Juditsky, A and Nemirovski, A},
	Booktitle = {Optimization for Machine Learning},
	Editor = {Sra, S and Nowozin, S and Wright, S},
	Pages = {121--147},
	Publisher = {MIT press},
	Title = {{First-Order Methods for Nonsmooth Convex Large-Scale Optimization, I: General Purpose Methods}},
	Year = {2011}}

@inproceedings{finkel08crfpcfg,
	Author = {Finkel, Jenny R and Kleeman, Alex and Manning, Christopher D},
	Booktitle = {ACL '08: Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics},
	Pages = {959--967},
	Publisher = {Association for Computational Linguistics},
	Title = {{Efficient, Feature-based, Conditional Random Field Parsing}},
	Year = {2008}}

@inproceedings{gabillon2012best,
	Author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Best arm identification: A unified approach to fixed budget and fixed confidence}},
	Year = {2012}}

@article{samukhin2008laplacian,
	Abstract = {We study the Laplacian operator of an uncorrelated random network and, as an application, consider hopping processes (diffusion, random walks, signal propagation, etc.) on networks. We develop a strict approach to these problems. We derive an exact closed set of integral equations, which provide the averages of the Laplacian operator's resolvent. This enables us to describe the propagation of a signal and random walks on the network. We show that the determining parameter in this problem is the minimum degree {\$}q{\_}m{\$} of vertices in the network and that the high-degree part of the degree distribution is not that essential. The position of the lower edge of the Laplacian spectrum {\$}\backslashlambda{\_}c{\$} appears to be the same as in the regular Bethe lattice with the coordination number {\$}q{\_}m{\$}. Namely, {\$}\backslashlambda{\_}c{\textgreater}0{\$} if {\$}q{\_}m{\textgreater}2{\$}, and {\$}\backslashlambda{\_}c=0{\$} if {\$}q{\_}m\backslashleq2{\$}. In both these cases the density of eigenvalues {\$}\backslashrho(\backslashlambda)\backslashto0{\$} as {\$}\backslashlambda\backslashto\backslashlambda{\_}c+0{\$}, but the limiting behaviors near {\$}\backslashlambda{\_}c{\$} are very different. In terms of a distance from a starting vertex, the hopping propagator is a steady moving Gaussian, broadening with time. This picture qualitatively coincides with that for a regular Bethe lattice. Our analytical results include the spectral density {\$}\backslashrho(\backslashlambda){\$} near {\$}\backslashlambda{\_}c{\$} and the long-time asymptotics of the autocorrelator and the propagator.},
	Archiveprefix = {arXiv},
	Arxivid = {0706.1176},
	Author = {Samukhin, A. N. and Dorogovtsev, S. N. and Mendes, J. F. F.},
	Doi = {10.1103/PhysRevE.77.036115},
	Eprint = {0706.1176},
	Issn = {1539-3755},
	Journal = {Physical Review E},
	Month = {mar},
	Number = {3},
	Pages = {036115},
	Title = {{Laplacian spectra of, and random walks on, complex networks: Are scale-free architectures really important?}},
	Url = {http://arxiv.org/abs/0706.1176 http://dx.doi.org/10.1103/PhysRevE.77.036115 https://link.aps.org/doi/10.1103/PhysRevE.77.036115},
	Volume = {77},
	Year = {2008},
	Bdsk-Url-1 = {http://arxiv.org/abs/0706.1176%20http://dx.doi.org/10.1103/PhysRevE.77.036115%20https://link.aps.org/doi/10.1103/PhysRevE.77.036115},
	Bdsk-Url-2 = {https://doi.org/10.1103/PhysRevE.77.036115}}

@inproceedings{calandriello2015large-scale,
	Abstract = {We introduce Sparse-HFS, a scalable algorithm that can compute solutions to SSL problems using only O(n polylog(n)) space and O(m polylog(n)) time.},
	Author = {Calandriello, Daniele and Lazaric, Alessandro and Valko, Michal},
	Booktitle = {Resource-Efficient Machine Learning workshop at International Conference on Machine Learning},
	Title = {{Large-scale semi-supervised learning with online spectral graph sparsification}},
	Year = {2015}}

@article{bertsekas1995counterexample,
	Author = {Bertsekas, Dimitri},
	Journal = {Neural Computation},
	Number = {2},
	Pages = {270--279},
	Title = {{A Counterexample for Temporal Differences Learning}},
	Volume = {7},
	Year = {1995}}

@article{hastie1996discriminant,
	Annote = {comps{\_}distance},
	Author = {Hastie, T and Tibshirani, R},
	Doi = {10.1109/34.506411},
	Issn = {0162-8828},
	Journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	Keywords = {adaptive systems,approximation theory,centroid information,curse of dimensionality,global dimension reduction,linear discriminant analysis,local decision boundaries,neighbourhood-based classifier,pattern classification,pattern recognition,posterior probability,probabilityadaptive nearest neighbor classificati},
	Month = {jun},
	Number = {6},
	Pages = {607--616},
	Title = {{Discriminant adaptive nearest neighbor classification}},
	Volume = {18},
	Year = {1996},
	Bdsk-Url-1 = {https://doi.org/10.1109/34.506411}}

@inproceedings{riquelme2017active,
	Abstract = {We explore the sequential decision making problem where the goal is to estimate uniformly well a number of linear models, given a shared budget of random contexts independently sampled from a known distribution. The decision maker must query one of the linear models for each incoming context, and receives an observation corrupted by noise levels that are unknown, and depend on the model instance. We present Trace-UCB, an adaptive allocation algorithm that learns the noise levels while balancing contexts accordingly across the different linear functions, and derive guarantees for simple regret in both expectation and high-probability. Finally, we extend the algorithm and its guarantees to high dimensional settings, where the number of linear models times the dimension of the contextual space is higher than the total budget of samples. Simulations with real data suggest that Trace-UCB is remarkably robust, outperforming a number of baselines even when its assumptions are violated.},
	Author = {Riquelme, Carlos and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
	Booktitle = {International Conference on Machine Learning},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Riquelme, Ghavamzadeh, Lazaric - 2017 - Active Learning for Accurate Estimation of Linear Models.pdf:pdf},
	Title = {{Active learning for accurate estimation of linear models}},
	Year = {2017}}

@incollection{Combettes2010,
	Author = {Combettes, P L and Pesquet, J.-C.},
	Booktitle = {Fixed-Point Algorithms for Inverse Problems in Science and Engineering},
	Publisher = {Springer},
	Title = {{Proximal splitting methods in signal processing}},
	Year = {2010}}

@book{boyd94lmi,
	Address = {Philadelphia, PA},
	Author = {Boyd, S and El{\~{}}Ghaoui, L and Feron, E and Balakrishnan, V},
	Isbn = {0-89871-334-X},
	Publisher = {SIAM},
	Series = {Studies in Applied Mathematics},
	Title = {{Linear Matrix Inequalities in System and Control Theory}},
	Volume = {15},
	Year = {1994}}

@article{yu09ArbitraryRewards,
	Address = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
	Author = {Yu, Jia Yuan and Mannor, Shie and Shimkin, Nahum},
	Doi = {http://dx.doi.org/10.1287/moor.1090.0397},
	Issn = {0364-765X},
	Journal = {Mathematics of Operations Research},
	Number = {3},
	Pages = {737--757},
	Publisher = {INFORMS},
	Title = {{{\{}M{\}}arkov Decision Processes with Arbitrary Reward Processes}},
	Volume = {34},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/moor.1090.0397}}

@inproceedings{hertz2006learning,
	Address = {New York, NY, USA},
	Annote = {comps{\_}distance},
	Author = {Hertz, Tomer and Hillel, Aharon Bar and Weinshall, Daphna},
	Booktitle = {ICML '06: Proceedings of the 23rd international conference on Machine learning},
	Doi = {http://doi.acm.org/10.1145/1143844.1143895},
	Isbn = {1-59593-383-2},
	Pages = {401--408},
	Publisher = {ACM},
	Title = {{Learning a kernel function for classification with small training samples}},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1143844.1143895}}

@inproceedings{kveton2014matroid,
	Author = {Kveton, Branislav and Wen, Zheng and Ashkan, Azin and Eydgahi, Hoda and Eriksson, Brian},
	Booktitle = {Uncertainty in Artificial Intelligence},
	Title = {{Matroid bandits: Fast combinatorial optimization with learning}},
	Year = {2014}}

@article{HK12,
	Author = {Hazan, Elad and Kale, Satyen},
	Journal = {Journal of Machine Learning Research},
	Title = {{Online Submodular Minimization}},
	Volume = {13(Oct)},
	Year = {2012}}

@inproceedings{Petrov-Klein-2007:AAAI,
	Annote = {(Nectar Track)},
	Author = {Petrov, Slav and Klein, Dan},
	Pages = {1663--1666},
	Title = {{Learning and Inference for Hierarchically Split {\{}PCFG{\}}s}},
	Url = {www.eecs.berkeley.edu/{~}petrov/data/aaai2007.pdf},
	Bdsk-Url-1 = {www.eecs.berkeley.edu/%7B~%7Dpetrov/data/aaai2007.pdf}}

@incollection{he2012imitation,
	Author = {He, He and III, Hal Daume and Eisner, Jason},
	Booktitle = {Advances in Neural Information Processing Systems 25},
	Editor = {Bartlett, P and Pereira, F C N and Burges, C J C and Bottou, L and Weinberger, K Q},
	Pages = {3158--3166},
	Title = {{Imitation Learning by Coaching}},
	Url = {http://books.nips.cc/papers/files/nips25/NIPS2012{\_}1449.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips25/NIPS2012%7B%5C_%7D1449.pdf}}

@article{Tibshirani1996,
	Author = {Tibshirani, R},
	Journal = {Journal of the Royal Statistical Society. Series B},
	Pages = {267--288},
	Title = {{Regression shrinkage and selection via the {\{}L{\}}asso}},
	Year = {1996}}

@book{Nesterov2004,
	Author = {Nesterov, Yurii},
	Publisher = {Kluwer Academic Publishers},
	Title = {{Introductory lectures on convex optimization: A basic course}},
	Year = {2004}}

@phdthesis{vanroy1998planning,
	Author = {{Van Roy}, Benjamin},
	School = {Massachusetts Institute of Technology},
	Title = {{Planning Under Uncertainty in Complex Structured Environments}},
	Year = {1998}}

@techreport{vaswani2016adaptive,
	Abstract = {Most previous work on influence maximization in social networks is limited to the non-adaptive setting in which the marketer is supposed to select all of the seed users, to give free samples or discounts to, up front. A disadvantage of this setting is that the marketer is forced to select all the seeds based solely on a diffusion model. If some of the selected seeds do not perform well, there is no opportunity to course-correct. A more practical setting is the adaptive setting in which the marketer initially selects a batch of users and observes how well seeding those users leads to a diffusion of product adoptions. Based on this market feedback, she formulates a policy for choosing the remaining seeds. In this paper, we study adaptive offline strategies for two problems: (a) MAXSPREAD -- given a budget on number of seeds and a time horizon, maximize the spread of influence and (b) MINTSS -- given a time horizon and an expected number of target users to be influenced, minimize the number of seeds that will be required. In particular, we present theoretical bounds and empirical results for an adaptive strategy and quantify its practical benefit over the non-adaptive strategy. We evaluate adaptive and non-adaptive policies on three real data sets. We conclude that while benefit of going adaptive for the MAXSPREAD problem is modest, adaptive policies lead to significant savings for the MINTSS problem.},
	Archiveprefix = {arXiv},
	Arxivid = {1604.08171},
	Author = {Vaswani, Sharan and Lakshmanan, Laks V. S.},
	Eprint = {1604.08171},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani, Lakshmanan - 2016 - Adaptive influence maximization in social networks Why commit when you can adapt.pdf:pdf},
	Title = {{Adaptive influence maximization in social networks: Why commit when you can adapt?}},
	Year = {2016}}

@techreport{bartlett2019simple,
	Abstract = {We study the problem of optimizing a function under a budgeted number of evaluations. We only assume that the function is locally smooth around one of its global optima. The difficulty of optimization is measured in terms of 1) the amount of noise b of the function evaluation and 2) the local smoothness, d, of the function. A smaller d results in smaller optimization error. We come with a new, simple, and parameter-free approach. First, for all values of b and d, this approach recovers at least the state-of-the-art regret guarantees. Second, our approach additionally obtains these results while being agnostic to the values of both b and d. This leads to the first algorithm that naturally adapts to an unknown range of noise b and leads to significant improvements in a moderate and low-noise regime. Third, our approach also obtains a remarkable improvement over the state-of-the-art SOO algorithm when the noise is very low which includes the case of optimization under deterministic feedback (b=0). There, under our minimal local smoothness assumption, this improvement is of exponential magnitude and holds for a class of functions that covers the vast majority of functions that practitioners optimize (d=0). We show that our algorithmic improvement is also borne out in the numerical experiments, where we empirically show faster convergence on common benchmark functions.},
	Author = {Bartlett, Peter L and Gabillon, Victor and Valko, Michal},
	Title = {{A simple parameter-free and adaptive approach to optimization under a minimal local smoothness assumption}},
	Year = {2019}}

@techreport{chekuri2011submodular,
	Author = {Chekuri, C and Vondr{\'{a}}k, J and Zenklusen, R},
	Institution = {Arxiv},
	Number = {1105.4593},
	Title = {{Submodular function maximization via the multilinear relaxation and contention resolution schemes}},
	Year = {2011}}

@inproceedings{silander2006simple,
	Address = {Arlington, Virginia},
	Author = {Silander, Tomi and Myllymaki, Petri},
	Booktitle = {Proceedings of the 22nd Annual Conference on Uncertainty in Artificial Intelligence (UAI-06)},
	Publisher = {AUAI Press},
	Title = {{A simple approach for finding the globally optimal Bayesian network structure}},
	Year = {2006}}

@book{lauritzen96graphical,
	Author = {Lauritzen, S L},
	Howpublished = {Hardcover},
	Month = {jul},
	Publisher = {Oxford University Press, USA},
	Title = {{Graphical Models (Oxford Statistical Science Series)}},
	Year = {1996}}

@inproceedings{he2007graph-based,
	Address = {San Francisco, CA, USA},
	Author = {He, Jingrui and Carbonell, Jaime and Liu, Yan},
	Booktitle = {Proceedings of the 20th international joint conference on Artifical intelligence},
	Pages = {2492--2497},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Title = {{Graph-based semi-supervised learning as a generative model}},
	Url = {http://portal.acm.org/citation.cfm?id=1625275.1625677},
	Year = {2007},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?id=1625275.1625677}}

@inproceedings{xia2016budgeted,
	Author = {Xia, Yingce and Qin, Tao and Ma, Weidong and Yu, Nenghai and Liu, Tie-Yan},
	Booktitle = {International Joint Conference on Artificial Intelligence},
	Title = {{Budgeted multi-armed bandits with multiple plays}},
	Url = {https://www.ijcai.org/Proceedings/16/Papers/315.pdf},
	Year = {2016},
	Bdsk-Url-1 = {https://www.ijcai.org/Proceedings/16/Papers/315.pdf}}

@article{lovasz1983submodular,
	Author = {Lov{\'{a}}sz, L{\'{a}}szl{\'{o}}},
	Editor = {Bachem, Armin and Gr{\"{o}}tschel, Martin and Korte, Bernhard H},
	Journal = {Mathematical programming the state of the art},
	Pages = {235--257},
	Publisher = {Berlin: Springer-Verlag},
	Title = {{Submodular functions and convexity}},
	Url = {http://www.cs.elte.hu/{~}lovasz/scans/submodular.pdf},
	Year = {1983},
	Bdsk-Url-1 = {http://www.cs.elte.hu/%7B~%7Dlovasz/scans/submodular.pdf}}

@book{manning99foundations,
	Address = {Cambridge, Massachusetts},
	Author = {Manning, Christopher D and Sch{\"{u}}tze, Hinrich},
	Publisher = {The {\{}MIT{\}} Press},
	Title = {{Foundations of Statistical Natural Language Processing}},
	Url = {citeseer.ist.psu.edu/635422.html},
	Year = {1999},
	Bdsk-Url-1 = {citeseer.ist.psu.edu/635422.html}}

@article{lecchini2009stochastic,
	Abstract = {We introduce bounds on the finite-time performance of Markov chain Monte Carlo algorithms in approaching the global solution of stochastic optimization problems over continuous domains. A comparison with other state-of-the-art methods having finite-time guarantees for solving stochastic programming problems is included.},
	Archiveprefix = {arXiv},
	Arxivid = {0906.1055},
	Author = {Lecchini-Visintini, A. and Lygeros, J. and Maciejowski, J.},
	Eprint = {0906.1055},
	Month = {jun},
	Pages = {29},
	Title = {{Stochastic optimization on continuous domains with finite-time guarantees by Markov chain Monte Carlo methods}},
	Url = {http://arxiv.org/abs/0906.1055},
	Year = {2009},
	Bdsk-Url-1 = {http://arxiv.org/abs/0906.1055}}

@inproceedings{matsuzaki05latent,
	Address = {Morristown, NJ, USA},
	Author = {Matsuzaki, Takuya and Miyao, Yusuke and Tsujii, Jun'ichi},
	Booktitle = {ACL '05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics},
	Doi = {http://dx.doi.org/10.3115/1219840.1219850},
	Pages = {75--82},
	Publisher = {Association for Computational Linguistics},
	Title = {{Probabilistic {\{}CFG{\}} with latent annotations}},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.3115/1219840.1219850}}

@article{mallows,
	Author = {Mallows, C L},
	Journal = {Technometrics},
	Number = {4},
	Pages = {661--675},
	Title = {{Some comments on {\{}C{\}}{\_}p}},
	Volume = {15},
	Year = {1973}}

@inproceedings{HKW10,
	Author = {Hazan, E and Kale, S and Warmuth, M},
	Booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)},
	Pages = {144--154},
	Title = {{Learning rotations with little regret}},
	Year = {2010}}

@inproceedings{klein03accurate,
	Author = {Klein, D and Manning, C D},
	Booktitle = {ACL '03: Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics},
	Pages = {423--430},
	Title = {{Accurate Unlexicalized Parsing}},
	Year = {2003}}

@article{glynn1990likelihood,
	Author = {Glynn, Peter},
	Journal = {Communications of the ACM},
	Number = {10},
	Pages = {75--84},
	Title = {{Likelihood Ratio Gradient Estimation for Stochastic Systems}},
	Volume = {33},
	Year = {1990}}

@inproceedings{ratliff2007online,
	Author = {Ratliff, Nathan and Bagnell, Andrew and Zinkevich, Martin},
	Booktitle = {Proceedings of the 11th International Conference on Artificial Intelligence and Statistics},
	Title = {{({\{}Online{\}}) Subgradient Methods for Structured Prediction}},
	Year = {2007}}

@inproceedings{titov07incremental,
	Address = {Prague, Czech Republic},
	Author = {Titov, Ivan and Henderson, James},
	Booktitle = {ACL '07: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics},
	Pages = {632--639},
	Publisher = {Association for Computational Linguistics},
	Title = {{Constituent Parsing with Incremental Sigmoid Belief Networks}},
	Url = {http://www.aclweb.org/anthology/P/P07/P07-0080},
	Year = {2007},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/P/P07/P07-0080}}

@inproceedings{samothrakis2013training,
	Author = {Samothrakis, Spyridon and Perez, Diego and Lucas, Simon},
	Booktitle = {NIPS Workshop on Causality},
	Month = {jan},
	Title = {{Training gradient boosting machines using curve-fitting and information-theoretic features for causal direction detection}},
	Url = {http://ssamot.me/papers/Samothrakis-NIPS2013-causality.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://ssamot.me/papers/Samothrakis-NIPS2013-causality.pdf}}

@article{dani2008price,
	Author = {Dani, Varsha and Hayes, Thomas and Kakade, Sham},
	Editor = {Platt, J C and Koller, D and Singer, Y and Roweis, S},
	Issn = {00368075},
	Journal = {Advances in Neural Information Processing Systems 20},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {1--8},
	Publisher = {MIT Press},
	Title = {{The Price of Bandit Information for Online Optimization}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.4607{\&}rep=rep1{\&}type=pdf},
	Volume = {20},
	Year = {2008},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.4607%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{shental2002adjustment,
	Address = {London, UK},
	Author = {Shental, Noam and Hertz, Tomer and Weinshall, Daphna and Pavel, Misha},
	Booktitle = {ECCV '02: Proceedings of the 7th European Conference on Computer Vision-Part IV},
	Isbn = {3-540-43748-7},
	Pages = {776--792},
	Publisher = {Springer-Verlag},
	Title = {{Adjustment Learning and Relevant Component Analysis}},
	Year = {2002}}

@incollection{boularias2012algorithms,
	Author = {Boularias, Abdeslam and Kroemer, Oliver and Peters, Jan},
	Booktitle = {Advances in Neural Information Processing Systems 25},
	Editor = {Bartlett, P and Pereira, F C N and Burges, C J C and Bottou, L and Weinberger, K Q},
	Pages = {2186--2194},
	Title = {{Algorithms for Learning Markov Field Policies}},
	Url = {http://books.nips.cc/papers/files/nips25/NIPS2012{\_}1084.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://books.nips.cc/papers/files/nips25/NIPS2012%7B%5C_%7D1084.pdf}}

@inproceedings{joachims1999transductive,
	Address = {San Francisco, CA, USA},
	Author = {Joachims, Thorsten},
	Booktitle = {ICML '99: Proceedings of the Sixteenth International Conference on Machine Learning},
	Isbn = {1-55860-612-2},
	Pages = {200--209},
	Title = {{Transductive Inference for Text Classification using Support Vector Machines}},
	Year = {1999}}

@inproceedings{joshi2005investigating,
	Address = {New York, NY, USA},
	Annote = {comps{\_}anX},
	Author = {Joshi, Shrijit S and Phoha, Vir V},
	Booktitle = {ACM-SE 43: Proceedings of the 43rd annual Southeast regional conference},
	Doi = {http://doi.acm.org/10.1145/1167350.1167387},
	Isbn = {1-59593-059-0},
	Pages = {98--103},
	Publisher = {ACM},
	Title = {{Investigating hidden Markov models capabilities in anomaly detection}},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1167350.1167387}}

@article{ashkan15optimal,
	Author = {Ashkan, Azin and Kveton, Branislav and Berkovsky, Shlomo and Wen, Zheng},
	Journal = {International Joint Conferences on Artificial Intelligence},
	Title = {{Optimal greedy diversity for recommendation}},
	Year = {2015}}

@inproceedings{price-bayesian,
	Author = {Price, Bob and Boutilier, Craig},
	Pages = {712--720},
	Title = {{A {\{}B{\}}ayesian Approach to Imitation in Reinforcement Learning}}}

@inproceedings{neu2015explore,
	Author = {Neu, Gergely},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Neu - 2015 - Explore no more Improved high-probability regret bounds for non-stochastic bandits.pdf:pdf},
	Title = {{Explore no more: Improved high-probability regret bounds for non-stochastic bandits}},
	Year = {2015}}

@inproceedings{kocak2014wspectral,
	Abstract = {Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this paper, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each recommended item is a node and its expected rating is similar to its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose two algorithms for solving our problem that scale linearly in this dimension. Our experiments on real-world content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens nodes evaluations.},
	Author = {Koc{\'{a}}k, Tom{\'{a}}{\v{s}} and Valko, Michal and Munos, R{\'{e}}mi and Kveton, Branislav and Agrawal, Shipra},
	Booktitle = {AAAI Workshop on Sequential Decision-Making with Big Data},
	Title = {{Spectral bandits for smooth graph functions with applications in recommender systems}},
	Year = {2014}}

@inproceedings{lin2011-class-submod-sum,
	Address = {Portland, OR},
	Annote = {(long paper)},
	Author = {Lin, H and Bilmes, J},
	Booktitle = {North American chapter of the Association for Computational Linguistics/Human Language Technology Conference (NAACL/HLT-2011)},
	Month = {jun},
	Title = {{A Class of Submodular Functions for Document Summarization}},
	Year = {2011}}

@article{littlestone1994weighted,
	Author = {Littlestone, Nick and Warmuth, Manfred},
	Journal = {Information and Computation},
	Number = {2},
	Pages = {212--261},
	Title = {{The weighted majority algorithm}},
	Volume = {108},
	Year = {1994}}

@techreport{submodlp,
	Author = {Obozinski, G and Bach, F},
	Institution = {HAL},
	Title = {{Convex relaxation of Combinatorial penalties}},
	Year = {2011}}

@techreport{pachocki2016analysis,
	Abstract = {We show that schemes for sparsifying matrices based on iteratively resampling rows yield guarantees matching classic 'offline' sparsifiers (see e.g. Spielman and Srivastava [STOC 2008]). In particular, this gives a formal analysis of a scheme very similar to the one proposed by Kelner and Levin [TCS 2013].},
	Archiveprefix = {arXiv},
	Arxivid = {1605.08194},
	Author = {Pachocki, Jakub},
	Eprint = {1605.08194},
	Title = {{Analysis of resparsification}},
	Url = {http://arxiv.org/abs/1605.08194},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1605.08194}}

@techreport{wen2016influence,
	Abstract = {We study a stochastic online problem of learning to influence in a social network with semi-bandit feedback, individual observations of how influenced users influence others. Our problem combines challenges of partial monitoring, because the learning agent only observes the influenced portion of the network, and combinatorial bandits, because the cardinality of the feasible set is exponential in the maximum number of influencers. We propose a computationally efficient UCB-like algorithm for solving our problem, IMLinUCB, and analyze it on forests. Our regret bounds are polynomial in all quantities of interest; reflect the structure of the network; and do not depend on inherently large quantities, such as the reciprocal of the minimum probability of being influenced and the cardinality of the action set. To the best of our knowledge, these are the first such results. IMLinUCB permits linear generalization and therefore is suitable for large-scale problems. We evaluate IMLinUCB on several synthetic problems and observe that the regret of IMLinUCB scales as suggested by our upper bounds. A special form of our problem can be viewed as a linear bandit and we match the regret bounds of LinUCB in this case.},
	Archiveprefix = {arXiv},
	Arxivid = {1605.06593},
	Author = {Wen, Zheng and Kveton, Branislav and Valko, Michal},
	Eprint = {1605.06593},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Wen, Kveton, Valko - 2016 - Influence maximization with semi-bandit feedback.pdf:pdf},
	Title = {{Influence maximization with semi-bandit feedback}},
	Year = {2016}}

@article{erdos1960evolution,
	Author = {Erdos, Paul and R{\'{e}}nyi, Alfr{\'{e}}d},
	Journal = {Publ. Mathematical Institute of the Hungarian Academy of Sciences},
	Title = {{On the evolution of random graphs}},
	Url = {http://www.citeulike.org/group/3072/article/1666220},
	Year = {1960},
	Bdsk-Url-1 = {http://www.citeulike.org/group/3072/article/1666220}}

@inproceedings{CWY13,
	Author = {Chen, Wei and Wang, Yajun and Yuan, Yang},
	Booktitle = {International Conference on Machine Learning},
	Pages = {151--159},
	Title = {{Combinatorial Multi-Armed Bandit: General Framework and Applications}},
	Year = {2013}}

@inproceedings{poupart2002piecewise,
	Author = {Poupart, Pascal and Boutilier, Craig and Patrascu, Relu and Schuurmans, Dale and Guestrin, Carlos},
	Booktitle = {Proceedings of the 18th National Conference on Artificial Intelligence},
	Pages = {292--299},
	Title = {{Greedy Linear Value-Approximation for Factored {\{}Markov{\}} Decision Processes}},
	Year = {2002}}

@article{Baraniuk2008,
	Author = {Baraniuk, R G and Cevher, V and Duarte, M F and Hegde, C},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {1982--2001},
	Title = {{Model-based compressive sensing}},
	Volume = {56},
	Year = {2010}}

@article{gale2003bayesian,
	Abstract = {We extend the standard model of social learning in two ways. First, we introduce a social network and assume that agents can only observe the actions of agents to whom they are connected by this network. Secondly, we allow agents to choose a different action at each date. If the network satisfies a connectedness assumption, the initial diversity resulting from diverse private information is eventually replaced by uniformity of actions, though not necessarily of beliefs, in finite time with probability one. We look at particular networks to illustrate the impact of network architecture on speed of convergence and the optimality of absorbing states. Convergence is remarkably rapid, so that asymptotic results are a good approximation even in the medium run. {\textcopyright} 2003 Elsevier Inc. All rights reserved.},
	Author = {Gale, Douglas and Kariv, Shachar},
	Journal = {Games and Economic Behavior},
	Number = {2},
	Pages = {329--346},
	Title = {{Bayesian learning in social networks}},
	Volume = {45},
	Year = {2003}}

@book{alpern2006theory,
	Author = {Alpern, Steve and Gal, Shmuel},
	Publisher = {Springer},
	Title = {{The theory of search games and rendezvous}},
	Url = {https://www.springer.com/fr/book/9780792374688},
	Year = {2006},
	Bdsk-Url-1 = {https://www.springer.com/fr/book/9780792374688}}

@article{higdon1998auxiliary,
	Author = {Higdon, David},
	Journal = {Journal of the American Statistical Association},
	Number = {442},
	Pages = {585--595},
	Title = {{Auxiliary Variable Methods for {\{}Markov{\}} Chain {\{}Monte Carlo{\}} with Applications}},
	Volume = {93},
	Year = {1998}}

@article{bar-hillel2005learning,
	Annote = {comps{\_}distance},
	Author = {Bar-Hillel, Aharon and Hertz, Tomer and Shental, Noam and Weinshall, Daphna},
	Journal = {Journal of Machine Learning Research},
	Pages = {937--965},
	Title = {{Learning a Mahalanobis Metric from Equivalence Constraints}},
	Url = {http://www.jmlr.org/papers/v6/bar-hillel05a.html},
	Volume = {6},
	Year = {2005},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/v6/bar-hillel05a.html}}

@inproceedings{locatelli2017adaptivity,
	Abstract = {This work addresses various open questions in the theory of active learning for nonparametric classification. Our contributions are both statistical and algorithmic: -We establish new minimax-rates for active learning under common $\backslash$textit{\{}noise conditions{\}}. These rates display interesting transitions -- due to the interaction between noise $\backslash$textit{\{}smoothness and margin{\}} -- not present in the passive setting. Some such transitions were previously conjectured, but remained unconfirmed. -We present a generic algorithmic strategy for adaptivity to unknown noise smoothness and margin; our strategy achieves optimal rates in many general situations; furthermore, unlike in previous work, we avoid the need for $\backslash$textit{\{}adaptive confidence sets{\}}, resulting in strictly milder distributional requirements.},
	Author = {Locatelli, Andrea and Carpentier, Alexandra and Kpotufe, Samory},
	Booktitle = {Conference on Learning Theory},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Locatelli, Carpentier, Kpotufe - 2017 - Adaptivity to noise parameters in nonparametric active learning.pdf:pdf},
	Month = {mar},
	Title = {{Adaptivity to noise parameters in nonparametric active learning}},
	Year = {2017}}

@inproceedings{yu09Modulated,
	Author = {Yu, Jia Yuan and Mannor, Shie},
	Booktitle = {Joint 48th IEEE Conference on Decision and Control and 28th Chinese Control Conference},
	Pages = {2946--2953},
	Publisher = {IEEE Press},
	Title = {{Arbitrarily modulated {\{}M{\}}arkov decision processes}},
	Year = {2009}}

@article{gittins1979bandit,
	Abstract = {The paper aims to give a unified account of the central concepts in recent work on bandit processes and dynamic allocation indices; to show how these reduce some previously intractable problems to the problem of calculating such indices; and to describe how these calculations may be carried out. Applications to stochastic scheduling, sequential clinical trials and a class of search problems are discussed.},
	Author = {Gittins, J C},
	Doi = {10.2307/2985029},
	Issn = {00359246},
	Journal = {Journal of the Royal Statistical Society Series B Methodological},
	Number = {2},
	Pages = {148--177},
	Publisher = {JSTOR},
	Series = {B},
	Title = {{Bandit processes and dynamic allocation indices}},
	Url = {http://www.jstor.org/stable/2985029},
	Volume = {41},
	Year = {1979},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2985029},
	Bdsk-Url-2 = {https://doi.org/10.2307/2985029}}

@article{harchaoui2008catching,
	Author = {Harchaoui, Z and L{\'{e}}vy-Leduc, C},
	Journal = {Adv. NIPS},
	Title = {{Catching change-points with {\{}L{\}}asso}},
	Volume = {20},
	Year = {2008}}

@article{HW98,
	Author = {Herbster, M and Warmuth, M},
	Journal = {Machine Learning},
	Pages = {151--178},
	Title = {{Tracking the Best Expert}},
	Volume = {32},
	Year = {1998}}

@article{jamakovic2006laplacian,
	Abstract = {The set of all eigenvalues of a characteristic matrix of a graph, also referred to as the spectrum, is a well-known topology retrieval method. In this paper, we study the spectrum of the Laplacian matrix of an ob- servable part of the Internet graph at the IP- level, extracted from traceroute measurements performed via RIPE NCC and PlanetLab. In order to investigate the factors influencing the Laplacian spectrum of the observed graphs, we study the following complex network models: the random graph of Erd˝ os-R{\'{e}}nyi, the small- world of Watts and Strogatz and the scale-free graph, derived from a Havel-Hakimi power- law degree sequence. Along with these complex network models, we also study the correspond- ing Minimum Spanning Tree (MST). Extensive simulations show that the Laplacian spectra of complex network models differ substantially from the spectra of the observed graphs. How- ever, the Laplacian spectra of the MST in the Erd˝ os-R{\'{e}}nyi random graph with uniformly distributed link weights does bear resemblance to it. Furthermore, we discuss an extensive set of topological characteristics extracted from the Laplacian spectra of the observed real-world graphs as well as from complex network models.},
	Author = {Jamakovic, A and Mieghem, P Van},
	Journal = {European Conference on Complex Systems},
	Pages = {1--6},
	Title = {{The Laplacian spectrum of complex networks}},
	Url = {http://repository.tudelft.nl/assets/uuid:abe61d93-4e25-41ab-90d4-2a55cf2982f5/The Laplacian Spectrum of Complex Networks.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://repository.tudelft.nl/assets/uuid:abe61d93-4e25-41ab-90d4-2a55cf2982f5/The%20Laplacian%20Spectrum%20of%20Complex%20Networks.pdf}}

@inproceedings{akoglu2010oddball:,
	Author = {Akoglu, Leman and McGlohon, Mary and Faloutsos, Christos},
	Booktitle = {Advances in Knowledge Discovery and Data Mining, 14th Pacific-Asia Conference, PAKDD 2010, Hyderabad, India, June 21-24, 2010. Proceedings. Part II},
	Pages = {410--421},
	Title = {{Oddball: Spotting Anomalies in Weighted Graphs}},
	Year = {2010}}

@techreport{malherbe2017global,
	Abstract = {The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional H$\backslash$"older like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive LIPO algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.},
	Author = {Malherbe, C{\'{e}}dric and Vayatis, Nicolas},
	Month = {mar},
	Title = {{Global optimization of Lipschitz functions}},
	Url = {http://arxiv.org/abs/1703.02628},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.02628}}

@misc{kaggle2013,
	Title = {{Kaggle}},
	Url = {https://www.kaggle.com/},
	Year = {2013},
	Bdsk-Url-1 = {https://www.kaggle.com/}}

@article{R76,
	Author = {Rockafellar, R Tyrrell},
	Journal = {SIAM Journal on Control and Optimization},
	Keywords = {algorithms,point,proximal},
	Number = {5},
	Pages = {877--898},
	Title = {{Monotone Operators and the Proximal Point Algorithm}},
	Volume = {14},
	Year = {1976}}

@article{Martinet1970,
	Author = {Martinet, B},
	Journal = {ESAIM: Mathematical Modelling and Numerical Analysis - Mod{\'{e}}lisation Math{\'{e}}matique et Analyse Num{\'{e}}rique},
	Number = {R3},
	Pages = {154--158},
	Publisher = {EDP Sciences},
	Title = {{R{\'{e}}gularisation d'in{\'{e}}quations variationnelles par approximations successives}},
	Volume = {4},
	Year = {1970}}

@misc{openmp2008openmp,
	Author = {OpenMP},
	Institution = {{\{}OpenMP{\}} Architecture Review Board},
	Title = {{{\{}OpenMP{\}} Application Program Interface -- Version 3.0}},
	Year = {2008}}

@incollection{bookchapter,
	Author = {Bach, F and Jenatton, R and Mairal, J and Obozinski, G},
	Booktitle = {Optimization for Machine Learning},
	Editor = {Sra, S and Nowozin, S and Wright, S J},
	Publisher = {MIT Press},
	Title = {{Convex optimization with sparsity-inducing norms}},
	Year = {2011}}

@inproceedings{pinto2009how,
	Author = {Pinto, Nicolas and DiCarlo, James and Cox, David},
	Booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	Title = {{How Far Can You Get with a Modern Face Recognition Test Set Using Only Simple Features?}},
	Year = {2009}}

@book{rockafellar97,
	Author = {Rockafellar, R T},
	Publisher = {Princeton University Press},
	Title = {{Convex Analysis}},
	Year = {1997}}

@inproceedings{dvijotham2010inverse,
	Abstract = {We present new algorithms for inverse optimal control (or inverse$\backslash$nreinforcement learning, IRL) within the framework of linearlysolvable$\backslash$nMDPs (LMDPs). Unlike most prior IRL algorithms which recover only$\backslash$nthe control policy of the expert, we recover the policy, the value$\backslash$nfunction and the cost function. This is possible because here the$\backslash$ncost and value functions are uniquely deffined given the policy.$\backslash$nDespite these special properties, we can handle a wide variety of$\backslash$nproblems such as the grid worlds popular in RL and most of the nonlinear$\backslash$nproblems arising in robotics and control engineering. Direct comparisons$\backslash$nto prior IRL algorithms show that our new algorithms provide more$\backslash$ninformation and are orders of magnitude faster. Indeed our fastest$\backslash$nalgorithm is the first inverse algorithm which does not require solving$\backslash$nthe forward problem; instead it performs unconstrained optimization$\backslash$nof a convex and easy-to-compute log-likelihood. Our work also sheds$\backslash$nlight on the recent Maximum Entropy (MaxEntIRL) algorithm, which$\backslash$nwas dened in terms of density estimation and the corresponding forward$\backslash$nproblem was left unspecified. We show that MaxEntIRL is inverting$\backslash$nan LMDP, using the less efficient of the algorithms derived here.$\backslash$nUnlike all prior IRL algorithms which assume pre-existing features,$\backslash$nwe study feature adaptation and show that such adaptation is essential$\backslash$nin continuous state spaces.},
	Author = {Dvijotham, Krishnamurthy and Todorov, Emanuel},
	Booktitle = {ICML},
	Keywords = {dblp},
	Pages = {335--342},
	Title = {{Inverse Optimal Control with Linearly-Solvable MDPs.}},
	Year = {2010}}

@inproceedings{colt2010,
	Author = {{Daniel Golovin} and {Andreas Krause}},
	Booktitle = {23rd Annual Conference on Learning Theory},
	Pages = {333--345},
	Title = {{Adaptive Submodularity: A New Approach to Active Learning and Stochastic Optimization}},
	Year = {2010}}

@inproceedings{Audibert2007,
	Address = {Berlin, Heidelberg},
	Author = {Audibert, Jean-Yves and Munos, R{\'{e}}mi and Szepesv{\'{a}}ri, Csaba},
	Doi = {10.1007/978-3-540-75225-7},
	Editor = {Hutter, Marcus and Servedio, Rocco A. and Takimoto, Eiji},
	Isbn = {978-3-540-75224-0},
	Issn = {0302-9743},
	Month = {oct},
	Pages = {150--165},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {{Algorithmic Learning Theory}},
	Url = {http://dl.acm.org/citation.cfm?id=1422422.1422442},
	Volume = {4754},
	Year = {2007},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1422422.1422442},
	Bdsk-Url-2 = {https://doi.org/10.1007/978-3-540-75225-7}}

@article{dunn1980convergence,
	Author = {Dunn, J C},
	Journal = {SIAM Journal on Control and Optimization},
	Pages = {473--487},
	Title = {{Convergence rates for conditional gradient sequences generated by implicit step length rules}},
	Volume = {18},
	Year = {1980}}

@inproceedings{locatelli2018adaptivity,
	Author = {Locatelli, Andrea and Carpentier, Alexandra},
	Booktitle = {Conference on Learning Theory},
	Title = {{Adaptivity to Smoothness in X-armed bandits}},
	Url = {http://proceedings.mlr.press/v75/locatelli18a/locatelli18a.pdf},
	Year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v75/locatelli18a/locatelli18a.pdf}}

@book{shor85minimization,
	Address = {New York, NY, USA},
	Author = {Shor, N Z and Kiwiel, K C and Ruszczynski, A},
	Isbn = {0-387-12763-1},
	Publisher = {Springer-Verlag New York, Inc.},
	Title = {{Minimization methods for non-differentiable functions}},
	Year = {1985}}

@article{ellison1993rules,
	Abstract = {This paper studies agents who consider the experiences of their neighbors in deciding which of two technologies to use. We analyze two learning environments, one in which the same technology is optimal for all players and another in which each technology is better for some of them. In both environments, players use exogenously specified rules of thumb that ignore historical data but may incorporate a tendency to use the more popular technology. In some cases these naive rules can lead to fairly efficient decisions in the long run, but adjustment can be slow when a superior technology is first introduced.},
	Author = {Ellison, Glenn and Fudenberg, Drew},
	Journal = {Journal of Political Economy},
	Number = {4},
	Pages = {612--643},
	Title = {{Rules of thumb for social learning}},
	Volume = {101},
	Year = {1993}}

@inproceedings{syed2008apprenticeship,
	Author = {Syed, Umar and Schapire, Robert and Bowling, Michael},
	Booktitle = {Proceedings of the Twenty-Fifth International Conference on Machine Learning (ICML)},
	Pages = {1032--1039},
	Title = {{Apprenticeship Learning Using Linear Programming}},
	Url = {http://www.cs.princeton.edu/{~}usyed/SyedBowlingSchapireICML2008.pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://www.cs.princeton.edu/%7B~%7Dusyed/SyedBowlingSchapireICML2008.pdf}}

@article{WK14,
	Author = {Warmuth, Manfred and Koolen, Wouter},
	Journal = {COLT 2014 open problem},
	Title = {{Shifting experts on easy data}},
	Year = {2014}}

@article{klopp2015matrix,
	Author = {Klopp, Olga},
	Journal = {Electronic journal of statistics},
	Number = {2},
	Pages = {2348--2369},
	Publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
	Title = {{Matrix completion by singular value thresholding: Sharp bounds}},
	Volume = {9},
	Year = {2015}}

@article{helmbold2000adaptive,
	Author = {Helmbold, David and Long, Darrell and Sconyers, Tracey and Sherrod, Bruce},
	Journal = {Mobile Networks and Applications},
	Number = {4},
	Pages = {285--297},
	Title = {{Adaptive Disk Spin-Down for Mobile Computers}},
	Volume = {5},
	Year = {2000}}

@inproceedings{garcke2005semi,
	Author = {Garcke, Jochen and Griebel, Michael},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Semi-supervised learning with sparse grids}},
	Year = {2005}}

@article{negahban2012restricted,
	Abstract = {We consider the matrix completion problem under a form of row/column weighted entrywise sam-pling, including the case of uniform entrywise sampling as a special case. We analyze the associated random observation operator, and prove that with high probability, it satisfies a form of restricted strong convexity with respect to weighted Frobenius norm. Using this property, we obtain as corol-laries a number of error bounds on matrix completion in the weighted Frobenius norm under noisy sampling and for both exact and near low-rank matrices. Our results are based on measures of the " spikiness " and " low-rankness " of matrices that are less restrictive than the incoherence con-ditions imposed in previous work. Our technique involves an M-estimator that includes controls on both the rank and spikiness of the solution, and we establish non-asymptotic error bounds in weighted Frobenius norm for recovering matrices lying with ℓ q - " balls " of bounded spikiness. Us-ing information-theoretic methods, we show that no algorithm can achieve better estimates (up to a logarithmic factor) over these same sets, showing that our conditions on matrices and associated rates are essentially optimal.},
	Author = {Negahban, Sahand and Wainwright, Martin J},
	Journal = {Journal of Machine Learning Research},
	Keywords = {collaborative filtering,convex optimization,matrix completion},
	Pages = {1665--1697},
	Title = {{Restricted strong convexity and weighted matrix completion: Optimal bounds with noise}},
	Volume = {13},
	Year = {2012}}

@phdthesis{collins99thesis,
	Author = {Collins, Michael},
	School = {University of Pennsylvania},
	Title = {{Head-Driven Statistical Models for Natural Language Processing}},
	Year = {1999}}

@inproceedings{AweKlein04,
	Author = {Awerbuch, Baruch and Kleinberg, Robert D},
	Pages = {45--53},
	Title = {{Adaptive routing with end-to-end feedback: distributed learning and geometric approaches}}}

@book{howard1960dynamic,
	Address = {Cambridge, MA},
	Author = {Howard, Ronald},
	Publisher = {MIT Press},
	Title = {{Dynamic Programming and {\{}Markov{\}} Processes}},
	Year = {1960}}

@inproceedings{contal2016stochastic,
	Abstract = {The paper considers the problem of global optimization in the setup of stochastic process bandits. We introduce an UCB algorithm which builds a cascade of discretization trees based on generic chaining in order to render possible his operability over a continuous domain. The theoretical framework applies to functions under weak probabilistic smoothness assumptions and also extends significantly the spectrum of application of UCB strategies. Moreover generic regret bounds are derived which are then specialized to Gaussian processes indexed on infinite-dimensional spaces as well as to quadratic forms of Gaussian processes. Lower bounds are also proved in the case of Gaussian processes to assess the optimality of the proposed algorithm.},
	Archiveprefix = {arXiv},
	Arxivid = {1602.04976},
	Author = {Contal, Emile and Vayatis, Nicolas},
	Eprint = {1602.04976},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Contal, Vayatis - 2016 - Stochastic process bandits Upper confidence bounds algorithms via generic chaining.pdf:pdf},
	Month = {feb},
	Title = {{Stochastic process bandits: Upper confidence bounds algorithms via generic chaining}},
	Year = {2016}}

@inproceedings{bartlett05exponentiated,
	Author = {Bartlett, Peter L and Collins, Michael and Taskar, Ben and McAllester, David},
	Pages = {113--120},
	Title = {{Exponentiated Gradient Algorithms for Large-margin Structured Classification}}}

@inproceedings{zhang1996high-performance,
	Author = {Zhang, Wei and Dietterich, Thomas},
	Booktitle = {Advances in Neural Information Processing Systems 8},
	Pages = {1024--1030},
	Title = {{High-Performance Job-Shop Scheduling with a Time-Delay {\{}TD{\}}($\lambda$) Network}},
	Year = {1996}}

@inproceedings{Kalai03efficient,
	Address = {New York, NY, USA},
	Author = {Kalai, A and Vempala, S},
	Booktitle = {Proceedings of the 16th Annual Conference on Learning Theory and the 7th Kernel Workshop, COLT-Kernel 2003},
	Editor = {Sch{\"{o}}lkopf, B and Warmuth, M},
	Pages = {26--40},
	Publisher = {Springer},
	Title = {{Efficient algorithms for the online decision problem}},
	Year = {2003}}

@inproceedings{guillou2015collaborative,
	Abstract = {Recommender Systems (RS) aim at suggesting to users one or several items in which they might have interest. Following the feedback they receive from the user, these systems have to adapt their model in order to improve future recommendations. The repetition of these steps defines the RS as a sequential process. This sequential aspect raises an exploration-exploitation dilemma, which is surprisingly rarely taken into account for RS without contextual information. In this paper we present an explore-exploit collaborative filtering RS, based on Matrix Factor-ization and Bandits algorithms. Using experiments on artificial and real datasets, we show the importance and practicability of using sequential approaches to perform recommendation. We also study the impact of the model update on both the quality and the computation time of the recommendation procedure.},
	Author = {Guillou, Fr{\'{e}}d{\'{e}}ric and Gaudel, Romaric and Preux, Philippe},
	Booktitle = {NIPS Workshop on Machine Learning for eCommerce},
	Keywords = {Collaborative Filtering,Matrix Factorization,Multi-Armed Bandtis,Recommender Systems,sequential Recommender Systems},
	Month = {dec},
	Title = {{Collaborative filtering as a multi-armed bandit}},
	Year = {2015}}

@techreport{krause2005note,
	Author = {Krause, Andreas and Guestrin, Carlos},
	Booktitle = {Technical Rep No CMUCALD},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Krause, Guestrin - 2005 - A Note on the Budgeted Maximization of Submodular Functions.pdf:pdf},
	Institution = {CMU},
	Keywords = {constraints,entropy maximization,optimization,submodular functions},
	Number = {June},
	Pages = {1--7},
	Publisher = {Citeseer},
	Title = {{A Note on the Budgeted Maximization of Submodular Functions}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.9721{\&}rep=rep1{\&}type=pdf},
	Volume = {5},
	Year = {2005},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.9721%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{mahadevan2006learning,
	Author = {Mahadevan, Sridhar and Maggioni, Mauro and Ferguson, Kimberly and Osentoski, Sarah},
	Booktitle = {Proceedings of the 21st National Conference on Artificial Intelligence},
	Title = {{Learning Representation and Control in Continuous {\{}Markov{\}} Decision Processes}},
	Year = {2006}}

@article{rusmevichientong2010linearly,
	Address = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
	Author = {Rusmevichientong, Paat and Tsitsiklis, John N},
	Journal = {Math. Oper. Res.},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Month = {may},
	Number = {2},
	Pages = {395--411},
	Publisher = {Informs},
	Title = {{Linearly Parameterized Bandits}},
	Volume = {35},
	Year = {2010}}

@inproceedings{valko2014spectral,
	Abstract = {Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this paper, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each item we can recommend is a node and its expected rating is similar to its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret with respect to the optimal policy would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose two algorithms for solving our problem that scale linearly and sublinearly in this dimension. Our experiments on real-world content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens of nodes evaluations.},
	Author = {Valko, Michal and Munos, R{\'{e}}mi and Kveton, Branislav and Koc{\'{a}}k, Tom{\'{a}}{\v{s}}},
	Booktitle = {International Conference on Machine Learning},
	Title = {{Spectral bandits for smooth graph functions}},
	Url = {http://proceedings.mlr.press/v32/valko14.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v32/valko14.pdf}}

@inproceedings{alaoui2014fast,
	Abstract = {One approach to improving the running time of kernel-based machine learning methods is to build a small sketch of the input and use it in lieu of the full kernel matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance. By extending the notion of $\backslash$emph{\{}statistical leverage scores{\}} to the setting of kernel ridge regression, our main statistical result is to identify an importance sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the $\backslash$emph{\{}effective dimensionality{\}} of the problem. This quantity is often much smaller than previous bounds that depend on the $\backslash$emph{\{}maximal degrees of freedom{\}}. Our main algorithmic result is to present a fast algorithm to compute approximations to these scores. This algorithm runs in time that is linear in the number of samples---more precisely, the running time is {\$}O(np{\^{}}2){\$}, where the parameter {\$}p{\$} depends only on the trace of the kernel matrix and the regularization parameter---and it can be applied to the matrix of feature vectors, without having to form the full kernel matrix. This is obtained via a variant of length-squared sampling that we adapt to the kernel setting in a way that is of independent interest. Lastly, we provide empirical results illustrating our theory, and we discuss how this new notion of the statistical leverage of a data point captures in a fine way the difficulty of the original statistical learning problem.},
	Author = {Alaoui, Ahmed El and Mahoney, Michael W.},
	Booktitle = {Neural Information Processing Systems},
	Title = {{Fast randomized kernel methods with statistical guarantees}},
	Year = {2015}}

@article{donoho,
	Author = {Donoho, D L and Johnstone, I M},
	Journal = {Journal of the American Statistical Association},
	Number = {432},
	Pages = {1200--1224},
	Publisher = {American Statistical Association},
	Title = {{Adapting to Unknown Smoothness Via Wavelet Shrinkage.}},
	Volume = {90},
	Year = {1995}}

@article{smola2004tutorial,
	Author = {Smola, Alex and Sch{\"{o}}lkopf, Bernhard},
	Journal = {Statistics and Computing},
	Number = {3},
	Pages = {199--222},
	Title = {{A Tutorial on Support Vector Regression}},
	Volume = {14},
	Year = {2004}}

@inproceedings{leskovec2005graphs,
	Author = {Leskovec, Jure and Kleinberg, Jon and Faloutsos, Christos},
	Booktitle = {Proceedings of KDD'05},
	Organization = {ACM},
	Pages = {177--187},
	Title = {{Graphs over time: densification laws, shrinking diameters and possible explanations}},
	Year = {2005}}

@article{kocak2018spectral,
	Author = {Koc{\'{a}}k, Tom{\'{a}}{\v{s}} and Munos, R{\'{e}}mi and Kveton, Branislav and Agrawal, Shipra and Valko, Michal},
	Journal = {Journal of Machine Learning Research},
	Title = {{Spectral Bandits}},
	Year = {2018}}

@book{tsybakov2009introduction,
	Address = {New York, NY},
	Author = {Tsybakov, Alexandre B.},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Tsybakov - 2009 - Introduction to Nonparametric Estimation.pdf:pdf},
	Publisher = {Springer New York},
	Series = {Springer Series in Statistics},
	Title = {{Introduction to Nonparametric Estimation}},
	Year = {2009}}

@article{post2008temporal,
	Abstract = {Large-scale clinical databases provide a detailed perspective on patient
phenotype in disease and the characteristics of health care processes.
Important information is often contained in the relationships between
the values and timestamps of sequences of clinical data. The analysis
of clinical time sequence data across entire patient populations
may reveal data patterns that enable a more precise understanding
of disease presentation, progression, and response to therapy, and
thus could be of great value for clinical and translational research.
Recent work suggests that the combination of temporal data mining
methods with techniques from artificial intelligence research on
knowledge-based temporal abstraction may enable the mining of clinically
relevant temporal features from these previously problematic general
clinical data.},
	Author = {Post, Andrew R and Harrison, James H},
	Doi = {10.1016/j.cll.2007.10.005},
	Institution = {Division of Clinical Informatics, Department of Public Health Sciences, University of Virginia, Suite 3181 West Complex, 1335 Hospital Drive, Charlottesville, VA 22908-0717, USA. arp4m@virginia.edu},
	Journal = {Clin Lab Med},
	Keywords = {Algorithms; Artificial Intelligence; Databases as,Automated; Software; Time Factors,methods; Pattern Recognition},
	Month = {mar},
	Number = {1},
	Pages = {83----100, vii},
	Pmid = {18194720},
	Title = {{Temporal data mining.}},
	Url = {http://dx.doi.org/10.1016/j.cll.2007.10.005},
	Volume = {28},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.cll.2007.10.005}}

@article{munos2002variable,
	Author = {Munos, Remi and Moore, Andrew},
	Journal = {Machine Learning},
	Pages = {291--323},
	Title = {{Variable Resolution Discretization in Optimal Control}},
	Volume = {49},
	Year = {2002}}

@article{nino-nora2010computing,
	Author = {Nino-Mora, J},
	Doi = {10.1287/ijoc.1100.0398},
	Issn = {10919856},
	Journal = {INFORMS Journal on Computing},
	Keywords = {accepted may 2010,accepted winfried grassmann,advance,analysis algorithms,area editor computational,bandits,computational complexity,dynamic programming,finite horizon,history,index policies,march 2009,markov,may 2010,probability analysis,published online articles,received,revised january 2010},
	Number = {2},
	Pages = {254--267},
	Title = {{Computing a Classic Index for Finite-Horizon Bandits}},
	Url = {http://joc.journal.informs.org/cgi/doi/10.1287/ijoc.1100.0398},
	Volume = {23},
	Year = {2010},
	Bdsk-Url-1 = {http://joc.journal.informs.org/cgi/doi/10.1287/ijoc.1100.0398},
	Bdsk-Url-2 = {https://doi.org/10.1287/ijoc.1100.0398}}

@article{bach2011learning,
	Archiveprefix = {arXiv},
	Arxivid = {1111.6453},
	Author = {Bach, Francis},
	Eprint = {1111.6453},
	Keywords = {Learning,Optimization and Control},
	Title = {{Learning with Submodular Functions: A Convex Optimization Perspective}},
	Url = {http://arxiv.org/abs/1111.6453},
	Year = {2011},
	Bdsk-Url-1 = {http://arxiv.org/abs/1111.6453}}

@inproceedings{Mairal2010a,
	Author = {Mairal, J and Jenatton, R and Obozinski, G and Bach, F},
	Booktitle = {Adv. NIPS},
	Number = {00512556},
	Organization = {HAL INRIA},
	Title = {{Network Flow Algorithms for Structured Sparsity}},
	Year = {2010}}

@article{BMSS11,
	Author = {Bubeck, S and Munos, R and Stoltz, G and Szepesvari, C},
	Journal = {Journal of Machine Learning Research},
	Pages = {1587--1627},
	Title = {{$\backslash$mathcal{\{}X{\}}-Armed Bandits}},
	Volume = {12},
	Year = {2011}}

@article{HaSe07,
	Author = {Hazan, E and Seshadhri, C},
	Journal = {Electronic Colloquium on Computational Complexity (ECCC)},
	Title = {{Adaptive algorithms for online decision problems}},
	Year = {2007}}

@phdthesis{sondik1971optimal,
	Author = {Sondik, Edward},
	School = {Stanford University},
	Title = {{The Optimal Control of Partially Observable {\{}Markov{\}} Decision Processes}},
	Year = {1971}}

@article{gine2010confidence,
	Author = {Gin{\'{e}}, Evarist and Nickl, Richard},
	Journal = {The Annals of Statistics},
	Number = {2},
	Pages = {1122--1170},
	Title = {{Confidence bands in density estimation}},
	Volume = {38},
	Year = {2010}}

@article{Nem79,
	Annote = {(In Russian)},
	Author = {Nemirovski, A},
	Journal = {Ekonomika i Matematicheskie Metody},
	Title = {{Efficient methods for large-scale convex optimization problems}},
	Volume = {15},
	Year = {1979}}

@article{belkin2006manifold,
	Author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
	Journal = {Journal of Machine Learning Research},
	Pages = {2399--2434},
	Title = {{Manifold regularization: A geometric framework for learning from labeled and unlabeled examples}},
	Url = {http://www.jmlr.org/papers/volume7/belkin06a/belkin06a.pdf},
	Volume = {7},
	Year = {2006},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume7/belkin06a/belkin06a.pdf}}

@inproceedings{syed2010unsupervised,
	Author = {Syed, Zeeshan and Rubinfeld, Ilan},
	Booktitle = {ICML},
	Editor = {F{\"{u}}rnkranz, Johannes and Joachims, Thorsten},
	Pages = {1023--1030},
	Publisher = {Omnipress},
	Title = {{Unsupervised Risk Stratification in Clinical Datasets: Identifying Patients at Risk of Rare Outcomes}},
	Year = {2010}}

@inproceedings{sutton2000policy,
	Author = {Sutton, Richard and McAllester, David and Singh, Satinder and Mansour, Yishay},
	Booktitle = {Advances in Neural Information Processing Systems 12},
	Pages = {1057--1063},
	Title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
	Year = {2000}}

@inproceedings{Kathuria2016,
	Abstract = {Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function requires training a model which may involve days or even weeks of computation. Most methods for this so-called "Bayesian optimization" only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.},
	Archiveprefix = {arXiv},
	Arxivid = {1611.04088},
	Author = {Kathuria, Tarun and Deshpande, Amit and Kohli, Pushmeet},
	Booktitle = {Neural Information Processing Systems},
	Eprint = {1611.04088},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Kathuria, Deshpande, Kohli - 2016 - Batched Gaussian Process Bandit Optimization via Determinantal Point Processes(2).pdf:pdf},
	Month = {nov},
	Title = {{Batched Gaussian Process Bandit Optimization via Determinantal Point Processes}},
	Url = {http://arxiv.org/abs/1611.04088},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1611.04088}}

@article{tropp2011freedman,
	Author = {Tropp, Joel Aaron},
	Journal = {Electronic Communications in Probability},
	Pages = {262--270},
	Title = {{Freedman's inequality for matrix martingales}},
	Volume = {16},
	Year = {2011}}

@book{Cao07,
	Address = {New York},
	Author = {Cao, Xi-Ren},
	Publisher = {Springer},
	Title = {{Stochastic Learning and Optimization: A Sensitivity-Based Approach}},
	Year = {2007}}

@article{Fokkink2016,
	Abstract = {Suppose some objects are hidden in a finite set {\$}S{\$} of hiding places which must be examined one-by-one. The cost of searching subsets of {\$}S{\$} is given by a submodular function and that the probability that all objects are contained in a subset is given by a supermodular function. We seek an ordering of S that finds all the objects in minimal expected cost. This problem is NP-hard and we give an efficient combinatorial {\$}2{\$}-approximation algorithm, generalizing analogous results in scheduling theory. We also give a new scheduling application {\$}1|prec|\backslashbackslashsum w{\_}A g(C{\_}A){\$}, where a set of jobs must be ordered subject to precedence constraints to minimize the weighted sum of some concave function {\$}g{\$} of the completion times of subsets of jobs. We go on to give better approximations for submodular functions with low total curvature and we give a full solution for cost functions we call fully reducible. Next, we consider a zero-sum game between a cost-maximizing Hider and a cost-minimizing Searcher. We prove that the equilibrium mixed strategies for the Hider are in the base polyhedron of the cost function, suitably scaled, and we solve the game for fully reducible cost functions, giving approximately optimal strategies in other cases.},
	Archiveprefix = {arXiv},
	Arxivid = {1607.07598},
	Author = {Fokkink, Robbert and Lidbetter, Thomas and V{\'{e}}gh, L{\'{a}}szl{\'{o}} A},
	Eprint = {1607.07598},
	Journal = {arXiv preprint},
	Title = {{On submodular search and machine scheduling}},
	Url = {http://arxiv.org/abs/1607.07598},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1607.07598}}

@article{lo2006evaluation,
	Abstract = {BACKGROUND: Heparin-induced thrombocytopenia (HIT) is a prothrombotic
adverse drug reaction caused by heparin. As thrombocytopenia is common
in hospitalized patients receiving heparin, it would be useful to
have a clinical scoring system that could differentiate patients
with HIT from those with other reasons for thrombocytopenia. AIM:
To compare prospectively the diagnostic utility of a clinical score
for HIT in two different clinical settings. METHODS: The pretest
clinical scoring system, the '4 T's', was used to classify 100 consecutive
patients referred for possible HIT in one hospital (Hamilton General
Hospital, HGH) into high, intermediate, and low probability groups.
This system was also used to classify likewise 236 patients by clinicians
in Germany referring blood for diagnostic testing for HIT in Greifswald
(GW). The clinical scores were correlated with the results of laboratory
testing for HIT antibodies using the serologic criteria for HIT with
high diagnostic specificity. RESULTS: In both centers, patients with
low scores were unlikely to test positive for HIT antibodies [HGH:
1/64 (1.6{\%}), GW: 0/55 (0{\%})]. Patients with intermediate [HGH: 8/28
(28.6{\%}), GW: 11/139 (7.9{\%})] or high scores [HGH: 8/8 (100{\%}), GW:
9/42 (21.4{\%})] were more likely to test positive for clinically significant
HIT antibodies. The positive predictive value of an intermediate
or high clinical score for clinically significant HIT antibodies
was higher at one center (HGH). CONCLUSIONS: A low pretest clinical
score for HIT seems to be suitable for ruling out HIT in most situations
(high-negative predictive value). The implications of an intermediate
or high score vary in different clinical settings.},
	Author = {Lo, G K and Juhl, D and Warkentin, T E and Sigouin, C S and Eichler, P and Greinacher, A},
	Doi = {10.1111/j.1538-7836.2006.01787.x},
	Institution = {Department of Medicine, McMaster University, Hamilton, ON, Canada.},
	Journal = {J Thromb Haemost},
	Keywords = {Adult; Aged; Algorithms; Enzyme-Linked Immunosorbe},
	Month = {apr},
	Number = {4},
	Pages = {759--765},
	Pmid = {16634744},
	Title = {{Evaluation of pretest clinical score (4 T's) for the diagnosis of heparin-induced thrombocytopenia in two clinical settings.}},
	Url = {http://dx.doi.org/10.1111/j.1538-7836.2006.01787.x},
	Volume = {4},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1111/j.1538-7836.2006.01787.x}}

@article{wolfe1976finding,
	Author = {Wolfe, P},
	Journal = {Math. Progr.},
	Number = {1},
	Pages = {128--149},
	Publisher = {Springer},
	Title = {{Finding the nearest point in a polytope}},
	Volume = {11},
	Year = {1976}}

@book{jeffreys1988methods,
	Address = {Cambridge, United Kingdom},
	Author = {Jeffreys, Harold and Jeffreys, Bertha},
	Publisher = {Cambridge University Press},
	Title = {{Methods of Mathematical Physics}},
	Year = {1988}}

@inproceedings{klein2012structured,
	Abstract = {TBD},
	Address = {Edinburgh (UK)},
	Author = {Klein, Edouard and PIOT, Bilal and Geist, Matthieu and Pietquin, Olivier},
	Booktitle = {European Workshop on Reinforcement Learning (EWRL 2012)},
	Month = {jun},
	Title = {{Structured Classification for Inverse Reinforcement Learning}},
	Url = {http://ewrl.files.wordpress.com/2011/12/ewrl2012{\_}submission{\_}30.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://ewrl.files.wordpress.com/2011/12/ewrl2012%7B%5C_%7Dsubmission%7B%5C_%7D30.pdf}}

@article{Langford2007,
	Abstract = {We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also known as bandits with side information). Epoch-Greedy has the following properties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. 3. The regret scales as O(T 2/3 S 1/3) or better (sometimes, much better). Here S is the complexity term in a sample complexity bound for standard supervised learning. 1},
	Author = {Langford, John},
	Editor = {Platt, J C and Koller, D and Singer, Y and Roweis, S},
	Journal = {Statistics},
	Pages = {1--8},
	Publisher = {Citeseer},
	Title = {{The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits}},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.8000{\&}rep=rep1{\&}type=pdf},
	Volume = {20},
	Year = {2007},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.8000%7B%5C&%7Drep=rep1%7B%5C&%7Dtype=pdf}}

@inproceedings{dhiman2006dynamic,
	Author = {Dhiman, Gaurav and Simunic, Tajana},
	Booktitle = {Proceedings of the 2006 IEEE / ACM International Conference on Computer-Aided Design},
	Title = {{Dynamic Power Management Using Machine Learning}},
	Year = {2006}}

@article{buntine1996guide,
	Address = {Thinkbank, 1678 Shattuck Ave, Suite 320, Berkeley, Ca, 94709},
	Author = {Buntine, W},
	Journal = {IEEE Transactions on Knowledge and Data Engineering},
	Pages = {195--210},
	Title = {{A Guide to the Literature on Learning Probabilistic Networks from Data}},
	Url = {citeseer.nj.nec.com/buntine96guide.html},
	Volume = {8},
	Year = {1996},
	Bdsk-Url-1 = {citeseer.nj.nec.com/buntine96guide.html}}

@inproceedings{minka2001expectation,
	Abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
	Author = {Minka, Tom},
	Booktitle = {Uncertainty in Artificial Intelligence},
	Title = {{Expectation oropagation for approximate Bayesian inference}},
	Year = {2001}}

@article{neal2000markov,
	Abstract = {This article reviews Markov chain methods for sampling from the posterior
distribution of a Dirichlet process mixture model and presents two
new classes of methods. One new approach is to make Metropolis-Hastings
updates of the indicators specifying which mixture component is associated
with each observation, perhaps supplemented with a partial form of
Gibbs sampling. The other new approach extends Gibbs sampling for
these indicators by using a set of auxiliary parameters. These methods
are simple to implement and are more efficient than previous ways
of handling general Dirichlet process mixture models with non-conjugate
priors.},
	Annote = {c{\_}omps{\_}models},
	Author = {Neal, Radford M},
	Journal = {Journal of Computational and Graphical Statistics},
	Keywords = {dirichlet,dp,gibbs-sampling,mcmc},
	Number = {2},
	Pages = {249--265},
	Title = {{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}},
	Url = {http://www.jstor.org/stable/1390653},
	Volume = {9},
	Year = {2000},
	Bdsk-Url-1 = {http://www.jstor.org/stable/1390653}}

@article{weissman03ineq,
	Author = {Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
	Institution = {Technical Report, HP Laboratories},
	Title = {{Inequalities for the L1 Deviation of the Empirical Distribution}},
	Year = {2003}}

@techreport{Warmuth97continuousand,
	Author = {Warmuth, M K and Jagota, A K},
	Institution = {In Fifth International Symposium on Artificial Intelligence and Mathematics},
	Title = {{Continuous and discrete-time nonlinear gradient descent: Relative loss bounds and convergence}},
	Year = {1997}}

@article{queyranne1995scheduling,
	Author = {Queyranne, M and Schulz, A},
	Journal = {Integer Programming and Combinatorial Optimization},
	Pages = {307--320},
	Publisher = {Springer},
	Title = {{Scheduling unit jobs with compatible release dates on parallel machines with nonstationary speeds}},
	Volume = {920},
	Year = {1995}}

@article{kempe2003maximizing,
	Abstract = {Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of ``word of mouth'' in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target? We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63{\%} of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks. We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform nodeselection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks.},
	Author = {Kempe, David and Kleinberg, Jon and Tardos, {\'{E}}va},
	Journal = {Knowledge Discovery and Data Mining},
	Pages = {137},
	Title = {{Maximizing the spread of influence through a social network}},
	Year = {2003}}

@inproceedings{Flaounas2011,
	Author = {Flaounas, I and Ali, O and Turchi, M and Snowsill, T and Nicart, F and {De Bie}, T and Cristianini, N},
	Booktitle = {Proceedings of the 2011 ACM SIGMOD international conference on Management of data},
	Pages = {1275--1278},
	Publisher = {ACM},
	Title = {{NOAM: News Outlets Analysis and Monitoring System}},
	Year = {2011}}

@article{kolmogorov2004energy,
	Author = {Kolmogorov, V and Zabih, R},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Number = {2},
	Pages = {147--159},
	Publisher = {Published by the IEEE Computer Society},
	Title = {{What energy functions can be minimized via graph cuts?}},
	Volume = {26},
	Year = {2004}}

@inproceedings{achlioptas2013near,
	Author = {Achlioptas, Dimitris and Karnin, Zohar S. and Liberty, Edo},
	Booktitle = {Neural Information Processing Systems},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Achlioptas, Karnin, Liberty - 2013 - Near-Optimal Entrywise Sampling for Data Matrices.pdf:pdf},
	Title = {{Near-optimal entrywise sampling for data matrices}},
	Year = {2013}}

@inproceedings{hillel2007learning,
	Address = {New York, NY, USA},
	Annote = {comps{\_}distance},
	Author = {Hillel, Aharon Bar and Weinshall, Daphna},
	Booktitle = {ICML '07: Proceedings of the 24th international conference on Machine learning},
	Doi = {http://doi.acm.org/10.1145/1273496.1273505},
	Isbn = {978-1-59593-793-3},
	Pages = {65--72},
	Publisher = {ACM},
	Title = {{Learning distance function by coding similarity}},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1273496.1273505}}

@inproceedings{kalyanakrishnan2012pac,
	Author = {Kalyanakrishnan, Shivaram and Tewari, Ambuj and Auer, Peter and Stone, Peter},
	Booktitle = {International Conference on Machine Learning},
	Title = {{PAC subset selection in stochastic multi-armed bandits}},
	Year = {2012}}

@book{ahuja1993network,
	Author = {Ahuja, R K and Magnanti, T L and Orlin, J B},
	Publisher = {Prentice hall},
	Title = {{Network flows: theory, algorithms, and applications}},
	Year = {1993}}

@article{shiraev03,
	Author = {Shiraev, Dmitry Eric},
	Institution = {VT Electronic Thesis and Dissertation Archive [http://scholar.lib.vt.edu/theses/OAI2/] (United States)},
	Keywords = {Computer Science},
	Publisher = {VT},
	Title = {{Inverse Reinforcement Learning and Routing Metric Discovery}},
	Url = {http://scholar.lib.vt.edu/theses/available/etd-08242003-224906/},
	Year = {2003},
	Bdsk-Url-1 = {http://scholar.lib.vt.edu/theses/available/etd-08242003-224906/}}

@inproceedings{kujala07perturbed,
	Author = {Kujala, Jussi and Elomaa, Tapio},
	Doi = {http://dx.doi.org/10.1007/978-3-540-75225-7_16},
	Pages = {166--180},
	Title = {{Following the Perturbed Leader to Gamble at Multi-armed Bandits}},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-540-75225-7_16}}

@article{baxter2001infinite-horizon,
	Author = {Baxter, Jonathan and Bartlett, Peter and Weaver, Lex},
	Journal = {Journal of Artificial Intelligence Research},
	Pages = {319--350},
	Title = {{Infinite-Horizon Policy-Gradient Estimation}},
	Volume = {15},
	Year = {2001}}

@inproceedings{li2017hyperband,
	Author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Talwalkar, Afshin Rostamizadeh Ameet},
	Booktitle = {International Conference on Learning Representations},
	Title = {{Hyperband: Bandit-based configuration evaluation for hyperparameter optimization}},
	Url = {https://openreview.net/pdf?id=ry18Ww5ee},
	Year = {2017},
	Bdsk-Url-1 = {https://openreview.net/pdf?id=ry18Ww5ee}}

@article{mannor2004sample,
	Author = {Mannor, S and Tsitsiklis, J N},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Mendeley-Tags = {bandits},
	Pages = {623--648},
	Title = {{The Sample Complexity of Exploration in the Multi-Armed Bandit Problem}},
	Volume = {5},
	Year = {2004}}

@inproceedings{kolla2016collaborative,
	Abstract = {We consider a collaborative online learning paradigm, wherein a group of agents connected through a social network are engaged in playing a stochastic multi-armed bandit game. Each time an agent takes an action, the corresponding reward is instantaneously observed by the agent, as well as its neighbours in the social network. We perform a regret analysis of various policies in this collaborative learning setting. A key finding of this paper is that natural extensions of widely-studied single agent learning policies to the network setting need not perform well in terms of regret. In particular, we identify a class of non-altruistic and individually consistent policies, and argue by deriving regret lower bounds that they are liable to suffer a large regret in the networked setting. We also show that the learning performance can be substantially improved if the agents exploit the structure of the network, and develop a simple learning algorithm based on dominating sets of the network. Specifically, we first consider a star network, which is a common motif in hierarchical social networks, and show analytically that the hub agent can be used as an information sink to expedite learning and improve the overall regret. We also derive networkwide regret bounds for the algorithm applied to general networks. We conduct numerical experiments on a variety of networks to corroborate our analytical results.},
	Author = {Kolla, Ravi Kumar and Jagannathan, Krishna and Gopalan, Aditya},
	Booktitle = {Annual Allerton Conference on Communication, Control, and Computing},
	File = {:Users/miki/Library/Application Support/Mendeley Desktop/Downloaded/Kolla, Jagannathan, Gopalan - 2016 - Collaborative learning of stochastic bandits over a social etwork.pdf:pdf},
	Title = {{Collaborative learning of stochastic bandits over a social network}},
	Year = {2016}}

@incollection{choi2011map,
	Author = {Choi, Jaedeug and Kim, Kee-Eung},
	Booktitle = {Advances in Neural Information Processing Systems 24},
	Editor = {Shawe-Taylor, J and Zemel, R S and Bartlett, P and Pereira, F C N and Weinberger, K Q},
	Pages = {1989--1997},
	Title = {{MAP Inference for Bayesian Inverse Reinforcement Learning}},
	Year = {2011}}

@inproceedings{kaufmann2013information,
	Author = {Kaufmann, Emilie and Kalyanakrishnan, Shivaram},
	Booktitle = {Conference on Learning Theory},
	Title = {{Information complexity in bandit subset selection}},
	Year = {2013}}
