@book{chow1997probability,
abstract = {3rd ed. Classes of sets, measures, and probability spaces -- Binomial random variables -- Independence -- Integration in a probability space -- Sums of independent random variables -- Measure extensions, Lebesgue-Stieltjes measure, Kolmogorov consistency theorem -- Conditional expectation, conditional independence, introduction to martingales -- Distribution functions and characteristic functions -- Central limit theorems -- Limit theorems for independent random variables -- Martingales -- Infinitely divisible laws.},
author = {Chow, Yuan Shih and Teicher, Henry.},
isbn = {9780387982281},
pages = {488},
publisher = {Springer},
title = {{Probability theory : independence, interchangeability, martingales}},
url = {https://www.springer.com/gp/book/9780387982281},
year = {1997}
}
@article{lai1985asymptotically,
author = {Lai, T. L. and Robbins, Herbert},
doi = {10.1016/0196-8858(85)90002-8},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Lai, Robbins - 1985 - Asymptotically efficient adaptive allocation rules.pdf:pdf},
issn = {10902074},
journal = {Advances in Applied Mathematics},
month = {mar},
number = {1},
pages = {4--22},
publisher = {Academic Press},
title = {{Asymptotically efficient adaptive allocation rules}},
url = {https://www.sciencedirect.com/science/article/pii/0196885885900028},
volume = {6},
year = {1985}
}
@article{auer2002nonstochastic,
abstract = {In the multiarmed bandit problem, a gambler must decide which arm of K non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines. In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the per-round payoff of our algorithm approaches that of the best arm at the rate O(T-1/2). We show by a matching lower bound that this is the best possible. We also prove that our algorithm approaches the per-round payoff of any set of strategies at a similar rate: if the best strategy is chosen from a pool of N strategies, then our algorithm approaches the per-round payoff of the strategy at the rate O((log N)1/2T-1/2). Finally, we apply our results to the problem of playing an unknown repeated matrix game. We show that our algorithm approaches the minimax payoff of the unknown game at the rate O(T-1/2).},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E.},
doi = {10.1137/S0097539701398375},
isbn = {10957111},
issn = {00975397},
journal = {SIAM Journal on Computing},
keywords = {Adversarial bandit problem,Unknown matrix games},
month = {jan},
number = {1},
pages = {48--77},
publisher = {Society for Industrial and Applied Mathematics},
title = {{The nonstochastic multiarmed bandit problem}},
volume = {32},
year = {2003}
}
@book{lattimore2020banditbook,
abstract = {Decision-making in the face of uncertainty is a significant challenge in machine learning, and the multi-armed bandit model is a commonly used framework to address it. This comprehensive and rigorous introduction to the multi-armed bandit problem examines all the major settings, including stochastic, adversarial, and Bayesian frameworks. A focus on both mathematical intuition and carefully worked proofs makes this an excellent reference for established researchers and a helpful resource for graduate students in computer science, engineering, statistics, applied mathematics and economics. Linear bandits receive special attention as one of the most useful models in applications, while other chapters are dedicated to combinatorial bandits, ranking, non-stationary problems, Thompson sampling and pure exploration. The book ends with a peek into the world beyond bandits with an introduction to partial monitoring and learning in Markov decision processes.},
author = {Lattimore, Tor and Szepesv{\'{a}}ri, Csaba},
isbn = {1108486827},
month = {jun},
publisher = {Cambridge University Press UK},
title = {{Bandit Algorithms}},
url = {https://tor-lattimore.com/downloads/book/book.pdf},
year = {2020}
}
@inproceedings{bifet2007learning,
abstract = {We present a new approach for dealing with distribution change and concept drift when learning from data sequences that may vary with time. We use sliding windows whose size, instead of being fixed a priori, is recomputed online according to the rate of change observed from the data in the window itself. This delivers the user or programmer from having to guess a time-scale for change. Contrary to many related works, we provide rigorous guarantees of performance, as bounds on the rates of false positives and false negatives. Using ideas from data stream algorithmics, we develop a time-and memory-efficient version of this algorithm, called ADWIN2. We show how to combine ADWIN2 with the Na{\"{i}}ve Bayes (NB) predictor, in two ways: one, using it to monitor the error rate of the current model and declare when revision is necessary and, two, putting it inside the NB predictor to maintain up-to-date estimations of conditional probabilities in the data. We test our approach using synthetic and real data streams and compare them to both fixed-size and variable-size window strategies with good results.},
author = {Bifet, Albert and Gavald{\`{a}}, Ricard},
booktitle = {Proceedings of the 7th SIAM International Conference on Data Mining},
doi = {10.1137/1.9781611972771.42},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Bifet, Gavald{\`{a}} - 2007 - Learning from time-changing data with adaptive windowing.pdf:pdf},
isbn = {9780898716306},
keywords = {Concept and distribution drift,Data streams,Na{\"{i}}ve bayes,Time-changing data},
pages = {443--448},
title = {{Learning from time-changing data with adaptive windowing}},
year = {2007}
}
@inproceedings{audibert2009minimax,
abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit problem. Concretely, we remove an extraneous logarithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays.},
author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
booktitle = {Proceedings of the Conference on Learning Theory (COLT), 2009},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Audibert, Bubeck - 2009 - Minimax policies for adversarial and stochastic bandits.pdf:pdf},
keywords = {()},
pages = {217--226},
title = {{Minimax policies for adversarial and stochastic bandits}},
url = {https://hal-enpc.archives-ouvertes.fr/hal-00834882},
year = {2009}
}
@article{besson2018doubling,
abstract = {An online reinforcement learning algorithm is anytime if it does not need to know in advance the horizon T of the experiment. A well-known technique to obtain an anytime algorithm from any non-anytime algorithm is the "Doubling Trick". In the context of adversarial or stochastic multi-armed bandits, the performance of an algorithm is measured by its regret, and we study two families of sequences of growing horizons (geometric and exponential) to generalize previously known results that certain doubling tricks can be used to conserve certain regret bounds. In a broad setting, we prove that a geometric doubling trick can be used to conserve (minimax) bounds in {\$}R\backslash{\_}T = O(\backslashsqrt{\{}T{\}}){\$} but cannot conserve (distribution-dependent) bounds in {\$}R\backslash{\_}T = O(\backslashlog T){\$}. We give insights as to why exponential doubling tricks may be better, as they conserve bounds in {\$}R\backslash{\_}T = O(\backslashlog T){\$}, and are close to conserving bounds in {\$}R\backslash{\_}T = O(\backslashsqrt{\{}T{\}}){\$}.},
archivePrefix = {arXiv},
arxivId = {1803.06971},
author = {Besson, Lilian and Kaufmann, Emilie},
eprint = {1803.06971},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Besson, Kaufmann - 2018 - What Doubling Tricks Can and Can't Do for Multi-Armed Bandits.pdf:pdf},
month = {mar},
title = {{What Doubling Tricks Can and Can't Do for Multi-Armed Bandits}},
url = {http://arxiv.org/abs/1803.06971},
year = {2018}
}
@inproceedings{garivier2011upper-confidence-bound,
abstract = {Many problems, such as cognitive radio, parameter control of a scanning tunnelling microscope or internet advertisement, can be modelled as non-stationary bandit problems where the distributions of rewards changes abruptly at unknown time instants. In this paper, we analyze two algorithms designed for solving this issue: discounted UCB (D-UCB) and sliding-window UCB (SW-UCB). We establish an upper-bound for the expected regret by upper-bounding the expectation of the number of times suboptimal arms are played. The proof relies on an interesting Hoeffding type inequality for self normalized deviations with a random number of summands. We establish a lower-bound for the regret in presence of abrupt changes in the arms reward distributions. We show that the discounted UCB and the sliding-window UCB both match the lower-bound up to a logarithmic factor. Numerical simulations show that D-UCB and SW-UCB perform significantly better than existing soft-max methods like EXP3.S. {\textcopyright} 2011 Springer-Verlag.},
author = {Garivier, Aur{\'{e}}lien and Moulines, Eric},
booktitle = {Proceedings of the 22nd International Conference on Algorithmic Learning Theory (ALT), 2011, Espoo, Finland.},
doi = {10.1007/978-3-642-24412-4_16},
isbn = {9783642244117},
issn = {03029743},
pages = {174--188},
publisher = {Springer, Berlin, Heidelberg},
title = {{On upper-confidence bound policies for switching bandit problems}},
volume = {6925 LNAI},
year = {2011}
}
@inproceedings{levine2017rotting,
abstract = {The Multi-Armed Bandits (MAB) framework highlights the trade-off between acquiring new knowledge (Exploration) and leveraging available knowledge (Ex-ploitation). In the classical MAB problem, a decision maker must choose an arm at each time step, upon which she receives a reward. The decision maker's objective is to maximize her cumulative expected reward over the time horizon. The MAB problem has been studied extensively, specifically under the assumption of the arms' rewards distributions being stationary, or quasi-stationary, over time. We consider a variant of the MAB framework, which we termed Rotting Bandits, where each arm's expected reward decays as a function of the number of times it has been pulled. We are motivated by many real-world scenarios such as online advertising , content recommendation, crowdsourcing, and more. We present algorithms, accompanied by simulations, and derive theoretical guarantees.},
archivePrefix = {arXiv},
arxivId = {1702.07274},
author = {Levine, Nir and Crammer, Koby and Mannor, Shie},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
eprint = {1702.07274},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Levine, Crammer, Mannor - 2017 - Rotting Bandits.pdf:pdf},
pages = {3074--3083},
title = {{Rotting Bandits}},
year = {2017}
}
@article{heidari2016tight,
abstract = {We consider a variant of the well-studied multi-armed bandit problem in which the reward from each action evolves monotonically in the number of times the decision maker chooses to take that action. We are motivated by settings in which we must give a series of homogeneous tasks to a finite set of arms (workers) whose performance may improve (due to learning) or decay (due to loss of interest) with repeated trials. We assume that the arm-dependent rates at which the rewards change are unknown to the decision maker, and propose algorithms with provably optimal policy regret bounds, a much stronger notion than the often-studied external regret. For the case where the rewards are increasing and concave, we give an algorithm whose policy regret is sublinear and has a (provably necessary) dependence on the time required to distinguish the optimal arm from the rest. We illustrate the behavior and performance of this algorithm via simulations. For the decreasing case, we present a simple greedy approach and show that the policy regret of this algorithm is constant and upper bounded by the number of arms.},
author = {Heidari, Hoda and Kearns, Michael and Roth, Aaron},
journal = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
pages = {1562--1570},
title = {{Tight Policy Regret Bounds for Improving and Decaying Bandits}},
year = {2016}
}
@article{auer2002finite,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Fischer, Paul},
doi = {10.1023/A:1013689704352},
file = {:Users/julien/Library/Application Support/Mendeley Desktop/Downloaded/Auer, Cesa-Bianchi, Fischer - 2002 - Finite-time analysis of the multiarmed bandit problem.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Adaptive allocation rules,Bandit problems,Finite horizon regret},
month = {may},
number = {2-3},
pages = {235--256},
publisher = {Springer},
title = {{Finite-time analysis of the multiarmed bandit problem}},
volume = {47},
year = {2002}
}
